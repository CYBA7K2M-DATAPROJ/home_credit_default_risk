{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ispkmjcqianj"
   },
   "source": [
    "# Pipeline desctipion\n",
    "\n",
    "1. Import resources to :\n",
    "Github/resources\n",
    "\\\n",
    "    V\\\n",
    "    V\\\n",
    "    V\\\n",
    "    V\\\n",
    "resources/\n",
    "  *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClVoyoRXil2_"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d632b08c-d252-4238-b496-e2c6edebec4b",
    "_uuid": "eb13bf76d4e1e60d0703856ec391cdc2c5bdf1fb",
    "id": "E-IZBtxjyLXM"
   },
   "source": [
    "## Imports\n",
    "\n",
    "We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ryG668meHO7o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (8.12.3)\n",
      "Requirement already satisfied: jupyter-server-proxy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (4.2.0)\n",
      "Requirement already satisfied: pyngrok in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (7.1.6)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (7.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (1.24.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (2.0.3)\n",
      "Requirement already satisfied: category_encoders in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (2.6.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (3.7.5)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (0.13.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (4.66.4)\n",
      "Requirement already satisfied: dask in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (2023.5.0)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (16.1.0)\n",
      "Requirement already satisfied: missingno in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 14)) (0.5.2)\n",
      "Requirement already satisfied: sweetviz in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (2.3.1)\n",
      "Requirement already satisfied: dataprep in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (0.4.5)\n",
      "Collecting termcolor\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from jupyter->-r requirements.txt (line 1)) (6.29.4)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.8/dist-packages (from jupyter->-r requirements.txt (line 1)) (8.1.3)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.8/dist-packages (from jupyter->-r requirements.txt (line 1)) (5.5.2)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from jupyter->-r requirements.txt (line 1)) (7.16.4)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.8/dist-packages (from jupyter->-r requirements.txt (line 1)) (6.6.3)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->-r requirements.txt (line 2)) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.8/dist-packages (from ipython->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython->-r requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython->-r requirements.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from ipython->-r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.8/dist-packages (from ipython->-r requirements.txt (line 2)) (5.14.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython->-r requirements.txt (line 2)) (0.1.7)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython->-r requirements.txt (line 2)) (2.18.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.8/dist-packages (from ipython->-r requirements.txt (line 2)) (3.0.47)\n",
      "Requirement already satisfied: jupyter-server>=1.24.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server-proxy->-r requirements.txt (line 3)) (2.14.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from jupyter-server-proxy->-r requirements.txt (line 3)) (3.9.5)\n",
      "Requirement already satisfied: simpervisor>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server-proxy->-r requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from jupyter-server-proxy->-r requirements.txt (line 3)) (7.2.1)\n",
      "Requirement already satisfied: tornado>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server-proxy->-r requirements.txt (line 3)) (6.4.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.8/dist-packages (from pyngrok->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /usr/local/lib/python3.8/dist-packages (from notebook->-r requirements.txt (line 5)) (0.2.4)\n",
      "Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from notebook->-r requirements.txt (line 5)) (4.2.2)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /usr/local/lib/python3.8/dist-packages (from notebook->-r requirements.txt (line 5)) (2.27.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 7)) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 7)) (2024.1)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from category_encoders->-r requirements.txt (line 8)) (6.4.0)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.8/dist-packages (from category_encoders->-r requirements.txt (line 8)) (0.5.6)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders->-r requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders->-r requirements.txt (line 8)) (1.10.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders->-r requirements.txt (line 8)) (0.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 9)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 9)) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 9)) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 9)) (24.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 9)) (4.53.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.8/dist-packages (from dask->-r requirements.txt (line 12)) (2024.6.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from dask->-r requirements.txt (line 12)) (0.12.1)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from dask->-r requirements.txt (line 12)) (3.0.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from dask->-r requirements.txt (line 12)) (1.4.1)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.8/dist-packages (from dask->-r requirements.txt (line 12)) (8.1.7)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.8/dist-packages (from sweetviz->-r requirements.txt (line 15)) (3.1.4)\n",
      "Requirement already satisfied: flask<3,>=2 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (2.3.3)\n",
      "Requirement already satisfied: regex<2022.0.0,>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (2021.11.10)\n",
      "Requirement already satisfied: metaphone<0.7,>=0.6 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (0.6)\n",
      "Requirement already satisfied: flask_cors<4.0.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (3.0.10)\n",
      "Requirement already satisfied: jsonpath-ng<2.0,>=1.5 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (1.6.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.7 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (3.8.1)\n",
      "Requirement already satisfied: pydot<2.0.0,>=1.4.2 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (1.4.2)\n",
      "Requirement already satisfied: python-crfsuite==0.9.8 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (0.9.8)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.6 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (1.10.17)\n",
      "Requirement already satisfied: bokeh<3,>=2 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (2.4.3)\n",
      "Requirement already satisfied: python-stdnum<2.0,>=1.16 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (1.20)\n",
      "Requirement already satisfied: wordcloud<2.0,>=1.8 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (1.9.3)\n",
      "Requirement already satisfied: sqlalchemy==1.3.24 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (1.3.24)\n",
      "Requirement already satisfied: varname<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (0.8.3)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.1.2 in /usr/local/lib/python3.8/dist-packages (from dataprep->-r requirements.txt (line 16)) (2.15.2)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (26.0.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (5.7.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (8.6.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (6.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 1)) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 1)) (3.0.11)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from qtconsole->jupyter->-r requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (4.12.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 2)) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->-r requirements.txt (line 2)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->-r requirements.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->-r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->-r requirements.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.8/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (0.20.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.8/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (1.8.3)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.8/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (4.4.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (0.18.1)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.8/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (7.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.3; python_version < \"3.10\"->jupyter-server-proxy->-r requirements.txt (line 3)) (3.19.2)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /usr/lib/python3/dist-packages (from jupyterlab<4.3,>=4.2.0->notebook->-r requirements.txt (line 5)) (45.2.0)\n",
      "Requirement already satisfied: tomli>=1.2.2; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from jupyterlab<4.3,>=4.2.0->notebook->-r requirements.txt (line 5)) (2.0.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab<4.3,>=4.2.0->notebook->-r requirements.txt (line 5)) (2.2.5)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab<4.3,>=4.2.0->notebook->-r requirements.txt (line 5)) (0.27.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab<4.3,>=4.2.0->notebook->-r requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 5)) (2.15.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 5)) (0.9.25)\n",
      "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 5)) (4.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders->-r requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders->-r requirements.txt (line 8)) (1.4.2)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=1.2.0->dask->-r requirements.txt (line 12)) (1.0.0)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.8/dist-packages (from flask<3,>=2->dataprep->-r requirements.txt (line 16)) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from flask<3,>=2->dataprep->-r requirements.txt (line 16)) (1.8.2)\n",
      "Requirement already satisfied: Werkzeug>=2.3.7 in /usr/local/lib/python3.8/dist-packages (from flask<3,>=2->dataprep->-r requirements.txt (line 16)) (3.0.3)\n",
      "Requirement already satisfied: ply in /usr/local/lib/python3.8/dist-packages (from jsonpath-ng<2.0,>=1.5->dataprep->-r requirements.txt (line 16)) (3.11)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->-r requirements.txt (line 1)) (4.2.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.7->nbconvert->jupyter->-r requirements.txt (line 1)) (2.20.0)\n",
      "Requirement already satisfied: webencodings>=0.4 in /usr/local/lib/python3.8/dist-packages (from tinycss2->nbconvert->jupyter->-r requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 1)) (2.5)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.8/dist-packages (from argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (21.2.0)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.8/dist-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (0.35.1)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (0.1.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.8/dist-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.8/dist-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (0.1.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from anyio>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.8/dist-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->-r requirements.txt (line 5)) (2024.6.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 5)) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 5)) (1.3.10)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 5)) (0.18.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 5)) (2023.12.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.8/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->-r requirements.txt (line 5)) (0.14.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->-r requirements.txt (line 3)) (2.22)\n",
      "Installing collected packages: termcolor\n",
      "Successfully installed termcolor-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "Djr6-_0ZyLXM"
   },
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from category_encoders import BinaryEncoder, HashingEncoder, LeaveOneOutEncoder, WOEEncoder, PolynomialEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "import numpy as np\n",
    "import gc  # Garbage collection\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import dask.dataframe as dd\n",
    "from collections import Counter, defaultdict\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as pacsv\n",
    "import pyarrow.parquet as pq\n",
    "import missingno as msno\n",
    "import sweetviz as sv\n",
    "from dataprep.eda import create_report\n",
    "import argparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from termcolor import colored\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "import category_encoders as ce\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeGzEqNqP-zU"
   },
   "source": [
    "## Constantes (Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "REti1sZOKk_0"
   },
   "outputs": [],
   "source": [
    "# Resources\n",
    "GITHUB_RESOURCES_FOLDER_PATH='https://github.com/lessons-data-ai-engineer/project_4-home_credit_default_risk/tree/main/resources'\n",
    "LOCAL_RESOURCES_FOLDER_PATH='/content/resources'\n",
    "# Resources > Files paths\n",
    "APP_TRAIN_FILE_PATH = LOCAL_RESOURCES_FOLDER_PATH+'/application_train.csv'\n",
    "PREVIOUS_APPLICATION_FILE_PATH = LOCAL_RESOURCES_FOLDER_PATH+'/previous_application.csv'\n",
    "BUREAU_FILE_PATH = LOCAL_RESOURCES_FOLDER_PATH+'/bureau.csv'\n",
    "BUREAU_BALANCE_FILE_PATH = LOCAL_RESOURCES_FOLDER_PATH+'/bureau_balance.csv'\n",
    "POS_CASH_BALANCE_FILE_PATH = LOCAL_RESOURCES_FOLDER_PATH+'/POS_CASH_balance.csv'\n",
    "CREDIT_CARD_BALANCE_FILE_PATH = LOCAL_RESOURCES_FOLDER_PATH+'/credit_card_balance.csv'\n",
    "INSTALLMENTS_PAYMENTS_FILE_PATH = LOCAL_RESOURCES_FOLDER_PATH+'/installments_payments.csv'\n",
    "APP_TEST_FILE_PATH = LOCAL_RESOURCES_FOLDER_PATH+'/application_test.csv'\n",
    "HOMECREDIT_COLUMNS_DESCRIPTION_FILE_PATH = LOCAL_RESOURCES_FOLDER_PATH+'/HomeCredit_columns_description.csv'\n",
    "# Exports\n",
    "LOCAL_EXPORT_FOLDER_PATH='/content/exports'\n",
    "# Export > General Settings\n",
    "TESTING_MODE=True\n",
    "TESTING_MODE_MAX_LINES=1000\n",
    "TESTING_MODE_SUB_FOLDER_NAME='testing_data'\n",
    "GENERAL_CHUNK_SIZE=10000\n",
    "# Exports > Columns metadata files\n",
    "LOCAL_EXPORT_COLUMNS_METADATA_FILES_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/columns_metadata_files'\n",
    "# Exports > Steps paths > Generate first sweetviz report\n",
    "LOCAL_EXPORT_FIRST_SWEETVIZ_REPORTS_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/fist_sweetviz_reports'\n",
    "# Exports > Steps paths > missing_values_replaced\n",
    "LOCAL_EXPORT_MISSING_VALUES_REPLACED_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/missing_values_replaced'\n",
    "# Exports > Steps paths > columns_types_checked\n",
    "LOCAL_EXPORT_COLUMNS_TYPES_CHECKED_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/columns_types_checked'\n",
    "# Exports > Steps paths > Replace to nan\n",
    "LOCAL_EXPORT_REPLACE_EMPTY_TO_NAN_PATH=LOCAL_EXPORT_FOLDER_PATH+'/replace_to_nan'\n",
    "# Exports > Steps paths > Imputation\n",
    "LOCAL_EXPORT_IMPUTATION_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/imputed'\n",
    "# Exports > Steps paths > Imputation checks\n",
    "LOCAL_EXPORT_IMPUTATION_CHECKS_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/imputed_checks'\n",
    "# Exports > Steps paths > Imputation evaluation\n",
    "LOCAL_EXPORT_IMPUTATION_RESULTS_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/imputed_results'\n",
    "# Exports > Steps paths > Without outliers\n",
    "LOCAL_EXPORT_IMPUTED_WITHOUT_OUTLIERS_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/imputed_without_outliers'\n",
    "# Exports > Steps paths > Without duplicates\n",
    "LOCAL_EXPORT_IMPUTED_WITHOUT_DUPLICATES_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/imputed_without_duplicates'\n",
    "# Exports > Steps paths > Feature Enginering\n",
    "LOCAL_EXPORT_FEATURE_ENGINEERED_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/feature_engineered'\n",
    "# Export > Steps paths > Merge dataframes\n",
    "LOCAL_EXPORT_MERGED_DATAFRAMES_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/merged_dataframes'\n",
    "# Export > Steps paths > Encoded & Aligned\n",
    "LOCAL_EXPORT_ENCODED_AND_ALIGNED_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/encoded_and_aligned'\n",
    "# Export > Steps paths > Manual verification after encoding\n",
    "LOCAL_EXPORT_MANUAL_CHECK_PATCH_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/manual_check_patch'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eF5Fa5IQCnX"
   },
   "source": [
    "## Resources import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJ8Jwm6ULmbk"
   },
   "outputs": [],
   "source": [
    "# Créer les dossiers locaux si nécessaire\n",
    "os.makedirs(LOCAL_RESOURCES_FOLDER_PATH, exist_ok=True)\n",
    "os.makedirs(LOCAL_EXPORT_FOLDER_PATH, exist_ok=True)\n",
    "\n",
    "# Télécharger la liste des fichiers depuis GitHub\n",
    "!wget -O {LOCAL_RESOURCES_FOLDER_PATH}/index.html {GITHUB_RESOURCES_FOLDER_PATH}\n",
    "\n",
    "# Extraire les liens des fichiers CSV\n",
    "!grep -oP 'href=\"\\K[^\"]+\\.csv' {LOCAL_RESOURCES_FOLDER_PATH}/index.html | awk '{print \"https://github.com\"$1}' > {LOCAL_RESOURCES_FOLDER_PATH}/csv_links.txt\n",
    "\n",
    "# Extraire les noms de fichiers CSV de la liste des liens\n",
    "csv_files = []\n",
    "with open(f'{LOCAL_RESOURCES_FOLDER_PATH}/csv_links.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        file_name = line.strip().split('/')[-1]\n",
    "        csv_files.append(file_name)\n",
    "\n",
    "# Liste des fichiers existants dans le dossier local\n",
    "local_files = os.listdir(LOCAL_RESOURCES_FOLDER_PATH)\n",
    "\n",
    "# Supprimer les fichiers locaux qui ne sont pas dans la liste des fichiers CSV sur GitHub\n",
    "for local_file in local_files:\n",
    "    if local_file not in csv_files and local_file.endswith('.csv'):\n",
    "        os.remove(os.path.join(LOCAL_RESOURCES_FOLDER_PATH, local_file))\n",
    "        print(f\"Deleted: {local_file}\")\n",
    "\n",
    "# Télécharger chaque fichier CSV sans duplication et mettre à jour les fichiers existants\n",
    "with open(f'{LOCAL_RESOURCES_FOLDER_PATH}/csv_links.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        # Modifier l'URL pour obtenir le lien de téléchargement direct\n",
    "        download_url = line.strip().replace('/blob/', '/raw/')\n",
    "        # Utiliser -N pour éviter la duplication des fichiers et mettre à jour les fichiers existants\n",
    "        !wget -P {LOCAL_RESOURCES_FOLDER_PATH} -N {download_url}\n",
    "\n",
    "# Vérifier les fichiers téléchargés\n",
    "!ls {LOCAL_RESOURCES_FOLDER_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5e67831-4751-4f11-8e07-527e3e092671",
    "_uuid": "ded520f73b9e94ed47ac2e994a5fb1bcb9093d0f",
    "id": "3EtpFgLjyLXM"
   },
   "source": [
    "## Resources check\n",
    "\n",
    "First, we can list all the available data files. There are a total of 9 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 6 other files containing additional information about each loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "2cdca894-e637-43a9-8f80-5791c2bb9041",
    "_uuid": "c54e1559611512ebd447ac24f2226c2fffd61dcd",
    "id": "z5n5iy5byLXN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'application_test.csv', 'POS_CASH_balance.csv', 'installments_payments.csv', 'bureau_balance.csv', 'previous_application.csv', 'bureau.csv', 'credit_card_balance.csv', 'application_train.csv', 'HomeCredit_columns_description.csv']\n"
     ]
    }
   ],
   "source": [
    "# List files available\n",
    "print(os.listdir(LOCAL_RESOURCES_FOLDER_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z7OK-jPQ_5G"
   },
   "source": [
    "## Dataframes creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "p0vvlTiUyLXN"
   },
   "outputs": [],
   "source": [
    "app_train = pd.read_csv(APP_TRAIN_FILE_PATH)\n",
    "previous_application = pd.read_csv(PREVIOUS_APPLICATION_FILE_PATH)\n",
    "bureau = pd.read_csv(BUREAU_FILE_PATH)\n",
    "bureau_balance = pd.read_csv(BUREAU_BALANCE_FILE_PATH)\n",
    "POS_CASH_balance = pd.read_csv(POS_CASH_BALANCE_FILE_PATH)\n",
    "credit_card_balance = pd.read_csv(CREDIT_CARD_BALANCE_FILE_PATH)\n",
    "installments_payments = pd.read_csv(INSTALLMENTS_PAYMENTS_FILE_PATH)\n",
    "app_test = pd.read_csv(APP_TEST_FILE_PATH)\n",
    "HomeCredit_columns_description = pd.read_csv(HOMECREDIT_COLUMNS_DESCRIPTION_FILE_PATH,encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "knZWAiFOhyv6"
   },
   "outputs": [],
   "source": [
    "app_test = pd.read_csv(APP_TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "HomeCredit_columns_description = pd.read_csv(HOMECREDIT_COLUMNS_DESCRIPTION_FILE_PATH,encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4695541966d3d29e8a7a8975b072d01caff1631d",
    "id": "YUJeNBW9yLXN"
   },
   "source": [
    "The training data has 307511 observations (each one a separate loan) and 122 features (variables) including the `TARGET` (the label we want to predict)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcrccyNKTkA7"
   },
   "source": [
    "## Dataframes preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDshNcHrSx73"
   },
   "outputs": [],
   "source": [
    "print('Training data shape: ', app_train.shape)\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvedc4DyS5P9"
   },
   "outputs": [],
   "source": [
    "print('previous_application data shape: ', previous_application.shape)\n",
    "previous_application.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmC2nv4SS9A0"
   },
   "outputs": [],
   "source": [
    "print('bureau data shape: ', bureau.shape)\n",
    "bureau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qz92ji9oTBGi"
   },
   "outputs": [],
   "source": [
    "print('bureau_balance data shape: ', bureau_balance.shape)\n",
    "bureau_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HASla0dTEvD"
   },
   "outputs": [],
   "source": [
    "print('POS_CASH_balance data shape: ', POS_CASH_balance.shape)\n",
    "POS_CASH_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRRpPl9dTHGE"
   },
   "outputs": [],
   "source": [
    "print('credit_card_balance data shape: ', credit_card_balance.shape)\n",
    "credit_card_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kx-mlOFSTKIt"
   },
   "outputs": [],
   "source": [
    "print('installments_payments data shape: ', installments_payments.shape)\n",
    "installments_payments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "qt_i9kCVTMRn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data shape:  (2, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version https://git-lfs.github.com/spec/v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oid sha256:a36161331d839150a67b6216d4de066f543...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>size 26567651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          version https://git-lfs.github.com/spec/v1\n",
       "0  oid sha256:a36161331d839150a67b6216d4de066f543...\n",
       "1                                      size 26567651"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Testing data shape: ', app_test.shape)\n",
    "app_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "p6DRp4G-T-4W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data shape:  (2, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version https://git-lfs.github.com/spec/v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oid sha256:eef7665398228a80f7367c9258220c5fbe1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>size 37383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          version https://git-lfs.github.com/spec/v1\n",
       "0  oid sha256:eef7665398228a80f7367c9258220c5fbe1...\n",
       "1                                         size 37383"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Testing data shape: ', HomeCredit_columns_description.shape)\n",
    "HomeCredit_columns_description.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e351f02c8a5886756507a2d4f1ddba4791220f12",
    "id": "VBDn1dtEyLXO"
   },
   "source": [
    "The test set is considerably smaller and lacks a `TARGET` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b1a02afd367d1c4ee3a3a936382ca42fb921b9d",
    "id": "BWzzfy0MyLXO"
   },
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "23b20e53-3484-4c4b-bec9-2d8ac2ac918d",
    "_uuid": "7c006a09627df1333c557dc11a09f372bde34dda",
    "id": "ZGF8Fd8fyLXO"
   },
   "source": [
    "## Examine the Distribution of the Target Column\n",
    "\n",
    "The target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5fb6ab16-1b38-4ecf-8123-e48c7c061773",
    "_uuid": "2163ca09678b53dbe88388ccbc7d0e0f7d6c6230",
    "id": "99_rosZgyLXO"
   },
   "outputs": [],
   "source": [
    "app_train['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0e93c1e2-f6b8-4a0b-82b6-7dad8df56048",
    "_uuid": "1b2611fb3cf392023c3f40fd2f7b96f56f5dee7d",
    "id": "PVbJ-zSgyLXO"
   },
   "outputs": [],
   "source": [
    "app_train['TARGET'].astype(int).plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "48f008ff-d81e-46b2-80a3-e58f2a6627ca",
    "_uuid": "119106000875202a0030109f14b73245fc4285e1",
    "id": "DSi0LbIUyLXO"
   },
   "source": [
    "From this information, we see this is an [_imbalanced class problem_](http://www.chioka.in/class-imbalance-problem/). There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can [weight the classes](http://xgboost.readthedocs.io/en/latest/parameter.html) by their representation in the data to reflect this imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "507ec6b1-99d0-4324-a3ed-bdea2f916227",
    "_uuid": "58851dfef481f32b3026e89b086534ea3683440d",
    "id": "E4RysgAWyLXP"
   },
   "source": [
    "## Examine Missing Values\n",
    "\n",
    "Next we can look at the number and percentage of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rJ7Q_WyMVJfF"
   },
   "outputs": [],
   "source": [
    "def missing_values_table(df, df_name):\n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "\n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "\n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns={0: 'Missing Values', 1: '% of Total Values'})\n",
    "\n",
    "    # Add a new column with the name of the dataframe\n",
    "    mis_val_table_ren_columns['DataFrame Name'] = df_name\n",
    "\n",
    "    # Correctly rename the index\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns.rename_axis('Row')\n",
    "\n",
    "    # Sort the table by percentage of missing descending, including those with 0% missing\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns.sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
    "          \"There are \" + str(mis_val_table_ren_columns[mis_val_table_ren_columns['Missing Values'] > 0].shape[0]) +\n",
    "          \" columns that have missing values.\")\n",
    "\n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zM7sEA8OVNyq"
   },
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values_app_train = missing_values_table(app_train,'application_{train|test}.csv')\n",
    "missing_values_bureau = missing_values_table(bureau, 'bureau.csv')\n",
    "missing_values_bureau_balance = missing_values_table(bureau_balance, 'bureau_balance.csv')\n",
    "missing_values_POS_CASH_balance = missing_values_table(POS_CASH_balance, 'POS_CASH_balance.csv')\n",
    "missing_values_credit_card_balance = missing_values_table(credit_card_balance, 'credit_card_balance.csv')\n",
    "missing_values_previous_application = missing_values_table(previous_application, 'previous_application.csv')\n",
    "missing_values_installments_payments = missing_values_table(installments_payments, 'installments_payments.csv')\n",
    "missing_values_app_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2Tr2QrNMzb-"
   },
   "outputs": [],
   "source": [
    "concat_columns = pd.concat([\n",
    "    missing_values_app_train,\n",
    "    missing_values_bureau,\n",
    "    missing_values_bureau_balance,\n",
    "    missing_values_POS_CASH_balance,\n",
    "    missing_values_credit_card_balance,\n",
    "    missing_values_previous_application,\n",
    "    missing_values_installments_payments], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoQ486nHS7DN"
   },
   "outputs": [],
   "source": [
    "# concat_columns_grouped = concat_columns.groupby('Row').sum()\n",
    "concat_columns.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFA98xaKkv6c"
   },
   "outputs": [],
   "source": [
    "hccd = HomeCredit_columns_description[[\"Row\", \"Description\"]]\n",
    "# result = pd.merge(hccd, missing_values, on=\"Row\", how=\"inner\")\n",
    "\n",
    "result_all_columns = pd.merge(hccd, concat_columns, on=\"Row\", how=\"inner\")\n",
    "result_all_columns.head(71)\n",
    "result_all_columns.to_csv('hccd_and_missing_values_all_dfs.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKJ_98tiGdFL"
   },
   "outputs": [],
   "source": [
    "LOCAL_EXPORT_COLUMNS_METADATA_FILES_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "df3c6a1d-b3ff-4565-bb32-5cba43c52729",
    "_uuid": "0b1be19103910ce83ebf54eeed99e42829643578",
    "id": "EkmRQwJ2yLXP"
   },
   "source": [
    "When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can [handle missing values with no need for imputation](https://stats.stackexchange.com/questions/235489/xgboost-can-handle-missing-data-in-the-forecasting-phase). Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPoJNDfPIl4_"
   },
   "source": [
    "## Columns description & missing values studings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBFvKB7yIt4u"
   },
   "source": [
    "### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "BFC9fgdBIsOa"
   },
   "outputs": [],
   "source": [
    "def analyze_missing_values_and_columns_description(input_folder, output_folder, columns_description_file, main_file, files_with_target=None, target_column=None, analyze_columns=None, extract_columns=None, testing=False, max_rows=1000, exclude_files=None, testing_output_folder=\"testing_data\", chunksize=50000):\n",
    "    \"\"\"\n",
    "    Analyze missing values for all CSV files in the input folder, and save reports in the output folder.\n",
    "    \"\"\"\n",
    "\n",
    "    def missing_values_table(df, df_name):\n",
    "        \"\"\"\n",
    "        Create a table with the number and percentage of missing values for each column of the DataFrame.\n",
    "        \"\"\"\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(columns={0: 'Missing Values', 1: '% of Total Values'})\n",
    "        mis_val_table_ren_columns['DataFrame Name'] = df_name\n",
    "        mis_val_table_ren_columns.index.name = 'Row'\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns.sort_values('% of Total Values', ascending=False).round(1)\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "    def column_metadata(df, df_name, target_column=None):\n",
    "        \"\"\"\n",
    "        Create a table with metadata for each column of the DataFrame, including correlation with the target column.\n",
    "        \"\"\"\n",
    "        metadata = []\n",
    "        for col in df.columns:\n",
    "            corr = None\n",
    "            if target_column and target_column in df.columns and col != target_column:\n",
    "                if df[col].dtype in [float, int]:\n",
    "                    if df[col].var() != 0 and df[col].notnull().sum() > 1:\n",
    "                        try:\n",
    "                            corr = df[col].corr(df[target_column])\n",
    "                        except FloatingPointError:\n",
    "                            corr = None\n",
    "            col_type = df[col].dtype\n",
    "            unique_vals = df[col].nunique()\n",
    "            metadata.append({\n",
    "                'Row': col,\n",
    "                'Corr_Target_Before_Cleaning': corr,\n",
    "                'Data_Type': col_type,\n",
    "                'Unique_Values': unique_vals,\n",
    "                'DataFrame Name': df_name\n",
    "            })\n",
    "        metadata_df = pd.DataFrame(metadata)\n",
    "        metadata_df.set_index('Row', inplace=True)\n",
    "        return metadata_df\n",
    "\n",
    "    # Load the column descriptions\n",
    "    hccd = pd.read_csv(os.path.join(input_folder, columns_description_file), encoding=\"ISO-8859-1\")\n",
    "\n",
    "    # Check that 'Row' and 'DataFrame Name' are in hccd\n",
    "    if 'Row' not in hccd.columns or 'DataFrame Name' not in hccd.columns:\n",
    "        raise KeyError(\"The columns 'Row' and 'DataFrame Name' must be present in the columns description file.\")\n",
    "\n",
    "    # Load the main file containing the target column\n",
    "    main_df = pd.read_csv(os.path.join(input_folder, main_file))\n",
    "\n",
    "    # Check if the output folder exists, otherwise create it\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # If in testing mode, create the subfolder for testing data\n",
    "    if testing:\n",
    "        testing_output_folder = os.path.join(output_folder, testing_output_folder)\n",
    "        if not os.path.exists(testing_output_folder):\n",
    "            os.makedirs(testing_output_folder)\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    # Exclude specified files\n",
    "    if exclude_files:\n",
    "        csv_files = [f for f in csv_files if f not in exclude_files]\n",
    "\n",
    "    # Initialize lists to store missing values and metadata results\n",
    "    missing_values_list = []\n",
    "    metadata_list = []\n",
    "\n",
    "    # Iterate through the CSV files with a progress bar\n",
    "    for filename in tqdm(csv_files, desc=\"Analyzing missing values and column metadata\", unit=\"file\"):\n",
    "        if filename == main_file:\n",
    "            continue  # The main file is already loaded\n",
    "\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        print(f\"\\nAnalyzing {file_path}...\")\n",
    "\n",
    "        # Load data in chunks\n",
    "        chunk_iter = pd.read_csv(file_path, chunksize=chunksize)\n",
    "\n",
    "        for chunk in chunk_iter:\n",
    "            if testing:\n",
    "                chunk = chunk.head(max_rows)\n",
    "\n",
    "            if extract_columns:\n",
    "                chunk = chunk[extract_columns]\n",
    "\n",
    "            if 'SK_ID_CURR' in chunk.columns:\n",
    "                chunk = chunk.merge(main_df[['SK_ID_CURR', target_column]], on='SK_ID_CURR', how='left')\n",
    "            elif 'SK_ID_BUREAU' in chunk.columns:\n",
    "                chunk = chunk.merge(main_df[['SK_ID_CURR', target_column]], left_on='SK_ID_BUREAU', right_on='SK_ID_CURR', how='left')\n",
    "            elif 'SK_ID_PREV' in chunk.columns:\n",
    "                chunk = chunk.merge(main_df[['SK_ID_CURR', target_column]], left_on='SK_ID_PREV', right_on='SK_ID_CURR', how='left')\n",
    "\n",
    "            missing_values = missing_values_table(chunk, filename)\n",
    "            missing_values_list.append(missing_values)\n",
    "\n",
    "            if files_with_target and filename in files_with_target:\n",
    "                metadata = column_metadata(chunk, filename, target_column)\n",
    "            else:\n",
    "                metadata = column_metadata(chunk, filename)\n",
    "\n",
    "            metadata_list.append(metadata)\n",
    "\n",
    "    concat_missing_values = pd.concat(missing_values_list, axis=0)\n",
    "    concat_metadata = pd.concat(metadata_list, axis=0)\n",
    "\n",
    "    concat_missing_values.reset_index(inplace=True)\n",
    "    concat_metadata.reset_index(inplace=True)\n",
    "\n",
    "    print(\"Missing Values DataFrame Columns:\", concat_missing_values.columns)\n",
    "    print(\"Metadata DataFrame Columns:\", concat_metadata.columns)\n",
    "\n",
    "    # Check if 'Row' and 'DataFrame Name' columns exist before merging\n",
    "    if 'Row' not in concat_missing_values.columns or 'Row' not in concat_metadata.columns:\n",
    "        raise KeyError(\"The 'Row' column is missing in one of the dataframes.\")\n",
    "    if 'DataFrame Name' not in concat_missing_values.columns or 'DataFrame Name' not in concat_metadata.columns:\n",
    "        raise KeyError(\"The 'DataFrame Name' column is missing in one of the dataframes.\")\n",
    "\n",
    "    # Merge with column descriptions using 'Row' and 'DataFrame Name'\n",
    "    result_all_columns = pd.merge(hccd, concat_missing_values, on=[\"Row\", \"DataFrame Name\"], how=\"inner\")\n",
    "    result_all_columns = pd.merge(result_all_columns, concat_metadata, on=[\"Row\", \"DataFrame Name\"], how=\"inner\")\n",
    "\n",
    "    if testing:\n",
    "        missing_values_output_file = os.path.join(testing_output_folder, 'hccd_and_missing_values_all_dfs.csv')\n",
    "    else:\n",
    "        missing_values_output_file = os.path.join(output_folder, 'hccd_and_missing_values_all_dfs.csv')\n",
    "\n",
    "    result_all_columns.to_csv(missing_values_output_file, index=True)\n",
    "    print(f\"Missing values and column metadata analysis saved to {missing_values_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e_Oong0I7WN"
   },
   "source": [
    "### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "G0CfFmZhI9Fr"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"The columns 'Row' and 'DataFrame Name' must be present in the columns description file.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalyze_missing_values_and_columns_description\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL_RESOURCES_FOLDER_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL_EXPORT_COLUMNS_METADATA_FILES_FOLDER_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmain_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapplication_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGENERAL_CHUNK_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles_with_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprevious_application.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTARGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns_description_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHomeCredit_columns_description.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43manalyze_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextract_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTESTING_MODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTESTING_MODE_MAX_LINES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapplication_test.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHomeCredit_columns_description.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msample_submission.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcredit_card_balance.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtesting_output_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTESTING_MODE_SUB_FOLDER_NAME\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 51\u001b[0m, in \u001b[0;36manalyze_missing_values_and_columns_description\u001b[0;34m(input_folder, output_folder, columns_description_file, main_file, files_with_target, target_column, analyze_columns, extract_columns, testing, max_rows, exclude_files, testing_output_folder, chunksize)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Check that 'Row' and 'DataFrame Name' are in hccd\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRow\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m hccd\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame Name\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m hccd\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe columns \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame Name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be present in the columns description file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Load the main file containing the target column\u001b[39;00m\n\u001b[1;32m     54\u001b[0m main_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, main_file))\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The columns 'Row' and 'DataFrame Name' must be present in the columns description file.\""
     ]
    }
   ],
   "source": [
    "analyze_missing_values_and_columns_description(\n",
    "        input_folder=LOCAL_RESOURCES_FOLDER_PATH,\n",
    "        output_folder=LOCAL_EXPORT_COLUMNS_METADATA_FILES_FOLDER_PATH,\n",
    "        main_file='application_train.csv',\n",
    "        chunksize=GENERAL_CHUNK_SIZE,\n",
    "        files_with_target=[\"application_train.csv\", \"previous_application.csv\"],\n",
    "        target_column='TARGET',\n",
    "        columns_description_file='HomeCredit_columns_description.csv',\n",
    "        analyze_columns=None,\n",
    "        extract_columns=None,\n",
    "        testing=TESTING_MODE,\n",
    "        max_rows=TESTING_MODE_MAX_LINES,\n",
    "        exclude_files=['application_test.csv','HomeCredit_columns_description.csv','sample_submission.csv','credit_card_balance.csv'],\n",
    "        testing_output_folder=TESTING_MODE_SUB_FOLDER_NAME\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHYgsQFeclWb"
   },
   "outputs": [],
   "source": [
    "hccd_and_missing_values_all_dfs = pd.read_csv(LOCAL_EXPORT_COLUMNS_METADATA_FILES_FOLDER_PATH+'/'+TESTING_MODE_SUB_FOLDER_NAME+'/' +'hccd_and_missing_values_all_dfs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNPY_xc2dPik"
   },
   "outputs": [],
   "source": [
    "hccd_and_missing_values_all_dfs.head(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_XNAocHDflM"
   },
   "source": [
    "## First analyse reports for all dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmXoQli0EFIQ"
   },
   "source": [
    "### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYog-hxODebx"
   },
   "outputs": [],
   "source": [
    "def generate_sweetviz_reports(input_folder, output_folder, analyze_columns=None, extract_columns=None, testing=False, max_lines=1000, exclude_files=None, testing_output_subfolder=\"testing_data\"):\n",
    "    \"\"\"\n",
    "    Génère des rapports Sweetviz pour tous les fichiers CSV dans le dossier d'entrée\n",
    "    et enregistre les rapports dans le dossier de sortie.\n",
    "\n",
    "    Parameters:\n",
    "    input_folder (str): Le chemin du dossier contenant les fichiers CSV.\n",
    "    output_folder (str): Le chemin du dossier où les rapports HTML seront enregistrés.\n",
    "    analyze_columns (list): Liste des colonnes à analyser.\n",
    "    extract_columns (list): Liste des colonnes à extraire.\n",
    "    testing (bool): Mode testing pour analyser seulement les max_lines premières lignes.\n",
    "    max_lines (int): Nombre maximum de lignes à analyser en mode testing.\n",
    "    exclude_files (list): Liste des fichiers à exclure de l'analyse.\n",
    "    testing_output_subfolder (str): Nom du sous-dossier pour les données de test en mode testing.\n",
    "    \"\"\"\n",
    "    # Déterminer le dossier de sortie final\n",
    "    if testing:\n",
    "        output_folder = os.path.join(output_folder, testing_output_subfolder)\n",
    "\n",
    "    # Vérifier si le dossier de sortie existe, sinon le créer\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Lister tous les fichiers CSV dans le dossier d'entrée\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    # Exclure les fichiers spécifiés\n",
    "    if exclude_files:\n",
    "        csv_files = [f for f in csv_files if f not in exclude_files]\n",
    "\n",
    "    # Parcourir les fichiers CSV avec une barre de progression\n",
    "    for filename in tqdm(csv_files, desc=\"Generating Sweetviz reports\", unit=\"file\"):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        print(f\"\\nAnalyzing {file_path}...\")\n",
    "\n",
    "        # Charger les données\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Mode testing : limiter le nombre de lignes\n",
    "        if testing:\n",
    "            df = df.head(max_lines)\n",
    "\n",
    "        # Appliquer les colonnes à extraire si spécifiées\n",
    "        if extract_columns:\n",
    "            df = df[extract_columns]\n",
    "\n",
    "        # Générer le rapport\n",
    "        if analyze_columns:\n",
    "            report = sv.analyze(df[analyze_columns])\n",
    "        else:\n",
    "            report = sv.analyze(df)\n",
    "\n",
    "        # Chemin du fichier de sortie\n",
    "        output_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_sweetviz_report.html\")\n",
    "\n",
    "        # Enregistrer le rapport en HTML\n",
    "        report.show_html(output_file_path, open_browser=False)\n",
    "        print(f\"Report saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_5O3XE5EGot"
   },
   "source": [
    "### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqCxeB_XEHmA"
   },
   "outputs": [],
   "source": [
    " generate_sweetviz_reports(\n",
    "     input_folder=LOCAL_RESOURCES_FOLDER_PATH,\n",
    "     output_folder=LOCAL_EXPORT_FIRST_SWEETVIZ_REPORTS_FOLDER_PATH,\n",
    "     analyze_columns=None,\n",
    "     extract_columns=None,\n",
    "     testing=TESTING_MODE,\n",
    "     max_lines=TESTING_MODE_MAX_LINES,\n",
    "     exclude_files=['HomeCredit_columns_description.csv','sample_submission.csv'],\n",
    "     testing_output_subfolder=TESTING_MODE_SUB_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAd5nn842M0i"
   },
   "source": [
    "## Replace empty values by NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83HTfugQ2asQ"
   },
   "source": [
    "### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzxeiWdI2WK3"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_dataframes(input_folder, export_folder, chunk_size=100000, ignore_files=None, max_lines=None, testing=False, testing_subfolder=\"testing_data\"):\n",
    "    \"\"\"\n",
    "    Charge plusieurs DataFrames à partir de fichiers CSV dans un dossier, remplace les valeurs vides par NaN,\n",
    "    et exporte les DataFrames corrigés dans un autre dossier.\n",
    "\n",
    "    :param input_folder: Dossier contenant les fichiers CSV à charger\n",
    "    :param export_folder: Dossier où les DataFrames corrigés seront exportés\n",
    "    :param chunk_size: Taille des chunks pour le traitement des DataFrames\n",
    "    :param ignore_files: Liste de fichiers à ignorer lors du traitement\n",
    "    :param max_lines: Nombre maximum de lignes à traiter par fichier (uniquement pris en compte si testing est True)\n",
    "    :param testing: Booléen indiquant si le mode test est activé\n",
    "    :param testing_subfolder: Nom du sous-dossier pour les données de test en mode testing\n",
    "    :return: Dictionnaire des DataFrames corrigés\n",
    "    \"\"\"\n",
    "    if ignore_files is None:\n",
    "        ignore_files = []\n",
    "\n",
    "    if testing:\n",
    "        export_folder = os.path.join(export_folder, testing_subfolder)\n",
    "\n",
    "    if not os.path.exists(export_folder):\n",
    "        os.makedirs(export_folder)\n",
    "\n",
    "    # Lister tous les fichiers CSV dans le dossier d'entrée, en excluant les fichiers ignorés\n",
    "    csv_files = {os.path.splitext(file)[0]: os.path.join(input_folder, file)\n",
    "                 for file in os.listdir(input_folder) if file.endswith('.csv') and file not in ignore_files}\n",
    "\n",
    "    for key, path in csv_files.items():\n",
    "        corrected_file_path = os.path.join(export_folder, f\"{key}.csv\")\n",
    "        total_lines_processed = 0\n",
    "\n",
    "        try:\n",
    "            with pd.read_csv(path, chunksize=chunk_size) as reader:\n",
    "                for i, chunk in enumerate(reader):\n",
    "                    if testing and max_lines is not None and total_lines_processed >= max_lines:\n",
    "                        break\n",
    "\n",
    "                    if testing and max_lines is not None:\n",
    "                        lines_to_process = chunk.head(max_lines - total_lines_processed)\n",
    "                    else:\n",
    "                        lines_to_process = chunk\n",
    "\n",
    "                    total_lines_processed += len(lines_to_process)\n",
    "\n",
    "                    lines_to_process.replace(['', ' ', 'NA', 'N/A', None, 'NaN'], np.nan, inplace=True)\n",
    "                    mode = 'w' if i == 0 else 'a'\n",
    "                    header = i == 0\n",
    "                    lines_to_process.to_csv(corrected_file_path, mode=mode, header=header, index=False)\n",
    "                    print(f\"{key}: chunk {i + 1} processed and saved to {corrected_file_path}\")\n",
    "\n",
    "            print(f\"Completed processing for {key}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {path} due to {e}\")\n",
    "\n",
    "    print(f\"All files processed and saved to {export_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUcc3A_v2frI"
   },
   "source": [
    "### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3MZ1Hhh2qdO"
   },
   "outputs": [],
   "source": [
    "load_and_preprocess_dataframes(\n",
    "    input_folder=LOCAL_RESOURCES_FOLDER_PATH,\n",
    "    export_folder=LOCAL_EXPORT_MISSING_VALUES_REPLACED_FOLDER_PATH,\n",
    "    chunk_size=GENERAL_CHUNK_SIZE,\n",
    "    ignore_files=[\"HomeCredit_columns_description.csv\",'sample_submission.csv'],\n",
    "    max_lines=TESTING_MODE_MAX_LINES,\n",
    "    testing=TESTING_MODE,\n",
    "    testing_subfolder=TESTING_MODE_SUB_FOLDER_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9cWfERVvbhS"
   },
   "source": [
    "## Columns types verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1P36MkHwHwt"
   },
   "source": [
    "### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yr6iMGAvfHr"
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(filename='data_processing.log', level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "def analyze_chunk_types(chunk):\n",
    "    type_summary = defaultdict(Counter)\n",
    "    for column in chunk.columns:\n",
    "        type_counts = chunk[column].apply(detect_type).value_counts(normalize=True)\n",
    "        for dtype, percentage in type_counts.items():\n",
    "            type_summary[column][dtype] += percentage * len(chunk) / len(chunk.index)\n",
    "    return type_summary\n",
    "\n",
    "def detect_type(value):\n",
    "    if pd.isnull(value):\n",
    "        return 'null'\n",
    "    try:\n",
    "        int(value)\n",
    "        return 'int'\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        float(value)\n",
    "        return 'float'\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        pd.to_datetime(value)\n",
    "        return 'datetime'\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    if str(value).lower() in ['true', 'false']:\n",
    "        return 'bool'\n",
    "    return 'str'\n",
    "\n",
    "def merge_type_summaries(main_summary, chunk_summary):\n",
    "    for column, types in chunk_summary.items():\n",
    "        for dtype, percentage in types.items():\n",
    "            main_summary[column][dtype] += percentage\n",
    "\n",
    "def normalize_type_summaries(type_summary, total_chunks):\n",
    "    if total_chunks == 0:\n",
    "        return type_summary\n",
    "    for column, types in type_summary.items():\n",
    "        for dtype in types:\n",
    "            type_summary[column][dtype] /= total_chunks\n",
    "    return type_summary\n",
    "\n",
    "def display_type_summary(file_name, type_summary):\n",
    "    logging.info(f\"\\nFile: {file_name}\")\n",
    "    for column, types in type_summary.items():\n",
    "        logging.info(f\"Column: {column}\")\n",
    "        for dtype, percentage in types.items():\n",
    "            logging.info(f\"  {dtype}: {percentage:.2%}\")\n",
    "\n",
    "def get_most_frequent_type(column_types):\n",
    "    if not column_types:\n",
    "        return None\n",
    "    return max(column_types, key=column_types.get)\n",
    "\n",
    "def convert_column_types(data, type_summary, identifier_columns):\n",
    "    for column, types in type_summary.items():\n",
    "        if column in identifier_columns:\n",
    "            data[column] = data[column].astype(str)\n",
    "            continue\n",
    "\n",
    "        most_frequent_type = get_most_frequent_type(types)\n",
    "        if most_frequent_type:\n",
    "            try:\n",
    "                if most_frequent_type == 'int':\n",
    "                    data[column] = pd.to_numeric(data[column], errors='coerce')\n",
    "                    data[column] = data[column].fillna(0).astype('Int64')\n",
    "                elif most_frequent_type == 'float':\n",
    "                    data[column] = pd.to_numeric(data[column], errors='coerce')\n",
    "                elif most_frequent_type == 'bool':\n",
    "                    data[column] = data[column].astype(bool)\n",
    "                elif most_frequent_type == 'datetime':\n",
    "                    data[column] = pd.to_datetime(data[column], errors='coerce')\n",
    "                elif most_frequent_type == 'str':\n",
    "                    data[column] = data[column].astype(str)\n",
    "                elif most_frequent_type == 'categorical':\n",
    "                    if data[column].nunique() < len(data[column]) * 0.05:\n",
    "                        data[column] = data[column].astype('category')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error converting column {column} to {most_frequent_type}: {e}\")\n",
    "    return data\n",
    "\n",
    "def generate_report(file_name, type_summary_before, type_summary_after):\n",
    "    report = f\"\\nFile: {file_name}\\n\"\n",
    "    report += \"Column Type Summary Before Conversion:\\n\"\n",
    "    for column, types in type_summary_before.items():\n",
    "        report += f\"  Column: {column}\\n\"\n",
    "        for dtype, percentage in types.items():\n",
    "            report += f\"    {dtype}: {percentage:.2%}\\n\"\n",
    "    report += \"Column Type Summary After Conversion:\\n\"\n",
    "    for column, types in type_summary_after.items():\n",
    "        report += f\"  Column: {column}\\n\"\n",
    "        for dtype in types:\n",
    "            report += f\"    {dtype}\\n\"\n",
    "    return report\n",
    "\n",
    "def save_type_summary_report(file_name, type_summary):\n",
    "    report = f\"Type Summary for {file_name}:\\n\"\n",
    "    for column, types in type_summary.items():\n",
    "        report += f\"  {column}:\\n\"\n",
    "        for dtype, percentage in types.items():\n",
    "            report += f\"    {dtype}: {percentage:.2%}\\n\"\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "def load_and_preprocess_dataframes(input_folder, export_folder, chunk_size=10000, ignore_files=None, max_rows=None, testing=False, testing_sub_folder_name=\"testing_data\", identifier_columns=None):\n",
    "    if ignore_files is None:\n",
    "        ignore_files = []\n",
    "\n",
    "    if identifier_columns is None:\n",
    "        identifier_columns = []\n",
    "\n",
    "    if testing:\n",
    "        input_folder = os.path.join(input_folder, testing_sub_folder_name)\n",
    "        export_folder = os.path.join(export_folder, testing_sub_folder_name)\n",
    "\n",
    "    if not os.path.exists(export_folder):\n",
    "        os.makedirs(export_folder)\n",
    "\n",
    "    csv_files = {os.path.splitext(file)[0]: os.path.join(input_folder, file)\n",
    "                 for file in os.listdir(input_folder) if file.endswith('.csv') and file not in ignore_files}\n",
    "\n",
    "    for key, path in csv_files.items():\n",
    "        corrected_file_path = os.path.join(export_folder, f\"{key}.csv\")\n",
    "        total_lines_processed = 0\n",
    "        type_summary = defaultdict(Counter)\n",
    "        chunk_count = 0\n",
    "\n",
    "        try:\n",
    "            with pd.read_csv(path, chunksize=chunk_size) as reader:\n",
    "                for i, chunk in enumerate(reader):\n",
    "                    if testing and max_rows is not None and total_lines_processed >= max_rows:\n",
    "                        break\n",
    "\n",
    "                    if testing and max_rows is not None:\n",
    "                        lines_to_process = chunk.head(max_rows - total_lines_processed)\n",
    "                    else:\n",
    "                        lines_to_process = chunk\n",
    "\n",
    "                    total_lines_processed += len(lines_to_process)\n",
    "                    chunk_count += 1\n",
    "\n",
    "                    chunk_types = analyze_chunk_types(lines_to_process)\n",
    "                    merge_type_summaries(type_summary, chunk_types)\n",
    "\n",
    "                    lines_to_process.replace(['', ' ', 'NA', 'N/A', None, 'NaN'], np.nan, inplace=True)\n",
    "                    mode = 'w' if i == 0 else 'a'\n",
    "                    header = i == 0\n",
    "                    lines_to_process.to_csv(corrected_file_path, mode=mode, header=header, index=False)\n",
    "                    logging.info(f\"{key}: chunk {i + 1} processed and saved to {corrected_file_path}\")\n",
    "\n",
    "            # Normalize type summary\n",
    "            normalized_type_summary = normalize_type_summaries(type_summary, chunk_count)\n",
    "\n",
    "            # Load the entire file to convert types\n",
    "            data = pd.read_csv(corrected_file_path)\n",
    "            data = convert_column_types(data, normalized_type_summary, identifier_columns)\n",
    "            data.to_csv(corrected_file_path, index=False)\n",
    "\n",
    "            # Generate and log the report\n",
    "            final_type_summary = analyze_chunk_types(data)\n",
    "            report = generate_report(key, normalized_type_summary, final_type_summary)\n",
    "            logging.info(report)\n",
    "\n",
    "            # Save the type summary report to a file\n",
    "            report_file_path = os.path.join(export_folder, f\"{key}_type_summary.txt\")\n",
    "            save_type_summary_report(report_file_path, final_type_summary)\n",
    "\n",
    "            logging.info(f\"Completed processing and profiling for {key}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process {path} due to {e}\")\n",
    "\n",
    "    logging.info(f\"All files processed and saved to {export_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u3SOidmIPY5"
   },
   "source": [
    "### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7A_idx3VwKpQ"
   },
   "outputs": [],
   "source": [
    "load_and_preprocess_dataframes(\n",
    "    input_folder=LOCAL_EXPORT_MISSING_VALUES_REPLACED_FOLDER_PATH,\n",
    "    export_folder=LOCAL_EXPORT_COLUMNS_TYPES_CHECKED_FOLDER_PATH,\n",
    "    chunk_size=GENERAL_CHUNK_SIZE,\n",
    "    ignore_files=None,\n",
    "    max_rows=TESTING_MODE_MAX_LINES,\n",
    "    testing=TESTING_MODE,\n",
    "    testing_sub_folder_name=TESTING_MODE_SUB_FOLDER_NAME,\n",
    "    identifier_columns=[\"SK_ID_PREV\", \"SK_ID_CURR\",\"SK_ID_BUREAU\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESKvTf38wP8D"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(LOCAL_EXPORT_COLUMNS_TYPES_CHECKED_FOLDER_PATH+\"/testing_data/application_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmNptwV_wcs0"
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3gbzkD5Dsaq"
   },
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZK_kgyhZocr"
   },
   "source": [
    "### Imputation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZHJ6f_IYzG-"
   },
   "outputs": [],
   "source": [
    "# Logger class as provided by you\n",
    "class Logger:\n",
    "    def __init__(self, level=4, export_logs=False, log_folder_path='logs'):\n",
    "        self.log_level = level\n",
    "        self.export_logs = export_logs\n",
    "        self.log_folder_path = log_folder_path\n",
    "        self.log_messages = []\n",
    "\n",
    "    def log(self, message, level=4, color=None):\n",
    "        if level <= self.log_level:\n",
    "            if color:\n",
    "                print(colored(message, color))\n",
    "            else:\n",
    "                print(message)\n",
    "            self.log_messages.append(message)\n",
    "\n",
    "    def export(self, log_file_name='logs.txt'):\n",
    "        if self.export_logs:\n",
    "            log_file_path = os.path.join(self.log_folder_path, log_file_name)\n",
    "            os.makedirs(self.log_folder_path, exist_ok=True)\n",
    "            with open(log_file_path, 'w') as log_file:\n",
    "                for message in self.log_messages:\n",
    "                    log_file.write(message + '\\n')\n",
    "            print(colored(f'Logs exported to {log_file_path}', 'green'))\n",
    "\n",
    "# Save original data types of columns\n",
    "def save_column_types(df):\n",
    "    return df.dtypes.to_dict()\n",
    "\n",
    "# Restore data types of columns\n",
    "def restore_column_types(df, column_types):\n",
    "    for column, dtype in column_types.items():\n",
    "        df[column] = df[column].astype(dtype)\n",
    "    return df\n",
    "\n",
    "def prepare_data_for_imputation(df, logger, exclude_columns=None, missing_threshold=0.8, extreme_value_threshold=1e15, small_value_threshold=1e-15, round_decimals=None):\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "    logger.log(\"Starting data preparation\", 2)\n",
    "\n",
    "    # Initial data stats\n",
    "    logger.log(f\"Initial dataframe shape: {df.shape}\", 3)\n",
    "    logger.log(f\"Initial columns with missing values above {missing_threshold * 100}%:\", 3)\n",
    "    for col in df.columns:\n",
    "        if col in exclude_columns:\n",
    "            continue\n",
    "        missing_ratio = df[col].isnull().mean()\n",
    "        if missing_ratio > missing_threshold:\n",
    "            logger.log(f\"Column {col} has {missing_ratio * 100:.2f}% missing values\", 4)\n",
    "\n",
    "    # Drop columns with missing values above the threshold\n",
    "    missing_ratio = df.isnull().mean()\n",
    "    cols_to_drop = missing_ratio[missing_ratio > missing_threshold].index\n",
    "    df = df.drop(columns=[col for col in cols_to_drop if col not in exclude_columns])\n",
    "    logger.log(f\"Dropped columns: {list(set(cols_to_drop) - set(exclude_columns))}\", 3)\n",
    "    logger.log(f\"Dataframe shape after dropping columns: {df.shape}\", 3)\n",
    "\n",
    "    # Cap extreme values and replace small values to avoid underflow\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col in exclude_columns:\n",
    "            continue\n",
    "        col_min = df[col].min()\n",
    "        col_max = df[col].max()\n",
    "        logger.log(f\"Column {col}: min before capping = {col_min}, max before capping = {col_max}\", 4)\n",
    "\n",
    "        df[col] = np.clip(df[col], -extreme_value_threshold, extreme_value_threshold)\n",
    "        df[col] = df[col].apply(lambda x: small_value_threshold if 0 < abs(x) < small_value_threshold else x)\n",
    "\n",
    "        col_min_after = df[col].min()\n",
    "        col_max_after = df[col].max()\n",
    "        logger.log(f\"Column {col}: min after capping = {col_min_after}, max after capping = {col_max_after}\", 4)\n",
    "\n",
    "    # Replace infinite values with NaN\n",
    "    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Ensure no duplicated indices\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "    # Convert all numeric columns to float64 to ensure uniformity\n",
    "    df[numeric_cols] = df[numeric_cols].astype(np.float64)\n",
    "\n",
    "    # Round numerical values if specified\n",
    "    if round_decimals is not None:\n",
    "        df[numeric_cols] = df[numeric_cols].round(round_decimals)\n",
    "        logger.log(f\"Rounded numerical values to {round_decimals} decimal places\", 3)\n",
    "\n",
    "    # Log final data stats\n",
    "    for col in numeric_cols:\n",
    "        logger.log(f\"Column {col}: final min = {df[col].min()}, final max = {df[col].max()}, final NaNs = {df[col].isnull().sum()}\", 3)\n",
    "\n",
    "    logger.log(\"Data preparation completed\", 2, \"green\")\n",
    "    return df\n",
    "\n",
    "def impute_and_save_dataframes_from_folder(input_folder, imputations, exclude_columns=None, chunk_size=10000, output_folder='LOCAL_EXPORT_FOLDER_PATH/',\n",
    "                                           clean_memory=False, testing=False, max_rows=None, testing_data_sub_folder='testing_data',\n",
    "                                           round_decimals=None, log_level=4, export_logs=False, log_folder_path='logs'):\n",
    "    logger = Logger(level=log_level, export_logs=export_logs, log_folder_path=log_folder_path)\n",
    "\n",
    "    if testing:\n",
    "        input_folder = os.path.join(input_folder, testing_data_sub_folder)\n",
    "        output_folder = os.path.join(output_folder, testing_data_sub_folder)\n",
    "        logger.log_folder_path = os.path.join(logger.log_folder_path, testing_data_sub_folder)\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    csv_files = {os.path.splitext(file)[0]: os.path.join(input_folder, file)\n",
    "                 for file in os.listdir(input_folder) if file.endswith('.csv')}\n",
    "\n",
    "    total_tasks = len(csv_files) * len(imputations)\n",
    "    task_count = 0\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for df_name, csv_file in csv_files.items():\n",
    "        try:\n",
    "            logger.log(f\"Processing file {csv_file}\", 2)\n",
    "            original_df = pd.read_csv(csv_file)\n",
    "            logger.log(f\"Original dataframe shape: {original_df.shape}\", 3)\n",
    "            if testing and max_rows is not None:\n",
    "                original_df = original_df.iloc[:max_rows]\n",
    "                logger.log(f\"Testing mode: reduced dataframe shape: {original_df.shape}\", 3)\n",
    "\n",
    "            # Save original column types\n",
    "            original_column_types = save_column_types(original_df)\n",
    "            logger.log(f\"Original column types: {original_column_types}\", 3)\n",
    "\n",
    "            original_df = prepare_data_for_imputation(original_df, logger, exclude_columns, round_decimals=round_decimals)\n",
    "\n",
    "            results[df_name] = {\n",
    "                \"original\": csv_file,\n",
    "                \"imputed\": {}\n",
    "            }\n",
    "\n",
    "            for imp_name, imputer in imputations:\n",
    "                try:\n",
    "                    logger.log(f\"Starting imputation for {df_name} using {imp_name}\", 2)\n",
    "                    df = original_df.copy()\n",
    "                    imputed_chunks = []\n",
    "                    for chunk_start in tqdm(range(0, df.shape[0], chunk_size), desc=f'Imputing {df_name} with {imp_name}', leave=False):\n",
    "                        chunk_end = min(chunk_start + chunk_size, df.shape[0])\n",
    "                        df_chunk = df.iloc[chunk_start:chunk_end].copy()\n",
    "\n",
    "                        logger.log(f\"Processing chunk: rows {chunk_start} to {chunk_end}\", 3)\n",
    "                        for col in df_chunk.select_dtypes(include=[np.number]).columns:\n",
    "                            logger.log(f\"Chunk {chunk_start}-{chunk_end} - Column {col}: min = {df_chunk[col].min()}, max = {df_chunk[col].max()}, NaNs = {df_chunk[col].isnull().sum()}\", 4)\n",
    "\n",
    "                        numeric_cols = df_chunk.select_dtypes(include=[np.number]).columns\n",
    "                        non_numeric_cols = df_chunk.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "                        if numeric_cols.size > 0:\n",
    "                            numeric_imputer = imputer if isinstance(imputer, (SimpleImputer, KNNImputer, IterativeImputer)) else None\n",
    "                            if numeric_imputer:\n",
    "                                df_chunk[numeric_cols] = numeric_imputer.fit_transform(df_chunk[numeric_cols])\n",
    "                            else:\n",
    "                                df_chunk[numeric_cols] = df_chunk[numeric_cols].apply(imputer)\n",
    "\n",
    "                        if non_numeric_cols.size > 0:\n",
    "                            non_numeric_imputer = SimpleImputer(strategy='most_frequent')\n",
    "                            df_chunk[non_numeric_cols] = non_numeric_imputer.fit_transform(df_chunk[non_numeric_cols])\n",
    "\n",
    "                        # Verify no NaN in the chunk\n",
    "                        if df_chunk.isnull().any().any():\n",
    "                            nan_columns = df_chunk.columns[df_chunk.isnull().any()].tolist()\n",
    "                            logger.log(f\"NaN values found in chunk: rows {chunk_start} to {chunk_end}, columns: {nan_columns}\", 1, 'red')\n",
    "                            logger.log(f\"Attempting to fill remaining NaNs with default value\", 2, 'yellow')\n",
    "\n",
    "                            # Fill NaN with 0 or median\n",
    "                            for col in nan_columns:\n",
    "                                if df_chunk[col].dtype == np.number:\n",
    "                                    df_chunk[col].fillna(df_chunk[col].median(), inplace=True)\n",
    "                                else:\n",
    "                                    df_chunk[col].fillna(0, inplace=True)\n",
    "\n",
    "                            # Verify again\n",
    "                            if df_chunk.isnull().any().any():\n",
    "                                nan_columns = df_chunk.columns[df_chunk.isnull().any()].tolist()\n",
    "                                logger.log(f\"Still NaN values found in chunk after default filling: rows {chunk_start} to {chunk_end}, columns: {nan_columns}\", 1, 'red')\n",
    "                                raise ValueError(f\"NaN values found in the imputed chunk for {df_name} using method {imp_name}\")\n",
    "\n",
    "                        imputed_chunks.append(df_chunk)\n",
    "\n",
    "                        del df_chunk\n",
    "                        gc.collect()\n",
    "\n",
    "                    imputed_df = pd.concat(imputed_chunks, ignore_index=True)\n",
    "\n",
    "                    # Restore original column types\n",
    "                    imputed_df = restore_column_types(imputed_df, original_column_types)\n",
    "\n",
    "                    # Second pass to handle any remaining NaNs\n",
    "                    remaining_na_cols = imputed_df.columns[imputed_df.isnull().any()].tolist()\n",
    "                    if remaining_na_cols:\n",
    "                        logger.log(f\"Columns with remaining NaNs after first imputation: {remaining_na_cols}\", 3)\n",
    "                        for col in remaining_na_cols:\n",
    "                            if imputed_df[col].dtype == np.number:\n",
    "                                imputed_df[col].fillna(imputed_df[col].median(), inplace=True)\n",
    "                            else:\n",
    "                                imputed_df[col].fillna(0, inplace=True)\n",
    "\n",
    "                    # Blocking condition: raise error if NaNs still exist\n",
    "                    if imputed_df.isnull().any().any():\n",
    "                        nan_columns = imputed_df.columns[imputed_df.isnull().any()].tolist()\n",
    "                        logger.log(f\"Blocking error: Columns with remaining NaNs: {nan_columns}\", 1, 'red')\n",
    "                        raise ValueError(f\"Imputation failed for {df_name} using {imp_name}. Remaining NaNs in columns: {nan_columns}\")\n",
    "\n",
    "                    df_output_dir = os.path.join(output_folder, imp_name)\n",
    "                    os.makedirs(df_output_dir, exist_ok=True)\n",
    "                    output_file = os.path.join(df_output_dir, f'{df_name}.csv')\n",
    "\n",
    "                    if os.path.exists(output_file):\n",
    "                        os.remove(output_file)\n",
    "                        logger.log(f'Existing file {output_file} removed.', 3, 'yellow')\n",
    "\n",
    "                    imputed_df.to_csv(output_file, index=False)\n",
    "                    logger.log(f'Successfully imputed and saved {df_name} using {imp_name} to {output_file}', 2, 'green')\n",
    "\n",
    "                    results[df_name][\"imputed\"][imp_name] = output_file\n",
    "\n",
    "                    if clean_memory:\n",
    "                        del imputed_chunks\n",
    "                        del df\n",
    "                        gc.collect()\n",
    "                        logger.log(f'Original dataframe {df_name} deleted and memory cleaned.', 3)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.log(f\"Failed to impute {df_name} using {imp_name} due to {e}\", 2, 'red')\n",
    "\n",
    "                task_count += 1\n",
    "                progress = (task_count / total_tasks) * 100\n",
    "                logger.log(f'Progress: {task_count}/{total_tasks} ({progress:.2f}%)', 2)\n",
    "                tqdm.write(f'Progress: {task_count}/{total_tasks} ({progress:.2f}%)')\n",
    "        except Exception as e:\n",
    "            logger.log(f\"Failed to process {csv_file} due to {e}\", 2, 'red')\n",
    "\n",
    "    json_output_file = os.path.join(output_folder, f'imputation_results{\"_test\" if testing else \"\"}.json')\n",
    "    with open(json_output_file, 'w') as json_file:\n",
    "        json.dump(results, json_file, indent=4)\n",
    "    logger.log(f'Successfully exported imputation results to {json_output_file}', 2, 'green')\n",
    "\n",
    "    logger.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7B5N2z_ZsEo"
   },
   "source": [
    "### Imputation method settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIrhv7z5Y6BP"
   },
   "outputs": [],
   "source": [
    "imputations = [\n",
    "    ('median', SimpleImputer(strategy='median')),\n",
    "    ('mean', SimpleImputer(strategy='mean')),\n",
    "    ('most_frequent', SimpleImputer(strategy='most_frequent')),\n",
    "    ('constant_0', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('knn', KNNImputer(n_neighbors=5)),\n",
    "    ('iterative', IterativeImputer()),\n",
    "    ('ffill', lambda x: x.ffill()),  # Forward fill\n",
    "    ('bfill', lambda x: x.bfill())   # Backward fill\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ-Caql8Zu2j"
   },
   "source": [
    "### Imputation method call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXoub_4PZmaP"
   },
   "outputs": [],
   "source": [
    "impute_and_save_dataframes_from_folder(\n",
    "    input_folder=LOCAL_EXPORT_COLUMNS_TYPES_CHECKED_FOLDER_PATH,\n",
    "    imputations=imputations,\n",
    "    chunk_size=GENERAL_CHUNK_SIZE,\n",
    "    output_folder=LOCAL_EXPORT_IMPUTATION_FOLDER_PATH,\n",
    "    clean_memory=True,\n",
    "    exclude_columns=[\"SK_ID_PREV\", \"SK_ID_CURR\",\"SK_ID_BUREAU\"],\n",
    "    testing=TESTING_MODE,\n",
    "    max_rows=TESTING_MODE_MAX_LINES,\n",
    "    testing_data_sub_folder=TESTING_MODE_SUB_FOLDER_NAME,\n",
    "    round_decimals=3,\n",
    "    log_level=2,\n",
    "    export_logs=True,\n",
    "    log_folder_path=LOCAL_EXPORT_IMPUTATION_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nXK2J3GmIZM"
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(filename='data_processing.log', level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "def validate_dataframes(original, imputed):\n",
    "    # Ensure the dataframes have the same columns\n",
    "    common_columns = original.columns.intersection(imputed.columns)\n",
    "    return original[common_columns], imputed[common_columns]\n",
    "\n",
    "def replace_missing_values(df, value=0):\n",
    "    \"\"\"\n",
    "    Remplace les valeurs manquantes dans un DataFrame par une valeur donnée.\n",
    "\n",
    "    :param df: DataFrame avec des valeurs manquantes.\n",
    "    :param value: Valeur de remplacement (par défaut 0).\n",
    "    :return: DataFrame avec les valeurs manquantes remplacées.\n",
    "    \"\"\"\n",
    "    return df.fillna(value)\n",
    "\n",
    "def check_alignment_and_calculate_rmse(original, imputed, mask, replacement_value=0):\n",
    "    \"\"\"\n",
    "    Calcule le RMSE entre les valeurs originales et imputées, uniquement sur les valeurs qui étaient manquantes.\n",
    "\n",
    "    :param original: DataFrame original.\n",
    "    :param imputed: DataFrame imputé.\n",
    "    :param mask: Masque indiquant les positions des valeurs manquantes initiales.\n",
    "    :param replacement_value: Valeur de remplacement pour les valeurs manquantes dans le calcul (par défaut 0).\n",
    "    :return: RMSE.\n",
    "    \"\"\"\n",
    "    original_filled = replace_missing_values(original, replacement_value)\n",
    "    diff = original_filled[mask] - imputed[mask]\n",
    "    mse = (diff ** 2).mean().mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def scan_directory_for_imputations(input_folder):\n",
    "    imputation_methods = {}\n",
    "    for method in os.listdir(input_folder):\n",
    "        method_path = os.path.join(input_folder, method)\n",
    "        if os.path.isdir(method_path):\n",
    "            csv_files = {os.path.splitext(file)[0]: os.path.join(method_path, file)\n",
    "                         for file in os.listdir(method_path) if file.endswith('.csv')}\n",
    "            imputation_methods[method] = csv_files\n",
    "    return imputation_methods\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    # Drop columns that are not numeric\n",
    "    df = df.select_dtypes(include=[np.number])\n",
    "    return df\n",
    "\n",
    "def evaluate_imputation_methods(originals_folder, imputations_folder, output_folder, chunk_size=10000, testing=False, max_rows=200, testing_data_sub_folder='testing_data'):\n",
    "    \"\"\"\n",
    "    Évalue les méthodes d'imputation sur plusieurs DataFrames originaux et leurs imputations correspondantes par chunks.\n",
    "\n",
    "    :param originals_folder: Chemin du dossier contenant les DataFrames originaux non imputés.\n",
    "    :param imputations_folder: Chemin du dossier contenant les DataFrames imputés par différentes méthodes.\n",
    "    :param output_folder: Chemin du dossier où les résultats doivent être exportés.\n",
    "    :param chunk_size: Taille des chunks pour le traitement des DataFrames.\n",
    "    :param testing: Booléen indiquant si le mode test est activé pour traiter un échantillon réduit de données.\n",
    "    :param max_rows: Nombre maximal de lignes à traiter en mode test.\n",
    "    :param testing_data_sub_folder: Nom du sous-dossier pour les données de test en mode testing.\n",
    "    :return: Dictionnaire contenant les scores RMSE moyens pour chaque méthode d'imputation pour chaque DataFrame original.\n",
    "    \"\"\"\n",
    "    if testing:\n",
    "        imputations_folder = os.path.join(imputations_folder, testing_data_sub_folder)\n",
    "        output_folder = os.path.join(output_folder, testing_data_sub_folder)\n",
    "        originals_folder = os.path.join(originals_folder, testing_data_sub_folder)\n",
    "\n",
    "    imputation_methods = scan_directory_for_imputations(imputations_folder)\n",
    "    results = {}\n",
    "\n",
    "    if testing and not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for method, files in imputation_methods.items():\n",
    "        for df_name, file_path in files.items():\n",
    "            try:\n",
    "                # Extraire le nom du fichier original en enlevant le suffixe d'imputation\n",
    "                original_file_path = os.path.join(originals_folder, f\"{df_name}.csv\")\n",
    "\n",
    "                if not os.path.exists(original_file_path):\n",
    "                    logging.error(f\"Original file {original_file_path} not found.\")\n",
    "                    continue\n",
    "\n",
    "                original_df = pd.read_csv(original_file_path)\n",
    "                imputed_df = pd.read_csv(file_path)\n",
    "\n",
    "                original_df = preprocess_dataframe(original_df)\n",
    "                imputed_df = preprocess_dataframe(imputed_df)\n",
    "\n",
    "                if testing:\n",
    "                    original_df = original_df.iloc[:max_rows]  # Take only the first max_rows for test mode\n",
    "                    imputed_df = imputed_df.iloc[:max_rows]  # Take only the first max_rows for test mode\n",
    "\n",
    "                # Créer un masque des valeurs manquantes initiales\n",
    "                missing_mask = original_df.isnull()\n",
    "\n",
    "                if df_name not in results:\n",
    "                    results[df_name] = {\"original\": original_file_path, \"imputed\": {}}\n",
    "\n",
    "                total_chunks = 0\n",
    "                method_scores = []\n",
    "\n",
    "                for chunk_start in tqdm(range(0, original_df.shape[0], chunk_size), desc=f'Evaluating {df_name} with {method}', leave=False):\n",
    "                    chunk_end = min(chunk_start + chunk_size, original_df.shape[0])\n",
    "                    original_chunk = original_df.iloc[chunk_start:chunk_end].copy()\n",
    "                    imputed_chunk = imputed_df.iloc[chunk_start:chunk_end].copy()\n",
    "\n",
    "                    numeric_cols = original_chunk.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "                    total_chunks += 1\n",
    "                    original_chunk_cleaned, imputed_chunk_cleaned = validate_dataframes(original_chunk, imputed_chunk)\n",
    "\n",
    "                    # Ajout de la vérification par assertion avant le calcul du RMSE\n",
    "                    assert not imputed_chunk_cleaned.isnull().values.any(), f\"NaN values found in the imputed chunk for {df_name} using method {method}\"\n",
    "\n",
    "                    if not original_chunk_cleaned.empty and not imputed_chunk_cleaned.empty:\n",
    "                        rmse = check_alignment_and_calculate_rmse(original_chunk_cleaned, imputed_chunk_cleaned, missing_mask.iloc[chunk_start:chunk_end], replacement_value=0)\n",
    "                        method_scores.append(rmse)\n",
    "\n",
    "                if method_scores:\n",
    "                    avg_rmse = np.mean(method_scores)\n",
    "                    results[df_name][\"imputed\"][method] = avg_rmse\n",
    "\n",
    "                if testing:\n",
    "                    json_output_file = os.path.join(output_folder, 'imputation_results_test.json')\n",
    "                else:\n",
    "                    json_output_file = os.path.join(output_folder, 'imputation_results.json')\n",
    "\n",
    "                with open(json_output_file, 'w') as json_file:\n",
    "                    json.dump(results, json_file, indent=4)\n",
    "                logging.info(f'Successfully exported imputation results to {json_output_file}')\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to process {file_path} due to {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def verify_imputed_files(imputations_folder):\n",
    "    \"\"\"\n",
    "    Vérifie que les fichiers imputés ne contiennent plus de valeurs NaN ou nulles et génère une assertion en cas d'échec.\n",
    "\n",
    "    :param imputations_folder: Chemin du dossier contenant les DataFrames imputés par différentes méthodes.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    imputation_methods = scan_directory_for_imputations(imputations_folder)\n",
    "    verification_results = {}\n",
    "\n",
    "    for method, files in imputation_methods.items():\n",
    "        method_results = {}\n",
    "        for df_name, file_path in files.items():\n",
    "            try:\n",
    "                imputed_df = pd.read_csv(file_path)\n",
    "                if imputed_df.isnull().values.any():\n",
    "                    method_results[df_name] = False\n",
    "                else:\n",
    "                    method_results[df_name] = True\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to verify {file_path} due to {e}\")\n",
    "                method_results[df_name] = False\n",
    "\n",
    "        verification_results[method] = method_results\n",
    "\n",
    "    # Affichage des résultats de la vérification\n",
    "    for method, results in verification_results.items():\n",
    "        print(f\"Verification results for method {method}:\")\n",
    "        for df_name, result in results.items():\n",
    "            status = \"Passed\" if result else \"Failed\"\n",
    "            print(f\" - {df_name}: {status}\")\n",
    "            assert result, f\"Verification failed for {df_name} using method {method}. NaN or null values found.\"\n",
    "\n",
    "    return verification_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPkbx3pEmLbf"
   },
   "outputs": [],
   "source": [
    "evaluate_imputation_methods(\n",
    "    originals_folder=LOCAL_EXPORT_COLUMNS_TYPES_CHECKED_FOLDER_PATH,\n",
    "    imputations_folder=LOCAL_EXPORT_IMPUTATION_FOLDER_PATH,\n",
    "    output_folder=LOCAL_EXPORT_IMPUTATION_CHECKS_FOLDER_PATH,\n",
    "    chunk_size=GENERAL_CHUNK_SIZE,\n",
    "    testing=TESTING_MODE,\n",
    "    max_rows=TESTING_MODE_MAX_LINES,\n",
    "    testing_data_sub_folder=TESTING_MODE_SUB_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEA6RADiwlwS"
   },
   "outputs": [],
   "source": [
    "test2 = pd.read_csv(LOCAL_EXPORT_IMPUTATION_FOLDER_PATH+\"/testing_data/bfill/application_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVz5hU_vwt7o"
   },
   "outputs": [],
   "source": [
    "test2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VztD7OsQtI9s"
   },
   "source": [
    "## Imputations Performances Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tdgwOv0trox"
   },
   "source": [
    "### Imputation Performances Evaluations Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yu05_oQ1v-iU"
   },
   "source": [
    "#### Check align & Calculate RMSE method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PF666I02kwd"
   },
   "outputs": [],
   "source": [
    "def check_alignment_and_calculate_rmse(original_df, imputed_df):\n",
    "    \"\"\"\n",
    "    Vérifie l'alignement des valeurs imputées avec les valeurs originales et calcule le RMSE.\n",
    "\n",
    "    :param original_df: DataFrame original avec des valeurs manquantes\n",
    "    :param imputed_df: DataFrame avec les valeurs imputées\n",
    "    :return: RMSE entre les valeurs originales et imputées\n",
    "    \"\"\"\n",
    "    # Assurez-vous que les indices sont alignés\n",
    "    if not original_df.index.equals(imputed_df.index):\n",
    "        raise ValueError(\"Les indices des DataFrames original et imputé ne sont pas alignés.\")\n",
    "\n",
    "    # Sélectionner uniquement les colonnes numériques\n",
    "    numeric_cols = original_df.select_dtypes(include=[np.number]).columns\n",
    "    original_df_numeric = original_df[numeric_cols]\n",
    "    imputed_df_numeric = imputed_df[numeric_cols]\n",
    "\n",
    "    # Masquer les valeurs manquantes dans les données originales\n",
    "    mask = original_df_numeric.isna()\n",
    "    original_masked = original_df_numeric[mask]\n",
    "    imputed_values = imputed_df_numeric[mask]\n",
    "\n",
    "    # Vérifiez l'alignement des valeurs\n",
    "    if not original_masked.index.equals(imputed_values.index):\n",
    "        raise ValueError(\"Les indices des valeurs manquantes et imputées ne sont pas alignés.\")\n",
    "    if not all(original_masked.columns == imputed_values.columns):\n",
    "        raise ValueError(\"Les colonnes des valeurs manquantes et imputées ne sont pas alignées.\")\n",
    "\n",
    "    # Convertir en arrays pour comparaison\n",
    "    y_true = original_masked.values.flatten()\n",
    "    y_pred = imputed_values.values.flatten()\n",
    "\n",
    "    # Filtrer les NaN avant de calculer le RMSE\n",
    "    non_nan_mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    y_true = y_true[non_nan_mask]\n",
    "    y_pred = y_pred[non_nan_mask]\n",
    "\n",
    "    if len(y_true) == 0 or len(y_pred) == 0:\n",
    "        raise ValueError(\"Aucune donnée valide pour calculer le RMSE.\")\n",
    "\n",
    "    # Calculer le RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1sKam5izBFD"
   },
   "source": [
    "#### Validate dataframes method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTJv7nluy9Oe"
   },
   "outputs": [],
   "source": [
    "def validate_dataframes(original_df, imputed_df):\n",
    "    \"\"\"\n",
    "    Vérifie et nettoie les DataFrames avant l'évaluation.\n",
    "\n",
    "    :param original_df: DataFrame original avec des valeurs manquantes\n",
    "    :param imputed_df: DataFrame avec les valeurs imputées\n",
    "    :return: DataFrames nettoyés et alignés\n",
    "    \"\"\"\n",
    "    # Assurez-vous que les indices sont alignés\n",
    "    if not original_df.index.equals(imputed_df.index):\n",
    "        raise ValueError(\"Les indices des DataFrames original et imputé ne sont pas alignés.\")\n",
    "\n",
    "    # Sélectionner uniquement les colonnes numériques\n",
    "    numeric_cols = original_df.select_dtypes(include=[np.number]).columns\n",
    "    original_df_numeric = original_df[numeric_cols]\n",
    "    imputed_df_numeric = imputed_df[numeric_cols]\n",
    "\n",
    "    # Supprimer les colonnes où toutes les valeurs sont manquantes dans le DataFrame original\n",
    "    valid_columns = original_df_numeric.columns[original_df_numeric.notna().any()].tolist()\n",
    "    original_df_cleaned = original_df_numeric[valid_columns]\n",
    "    imputed_df_cleaned = imputed_df_numeric[valid_columns]\n",
    "\n",
    "    return original_df_cleaned, imputed_df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tAcjFyGzEic"
   },
   "source": [
    "#### Evaluate imputation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-HRSVMutPFs"
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(filename='data_processing.log', level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "def validate_dataframes(original, imputed):\n",
    "    # Ensure the dataframes have the same columns\n",
    "    common_columns = original.columns.intersection(imputed.columns)\n",
    "    return original[common_columns], imputed[common_columns]\n",
    "\n",
    "def replace_missing_values(df, value=0):\n",
    "    \"\"\"\n",
    "    Remplace les valeurs manquantes dans un DataFrame par une valeur donnée.\n",
    "\n",
    "    :param df: DataFrame avec des valeurs manquantes.\n",
    "    :param value: Valeur de remplacement (par défaut 0).\n",
    "    :return: DataFrame avec les valeurs manquantes remplacées.\n",
    "    \"\"\"\n",
    "    return df.fillna(value)\n",
    "\n",
    "def check_alignment_and_calculate_rmse(original, imputed, mask, replacement_value=0):\n",
    "    \"\"\"\n",
    "    Calcule le RMSE entre les valeurs originales et imputées, uniquement sur les valeurs qui étaient manquantes.\n",
    "\n",
    "    :param original: DataFrame original.\n",
    "    :param imputed: DataFrame imputé.\n",
    "    :param mask: Masque indiquant les positions des valeurs manquantes initiales.\n",
    "    :param replacement_value: Valeur de remplacement pour les valeurs manquantes dans le calcul (par défaut 0).\n",
    "    :return: RMSE.\n",
    "    \"\"\"\n",
    "    original_filled = replace_missing_values(original, replacement_value)\n",
    "    diff = original_filled[mask] - imputed[mask]\n",
    "    mse = (diff ** 2).mean().mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def scan_directory_for_imputations(input_folder):\n",
    "    imputation_methods = {}\n",
    "    for method in os.listdir(input_folder):\n",
    "        method_path = os.path.join(input_folder, method)\n",
    "        if os.path.isdir(method_path):\n",
    "            csv_files = {os.path.splitext(file)[0]: os.path.join(method_path, file)\n",
    "                         for file in os.listdir(method_path) if file.endswith('.csv')}\n",
    "            imputation_methods[method] = csv_files\n",
    "    return imputation_methods\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    # Drop columns that are not numeric\n",
    "    df = df.select_dtypes(include=[np.number])\n",
    "    return df\n",
    "\n",
    "def evaluate_imputation_methods(originals_folder, imputations_folder, output_folder, chunk_size=10000, testing=False, max_rows=200, testing_data_sub_folder='testing_data'):\n",
    "    \"\"\"\n",
    "    Évalue les méthodes d'imputation sur plusieurs DataFrames originaux et leurs imputations correspondantes par chunks.\n",
    "\n",
    "    :param originals_folder: Chemin du dossier contenant les DataFrames originaux non imputés.\n",
    "    :param imputations_folder: Chemin du dossier contenant les DataFrames imputés par différentes méthodes.\n",
    "    :param output_folder: Chemin du dossier où les résultats doivent être exportés.\n",
    "    :param chunk_size: Taille des chunks pour le traitement des DataFrames.\n",
    "    :param testing: Booléen indiquant si le mode test est activé pour traiter un échantillon réduit de données.\n",
    "    :param max_rows: Nombre maximal de lignes à traiter en mode test.\n",
    "    :param testing_data_sub_folder: Nom du sous-dossier pour les données de test en mode testing.\n",
    "    :return: Dictionnaire contenant les scores RMSE moyens pour chaque méthode d'imputation pour chaque DataFrame original.\n",
    "    \"\"\"\n",
    "    if testing:\n",
    "        imputations_folder = os.path.join(imputations_folder, testing_data_sub_folder)\n",
    "        output_folder = os.path.join(output_folder, testing_data_sub_folder)\n",
    "        originals_folder = os.path.join(originals_folder, testing_data_sub_folder)\n",
    "\n",
    "    imputation_methods = scan_directory_for_imputations(imputations_folder)\n",
    "    results = {}\n",
    "\n",
    "    if testing and not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for method, files in imputation_methods.items():\n",
    "        for df_name, file_path in files.items():\n",
    "            try:\n",
    "                # Extraire le nom du fichier original en enlevant le suffixe d'imputation\n",
    "                original_file_path = os.path.join(originals_folder, f\"{df_name}.csv\")\n",
    "\n",
    "                if not os.path.exists(original_file_path):\n",
    "                    logging.error(f\"Original file {original_file_path} not found.\")\n",
    "                    continue\n",
    "\n",
    "                original_df = pd.read_csv(original_file_path)\n",
    "                imputed_df = pd.read_csv(file_path)\n",
    "\n",
    "                original_df = preprocess_dataframe(original_df)\n",
    "                imputed_df = preprocess_dataframe(imputed_df)\n",
    "\n",
    "                if testing:\n",
    "                    original_df = original_df.iloc[:max_rows]  # Take only the first max_rows for test mode\n",
    "                    imputed_df = imputed_df.iloc[:max_rows]  # Take only the first max_rows for test mode\n",
    "\n",
    "                # Créer un masque des valeurs manquantes initiales\n",
    "                missing_mask = original_df.isnull()\n",
    "\n",
    "                if df_name not in results:\n",
    "                    results[df_name] = {\"original\": original_file_path, \"imputed\": {}}\n",
    "\n",
    "                total_chunks = 0\n",
    "                method_scores = []\n",
    "\n",
    "                for chunk_start in tqdm(range(0, original_df.shape[0], chunk_size), desc=f'Evaluating {df_name} with {method}', leave=False):\n",
    "                    chunk_end = min(chunk_start + chunk_size, original_df.shape[0])\n",
    "                    original_chunk = original_df.iloc[chunk_start:chunk_end].copy()\n",
    "                    imputed_chunk = imputed_df.iloc[chunk_start:chunk_end].copy()\n",
    "\n",
    "                    numeric_cols = original_chunk.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "                    total_chunks += 1\n",
    "                    original_chunk_cleaned, imputed_chunk_cleaned = validate_dataframes(original_chunk, imputed_chunk)\n",
    "                    if not original_chunk_cleaned.empty and not imputed_chunk_cleaned.empty:\n",
    "                        rmse = check_alignment_and_calculate_rmse(original_chunk_cleaned, imputed_chunk_cleaned, missing_mask.iloc[chunk_start:chunk_end], replacement_value=0)\n",
    "                        method_scores.append(rmse)\n",
    "\n",
    "                if method_scores:\n",
    "                    avg_rmse = np.mean(method_scores)\n",
    "                    results[df_name][\"imputed\"][method] = avg_rmse\n",
    "\n",
    "                if testing:\n",
    "                    json_output_file = os.path.join(output_folder, 'imputation_results_test.json')\n",
    "                else:\n",
    "                    json_output_file = os.path.join(output_folder, 'imputation_results.json')\n",
    "\n",
    "                with open(json_output_file, 'w') as json_file:\n",
    "                    json.dump(results, json_file, indent=4)\n",
    "                logging.info(f'Successfully exported imputation results to {json_output_file}')\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to process {file_path} due to {e}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzDF74K7vwLJ"
   },
   "source": [
    "#### Plot result method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAf0D1eDvvPR"
   },
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    \"\"\"\n",
    "    Affiche les résultats des scores RMSE moyens pour chaque méthode d'imputation pour chaque DataFrame original.\n",
    "\n",
    "    :param results: Dictionnaire contenant les scores RMSE moyens pour chaque méthode d'imputation pour chaque DataFrame original.\n",
    "    \"\"\"\n",
    "    for df_name, df_results in results.items():\n",
    "        method_names = list(df_results['imputed'].keys())\n",
    "        rmse_scores = list(df_results['imputed'].values())\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(method_names, rmse_scores, color='skyblue')\n",
    "        plt.xlabel('Méthodes d\\'imputation')\n",
    "        plt.ylabel('RMSE moyen')\n",
    "        plt.title(f'Comparaison des performances des méthodes d\\'imputation pour {df_name}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fZMWax2R__2"
   },
   "source": [
    "### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUFFLklvSBku"
   },
   "outputs": [],
   "source": [
    "result = evaluate_imputation_methods(\n",
    "      originals_folder=LOCAL_EXPORT_COLUMNS_TYPES_CHECKED_FOLDER_PATH,\n",
    "        imputations_folder=LOCAL_EXPORT_IMPUTATION_FOLDER_PATH,\n",
    "        output_folder=LOCAL_EXPORT_IMPUTATION_RESULTS_FOLDER_PATH,\n",
    "        chunk_size=GENERAL_CHUNK_SIZE,\n",
    "        testing=TESTING_MODE,\n",
    "        max_rows=TESTING_MODE_MAX_LINES,\n",
    "        testing_data_sub_folder=TESTING_MODE_SUB_FOLDER_NAME\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwQQr4gjl56e"
   },
   "outputs": [],
   "source": [
    "def read_and_display_results(results_folder, testing=False, testing_data_sub_folder='testing_data'):\n",
    "    \"\"\"\n",
    "    Lit et affiche les résultats des évaluations d'imputation.\n",
    "\n",
    "    :param results_folder: Chemin du dossier contenant les fichiers de résultats JSON.\n",
    "    :param testing: Booléen indiquant si le mode test est activé.\n",
    "    :param testing_data_sub_folder: Nom du sous-dossier pour les résultats de test en mode testing.\n",
    "    :return: DataFrame des résultats d'imputation.\n",
    "    \"\"\"\n",
    "    # Déterminer le chemin du fichier de résultats\n",
    "    if testing:\n",
    "        results_file_path = os.path.join(results_folder, testing_data_sub_folder, 'imputation_results_test.json')\n",
    "    else:\n",
    "        results_file_path = os.path.join(results_folder, 'imputation_results.json')\n",
    "\n",
    "    # Vérifier si le fichier de résultats existe\n",
    "    if not os.path.exists(results_file_path):\n",
    "        print(f\"Le fichier de résultats {results_file_path} n'existe pas.\")\n",
    "        return\n",
    "\n",
    "    # Lire les résultats du fichier JSON\n",
    "    with open(results_file_path, 'r') as json_file:\n",
    "        results = json.load(json_file)\n",
    "\n",
    "    # Afficher les résultats de manière structurée\n",
    "    imputation_results = []\n",
    "    for df_name, df_results in results.items():\n",
    "        original_file = df_results['original']\n",
    "        for method, score in df_results['imputed'].items():\n",
    "            imputation_results.append({\n",
    "                'DataFrame': df_name,\n",
    "                'Method': method,\n",
    "                'RMSE': score,\n",
    "                'Original File': original_file\n",
    "            })\n",
    "\n",
    "    # Convertir les résultats en DataFrame\n",
    "    results_df = pd.DataFrame(imputation_results)\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(\"Résultats de l'évaluation des méthodes d'imputation :\")\n",
    "    print(results_df)\n",
    "\n",
    "    # Afficher les résultats graphiquement\n",
    "    plot_results(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCfgsLRkl751"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eN35IFGimQ0m"
   },
   "outputs": [],
   "source": [
    "results_df = read_and_display_results(LOCAL_EXPORT_IMPUTATION_RESULTS_FOLDER_PATH, testing=True, testing_data_sub_folder=TESTING_MODE_SUB_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKrWQ9sgSoo6"
   },
   "source": [
    "### Plot displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SXb6luz2b3s"
   },
   "outputs": [],
   "source": [
    "def compare_original_and_imputed(originals_folder, imputations_folder, output_folder, num_lines=5, testing=False, testing_data_sub_folder='testing_data'):\n",
    "    \"\"\"\n",
    "    Compare les valeurs originales et imputées pour les 5 premières lignes de chaque DataFrame.\n",
    "\n",
    "    :param originals_folder: Chemin du dossier contenant les DataFrames originaux non imputés.\n",
    "    :param imputations_folder: Chemin du dossier contenant les DataFrames imputés par différentes méthodes.\n",
    "    :param output_folder: Chemin du dossier où les résultats doivent être exportés.\n",
    "    :param num_lines: Nombre de lignes à comparer (par défaut 5).\n",
    "    :param testing: Booléen indiquant si le mode test est activé.\n",
    "    :param testing_data_sub_folder: Nom du sous-dossier pour les données de test en mode testing.\n",
    "    \"\"\"\n",
    "    if testing:\n",
    "        imputations_folder = os.path.join(imputations_folder, testing_data_sub_folder)\n",
    "        originals_folder = os.path.join(originals_folder, testing_data_sub_folder)\n",
    "\n",
    "    imputation_methods = scan_directory_for_imputations(imputations_folder)\n",
    "\n",
    "    for method, files in imputation_methods.items():\n",
    "        for df_name, file_path in files.items():\n",
    "            try:\n",
    "                # Extraire le nom du fichier original en enlevant le suffixe d'imputation\n",
    "                # df_name = '_'.join(df_name_with_imputation.split('_')[:-2])\n",
    "                original_file_path = os.path.join(originals_folder, f\"{df_name}.csv\")\n",
    "\n",
    "                if not os.path.exists(original_file_path):\n",
    "                    logging.error(f\"Original file {original_file_path} not found.\")\n",
    "                    continue\n",
    "\n",
    "                original_df = pd.read_csv(original_file_path)\n",
    "                imputed_df = pd.read_csv(file_path)\n",
    "\n",
    "                original_df = preprocess_dataframe(original_df)\n",
    "                imputed_df = preprocess_dataframe(imputed_df)\n",
    "\n",
    "                # Prendre les num_lines premières lignes\n",
    "                original_sample = original_df.head(num_lines)\n",
    "                imputed_sample = imputed_df.head(num_lines)\n",
    "\n",
    "                # Créer un DataFrame pour la comparaison\n",
    "                comparison_df = pd.DataFrame()\n",
    "\n",
    "                for i in range(num_lines):\n",
    "                    original_row = original_sample.iloc[i].rename(f\"Original_Ligne_{i+1}\")\n",
    "                    imputed_row = imputed_sample.iloc[i].rename(f\"Impute_Ligne_{i+1}\")\n",
    "                    comparison_df = pd.concat([comparison_df, original_row, imputed_row], axis=1)\n",
    "\n",
    "                print(f\"Comparaison des premières lignes pour {df_name} avec la méthode {method}:\")\n",
    "                display(comparison_df.T)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to process {file_path} due to {e}\")\n",
    "\n",
    "# Fonction d'aide pour afficher les DataFrames de manière plus claire\n",
    "def display(df):\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZcEYxEgSjMJ"
   },
   "outputs": [],
   "source": [
    "compare_original_and_imputed( originals_folder=LOCAL_EXPORT_COLUMNS_TYPES_CHECKED_FOLDER_PATH,\n",
    "        imputations_folder=LOCAL_EXPORT_IMPUTATION_FOLDER_PATH,\n",
    "        output_folder=LOCAL_EXPORT_IMPUTATION_RESULTS_FOLDER_PATH,\n",
    "        testing=TESTING_MODE,\n",
    "        testing_data_sub_folder=TESTING_MODE_SUB_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipid9VpxcnvA"
   },
   "source": [
    "## Outlier handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hT02ncNkd7P0"
   },
   "source": [
    "### Interquartile Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "af3Y_djRdxF6"
   },
   "outputs": [],
   "source": [
    "def identify_outliers_iqr(chunk):\n",
    "    outliers_dict = {}\n",
    "    for column in chunk.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = chunk[column].quantile(0.25)\n",
    "        Q3 = chunk[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers = chunk[(chunk[column] < lower_bound) | (chunk[column] > upper_bound)]\n",
    "        outliers_dict[column] = outliers\n",
    "    return outliers_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwqFaViAeA0f"
   },
   "source": [
    "### Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89IBJI_neDZJ"
   },
   "outputs": [],
   "source": [
    "def identify_outliers_z_score(chunk):\n",
    "    outliers_dict = {}\n",
    "    for column in chunk.select_dtypes(include=[np.number]).columns:\n",
    "        mean = chunk[column].mean()\n",
    "        std = chunk[column].std()\n",
    "        chunk['z_score'] = (chunk[column] - mean) / std\n",
    "\n",
    "        outliers = chunk[(chunk['z_score'] > 3) | (chunk['z_score'] < -3)]\n",
    "        chunk.drop(columns=['z_score'], inplace=True)\n",
    "        outliers_dict[column] = outliers\n",
    "    return outliers_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CtI_0vPgTGl"
   },
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAp7ESStgdNN"
   },
   "outputs": [],
   "source": [
    "def identify_outliers_isolation_forest(chunk):\n",
    "    outliers_dict = {}\n",
    "    for column in chunk.select_dtypes(include=[np.number]).columns:\n",
    "        model = IsolationForest(contamination=0.05)\n",
    "        chunk['anomaly'] = model.fit_predict(chunk[[column]])\n",
    "        outliers = chunk[chunk['anomaly'] == -1]\n",
    "        chunk.drop(columns=['anomaly'], inplace=True)\n",
    "        outliers_dict[column] = outliers\n",
    "    return outliers_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYvQY_gtgeyc"
   },
   "source": [
    "### DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N96UF5PFghdE"
   },
   "outputs": [],
   "source": [
    "def identify_outliers_dbscan(chunk):\n",
    "    outliers_dict = {}\n",
    "    for column in chunk.select_dtypes(include=[np.number]).columns:\n",
    "        model = DBSCAN(eps=3, min_samples=2)\n",
    "        chunk['cluster'] = model.fit_predict(chunk[[column]])\n",
    "        outliers = chunk[chunk['cluster'] == -1]\n",
    "        chunk.drop(columns=['cluster'], inplace=True)\n",
    "        outliers_dict[column] = outliers\n",
    "    return outliers_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsCaufj6gj9z"
   },
   "source": [
    "### LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DYzGwTqgmfs"
   },
   "outputs": [],
   "source": [
    "def identify_outliers_lof(chunk):\n",
    "    outliers_dict = {}\n",
    "    for column in chunk.select_dtypes(include=[np.number]).columns:\n",
    "        model = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "        chunk['lof'] = model.fit_predict(chunk[[column]])\n",
    "        outliers = chunk[chunk['lof'] == -1]\n",
    "        outliers_dict[column] = outliers\n",
    "    return outliers_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec0HyclVgov3"
   },
   "source": [
    "### MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVUtJ3qbgqPz"
   },
   "outputs": [],
   "source": [
    "def identify_outliers_mad(chunk):\n",
    "    outliers_dict = {}\n",
    "    for column in chunk.select_dtypes(include=[np.number]).columns:\n",
    "        median = chunk[column].median()\n",
    "        mad = (chunk[column] - median).abs().median()\n",
    "        threshold = 3 * mad\n",
    "        lower_bound = median - threshold\n",
    "        upper_bound = median + threshold\n",
    "\n",
    "        outliers = chunk[(chunk[column] < lower_bound) | (chunk[column] > upper_bound)]\n",
    "        outliers_dict[column] = outliers\n",
    "    return outliers_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V70hgU41gt3g"
   },
   "source": [
    "### Plot box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo-oE19jgv9C"
   },
   "outputs": [],
   "source": [
    "def plot_box(chunk, output_dir, file_name, chunk_num):\n",
    "    for column in chunk.select_dtypes(include=[np.number]).columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.boxplot(chunk[column])\n",
    "        plt.title(f'Box plot of {column} - {file_name} - Chunk {chunk_num}')\n",
    "        plt.ylabel(column)\n",
    "        plt.savefig(os.path.join(output_dir, f'boxplot_{file_name}_chunk{chunk_num}_{column}.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4YUaO3ijAOf"
   },
   "source": [
    "### Remove method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vP_6ulwojCZT"
   },
   "outputs": [],
   "source": [
    "def remove_outliers(chunk, outliers_dict):\n",
    "    for column, outliers in outliers_dict.items():\n",
    "        if not outliers.empty:\n",
    "            chunk = chunk[~chunk.index.isin(outliers.index)]\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ulEjlnPgyJ0"
   },
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZL2ghuhfwSb"
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, level=4, export_logs=False, log_folder_path='logs'):\n",
    "        self.log_level = level\n",
    "        self.export_logs = export_logs\n",
    "        self.log_folder_path = log_folder_path\n",
    "        self.log_messages = []\n",
    "\n",
    "    def log(self, message, level=4, color=None):\n",
    "        if level <= self.log_level:\n",
    "            if color:\n",
    "                print(colored(message, color))\n",
    "            else:\n",
    "                print(message)\n",
    "            self.log_messages.append(message)\n",
    "\n",
    "    def export(self, log_file_name='logs.txt'):\n",
    "        if self.export_logs:\n",
    "            log_file_path = os.path.join(self.log_folder_path, log_file_name)\n",
    "            os.makedirs(self.log_folder_path, exist_ok=True)\n",
    "            with open(log_file_path, 'w') as log_file:\n",
    "                for message in self.log_messages:\n",
    "                    log_file.write(message + '\\n')\n",
    "            print(colored(f'Logs exported to {log_file_path}', 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1BmrVMcgxxf"
   },
   "outputs": [],
   "source": [
    "def outliers_cleaning(input_folder, output_folder, chunk_size=100000, testing=False, max_rows=200, testing_data_sub_folder='testing_data', logger=None):\n",
    "    if logger is None:\n",
    "        logger = Logger()\n",
    "\n",
    "    if testing:\n",
    "        output_folder = os.path.join(output_folder, testing_data_sub_folder)\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    files_to_process = []\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        if testing and testing_data_sub_folder not in root:\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                files_to_process.append((root, file))\n",
    "\n",
    "    for root, file in tqdm(files_to_process, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        chunk_num = 0\n",
    "\n",
    "        logger.log(f'Starting processing of {file_path}', level=3)\n",
    "\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            if testing and chunk_num * chunk_size >= max_rows:\n",
    "                break\n",
    "\n",
    "            chunk_num += 1\n",
    "            logger.log(f'Processing chunk {chunk_num} of {file_name}', level=2)\n",
    "\n",
    "            # Dictionnaire pour stocker les chunks nettoyés par méthode\n",
    "            cleaned_chunks = {\n",
    "                'IQR': chunk.copy(),\n",
    "                'Z-Score': chunk.copy(),\n",
    "                'Isolation Forest': chunk.copy(),\n",
    "                'DBSCAN': chunk.copy(),\n",
    "                'LOF': chunk.copy(),\n",
    "                'MAD': chunk.copy()\n",
    "            }\n",
    "\n",
    "            # Identification et nettoyage des outliers pour chaque méthode\n",
    "            outliers_methods = {\n",
    "                'IQR': identify_outliers_iqr(chunk),\n",
    "                'Z-Score': identify_outliers_z_score(chunk),\n",
    "                'Isolation Forest': identify_outliers_isolation_forest(chunk),\n",
    "                'DBSCAN': identify_outliers_dbscan(chunk),\n",
    "                'LOF': identify_outliers_lof(chunk),\n",
    "                'MAD': identify_outliers_mad(chunk)\n",
    "            }\n",
    "\n",
    "            for method, outliers in outliers_methods.items():\n",
    "                all_outliers = pd.concat([df for df in [outliers.get(column) for column in outliers] if df is not None]).drop_duplicates()\n",
    "                cleaned_chunks[method] = chunk[~chunk.index.isin(all_outliers.index)]\n",
    "\n",
    "                relative_path = os.path.relpath(root, input_folder)\n",
    "                if testing and relative_path.startswith(testing_data_sub_folder):\n",
    "                    relative_path = relative_path[len(testing_data_sub_folder):].lstrip(os.sep)\n",
    "\n",
    "                # Dossiers de sortie pour les données nettoyées et les outliers\n",
    "                method_output_dir = os.path.join(output_folder, relative_path, method)\n",
    "                outliers_output_dir = os.path.join(method_output_dir, 'outliers')\n",
    "\n",
    "                if not os.path.exists(method_output_dir):\n",
    "                    os.makedirs(method_output_dir)\n",
    "                if not os.path.exists(outliers_output_dir):\n",
    "                    os.makedirs(outliers_output_dir)\n",
    "\n",
    "                # Export du chunk nettoyé\n",
    "                output_csv_path = os.path.join(method_output_dir, f'{file_name}')\n",
    "                cleaned_chunks[method].to_csv(output_csv_path, index=False)\n",
    "\n",
    "                # Export des outliers\n",
    "                outliers_csv_path = os.path.join(outliers_output_dir, f'{file_name}')\n",
    "                all_outliers.to_csv(outliers_csv_path, index=False)\n",
    "\n",
    "                if file_name not in results:\n",
    "                    results[file_name] = {\n",
    "                        'IQR': 0,\n",
    "                        'Z-Score': 0,\n",
    "                        'Isolation Forest': 0,\n",
    "                        'DBSCAN': 0,\n",
    "                        'LOF': 0,\n",
    "                        'MAD': 0\n",
    "                    }\n",
    "\n",
    "                results[file_name][method] += len(all_outliers)\n",
    "\n",
    "            logger.log(f'Finished processing chunk {chunk_num} of {file_name}', level=2)\n",
    "\n",
    "            del chunk\n",
    "            del cleaned_chunks\n",
    "            del outliers_methods\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSHK6lswhjwE"
   },
   "outputs": [],
   "source": [
    "app_train_validated = pd.read_csv('/content/exports/imputed_without_outliers/testing_data/bfill/IQR/application_train.csv')\n",
    "app_train_outliers = pd.read_csv('/content/exports/imputed_without_outliers/testing_data/bfill/IQR/application_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKKPueh_iEC6"
   },
   "outputs": [],
   "source": [
    "app_train_validated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yUNXetlh6MT"
   },
   "outputs": [],
   "source": [
    "app_train_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrpQ4qSfjkLE"
   },
   "source": [
    "### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mRGwTRAjlvB"
   },
   "outputs": [],
   "source": [
    "outliers_results = outliers_cleaning(\n",
    "  input_folder=LOCAL_EXPORT_IMPUTATION_FOLDER_PATH,\n",
    "  output_folder=LOCAL_EXPORT_IMPUTED_WITHOUT_OUTLIERS_FOLDER_PATH,\n",
    "  chunk_size=GENERAL_CHUNK_SIZE,\n",
    "  testing=TESTING_MODE,\n",
    "  max_rows=TESTING_MODE_MAX_LINES,\n",
    "  testing_data_sub_folder=TESTING_MODE_SUB_FOLDER_NAME)\n",
    "outliers_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcdXkqktjjiM"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCT4SGwWc6y9"
   },
   "source": [
    "## Duplicate managment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL71ZBRzQQZP"
   },
   "source": [
    "### Duplicate managment method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypUxPjDKUizk"
   },
   "outputs": [],
   "source": [
    "for root, _, files in os.walk(os.path.join(LOCAL_EXPORT_IMPUTED_WITHOUT_OUTLIERS_FOLDER_PATH, 'testing_data')):\n",
    "  print(root)\n",
    "  # print(_)\n",
    "  print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MR0WYX7iXDUz"
   },
   "outputs": [],
   "source": [
    "def without_duplicates(base_dir, taille_chunk, dossier_export, mode_test=False, max_rows=1000, testing_data_sub_folder_name='testing_data'):\n",
    "    fichiers = {}\n",
    "    base_search_dir = os.path.join(base_dir, testing_data_sub_folder_name) if mode_test else base_dir\n",
    "\n",
    "    for root, _, files in os.walk(base_search_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv') and \"/outliers\" not in root:  # Ignorer les fichiers outliers\n",
    "                chemin_complet = os.path.join(root, file)\n",
    "                cle = os.path.relpath(root, base_search_dir)\n",
    "                if cle not in fichiers:\n",
    "                    fichiers[cle] = []\n",
    "                fichiers[cle].append(chemin_complet)\n",
    "                logging.info(f'Fichier ajouté: {chemin_complet} sous clé: {cle}')\n",
    "\n",
    "    # Log pour vérifier les fichiers trouvés\n",
    "    logging.info(f'Fichiers trouvés: {fichiers.items()}')\n",
    "\n",
    "    try:\n",
    "        for cle, chemins in fichiers.items():\n",
    "            df_complet = pd.DataFrame()\n",
    "            unique_rows = set()\n",
    "            for fichier in chemins:\n",
    "                try:\n",
    "                    logging.info(f'Traitement du fichier: {fichier}')\n",
    "                    reader = pd.read_csv(fichier, chunksize=taille_chunk)\n",
    "                    total_rows = sum(1 for row in open(fichier)) - 1  # Nombre total de lignes (moins l'en-tête)\n",
    "                    pbar = tqdm(total=total_rows, desc=f'Traitement de {fichier}')\n",
    "\n",
    "                    for chunk in reader:\n",
    "                        chunk_tuples = [tuple(row) for row in chunk.to_numpy()]\n",
    "                        new_rows = [row for row, row_tuple in zip(chunk.values, chunk_tuples) if row_tuple not in unique_rows]\n",
    "                        unique_rows.update(chunk_tuples)\n",
    "                        if new_rows:\n",
    "                            df_complet = pd.concat([df_complet, pd.DataFrame(new_rows, columns=chunk.columns)], ignore_index=True)\n",
    "                        pbar.update(len(chunk))\n",
    "                        if mode_test and len(df_complet) >= max_rows:\n",
    "                            break\n",
    "\n",
    "                    pbar.close()\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f'Erreur lors du traitement de {fichier}: {str(e)}')\n",
    "                    if 'pbar' in locals():\n",
    "                        pbar.close()\n",
    "\n",
    "            dossier_export_final = os.path.join(dossier_export, testing_data_sub_folder_name, cle) if mode_test else os.path.join(dossier_export, cle)\n",
    "\n",
    "            if not os.path.exists(dossier_export_final):\n",
    "                os.makedirs(dossier_export_final)\n",
    "\n",
    "            nom_fichier_export = os.path.join(dossier_export_final, f\"{os.path.basename(fichier)}\")\n",
    "            df_complet.to_csv(nom_fichier_export, index=False)\n",
    "            logging.info(f'Traitement de {cle} terminé avec succès. Fichier exporté : {nom_fichier_export}')\n",
    "\n",
    "        print(\"Traitement des fichiers et exportation terminés.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.critical(f'Erreur critique: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVM2CWmgXG7m"
   },
   "source": [
    "### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-n_u-dpXGQn"
   },
   "outputs": [],
   "source": [
    "without_duplicates(\n",
    "  base_dir=LOCAL_EXPORT_IMPUTED_WITHOUT_OUTLIERS_FOLDER_PATH,\n",
    "  taille_chunk=GENERAL_CHUNK_SIZE,\n",
    "  dossier_export=LOCAL_EXPORT_IMPUTED_WITHOUT_DUPLICATES_FOLDER_PATH,\n",
    "  mode_test=TESTING_MODE,\n",
    "  max_rows=TESTING_MODE_MAX_LINES,\n",
    "  testing_data_sub_folder_name=TESTING_MODE_SUB_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Qse_EIoaXcS"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQCIPF9uaahY"
   },
   "source": [
    "### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfWKvVaQaWde"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # Conversion des Jours en Années\n",
    "    if 'DAYS_BIRTH' in df.columns:\n",
    "        df['YEARS_BIRTH'] = df['DAYS_BIRTH'] / -365\n",
    "    if 'DAYS_EMPLOYED' in df.columns:\n",
    "        df['YEARS_EMPLOYED'] = df['DAYS_EMPLOYED'] / -365\n",
    "    if 'DAYS_ID_PUBLISH' in df.columns:\n",
    "        df['YEARS_ID_PUBLISH'] = df['DAYS_ID_PUBLISH'] / -365\n",
    "    if 'DAYS_LAST_PHONE_CHANGE' in df.columns:\n",
    "        df['YEARS_LAST_PHONE_CHANGE'] = df['DAYS_LAST_PHONE_CHANGE'] / -365\n",
    "\n",
    "    # Interaction entre Âge et Durée de l'Emploi\n",
    "    if 'YEARS_EMPLOYED' in df.columns and 'YEARS_BIRTH' in df.columns:\n",
    "        df['AGE_EMPLOYED_RATIO'] = df['YEARS_EMPLOYED'] / df['YEARS_BIRTH']\n",
    "\n",
    "    # Ratios Financiers\n",
    "    if 'AMT_CREDIT' in df.columns and 'AMT_INCOME_TOTAL' in df.columns:\n",
    "        df['CREDIT_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "\n",
    "    # Durée des Crédits Restants\n",
    "    if 'DAYS_CREDIT_ENDDATE' in df.columns:\n",
    "        df['CREDIT_DURATION'] = df['DAYS_CREDIT_ENDDATE'] / -365\n",
    "    if 'DAYS_CREDIT_UPDATE' in df.columns:\n",
    "        df['CREDIT_UPDATE_DURATION'] = df['DAYS_CREDIT_UPDATE'] / -365\n",
    "\n",
    "    # Historique de Demandes de Crédit\n",
    "    credit_request_columns = ['AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "                              'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON',\n",
    "                              'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "    if any(col in df.columns for col in credit_request_columns):\n",
    "        df['TOTAL_CREDIT_BUREAU_REQUESTS'] = df[credit_request_columns].sum(axis=1)\n",
    "\n",
    "    # Indicateurs Basés sur la Connaissance du Domaine\n",
    "    if 'CNT_CHILDREN' in df.columns and 'CNT_FAM_MEMBERS' in df.columns:\n",
    "        df['CHILDREN_RATIO'] = df['CNT_CHILDREN'] / (df['CNT_FAM_MEMBERS'] + 1)\n",
    "    if 'AMT_INCOME_TOTAL' in df.columns and 'CNT_FAM_MEMBERS' in df.columns:\n",
    "        df['INCOME_PER_FAMILY_MEMBER'] = df['AMT_INCOME_TOTAL'] / (df['CNT_FAM_MEMBERS'] + 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_in_chunks(input_file, output_file, chunk_size=10000):\n",
    "    columns_to_check = [\n",
    "        'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE',\n",
    "        'YEARS_EMPLOYED', 'YEARS_BIRTH', 'AMT_CREDIT', 'AMT_INCOME_TOTAL',\n",
    "        'DAYS_CREDIT_ENDDATE', 'DAYS_CREDIT_UPDATE', 'CNT_CHILDREN', 'CNT_FAM_MEMBERS',\n",
    "        'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "        'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR'\n",
    "    ]\n",
    "\n",
    "    column_exists = False\n",
    "    chunks = []\n",
    "\n",
    "    # Read in chunks, apply feature engineering if applicable\n",
    "    for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "        if any(col in chunk.columns for col in columns_to_check):\n",
    "            column_exists = True\n",
    "            chunk = feature_engineering(chunk)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    # Concatenate chunks and write to output\n",
    "    df_full = pd.concat(chunks, ignore_index=True)\n",
    "    df_full.to_csv(output_file, index=False)\n",
    "\n",
    "    # If no relevant columns exist, copy the original file to ensure no data is lost\n",
    "    if not column_exists:\n",
    "        df_full.to_csv(output_file, index=False)\n",
    "\n",
    "def process_feature_engineering_to_all_files(base_dir, output_dir, chunk_size=10000, mode_test=False, testing_data_sub_folder_name='testing_data'):\n",
    "    base_search_dir = os.path.join(base_dir, testing_data_sub_folder_name) if mode_test else base_dir\n",
    "\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(base_search_dir):\n",
    "        for file in files:\n",
    "            chemin_complet = os.path.join(root, file)\n",
    "            if file.endswith('.csv') and \"/outliers\" not in chemin_complet:  # Ignorer les fichiers outliers\n",
    "                cle = os.path.relpath(root, base_search_dir)\n",
    "                output_sub_dir = os.path.join(output_dir, testing_data_sub_folder_name, cle) if mode_test else os.path.join(output_dir, cle)\n",
    "                input_file = chemin_complet\n",
    "                output_file = os.path.join(output_sub_dir, f\"{os.path.splitext(file)[0]}.csv\")\n",
    "                all_files.append((input_file, output_file, output_sub_dir))\n",
    "\n",
    "    with tqdm(total=len(all_files), desc=\"Traitement des fichiers\") as pbar:\n",
    "        for input_file, output_file, output_sub_dir in all_files:\n",
    "            if not os.path.exists(output_sub_dir):\n",
    "                os.makedirs(output_sub_dir)\n",
    "            logging.info(f'Traitement du fichier: {input_file}')\n",
    "            process_in_chunks(input_file, output_file, chunk_size)\n",
    "            logging.info(f'Traitement de {input_file} terminé avec succès. Fichier exporté : {output_file}')\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(\"Traitement des fichiers et exportation terminés.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96edq20Zab6T"
   },
   "source": [
    "### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlEeJW8Fac7P"
   },
   "outputs": [],
   "source": [
    "process_feature_engineering_to_all_files(base_dir=LOCAL_EXPORT_IMPUTED_WITHOUT_OUTLIERS_FOLDER_PATH,\n",
    "                  output_dir=LOCAL_EXPORT_FEATURE_ENGINEERED_FOLDER_PATH,\n",
    "                  chunk_size=GENERAL_CHUNK_SIZE,\n",
    "                  mode_test=TESTING_MODE,\n",
    "                  testing_data_sub_folder_name=TESTING_MODE_SUB_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xL72y7wPdifU"
   },
   "outputs": [],
   "source": [
    "pd.read_csv('/content/exports/imputed_without_outliers/testing_data/bfill/LOF/application_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2k9hZ93dLaK"
   },
   "outputs": [],
   "source": [
    "pd.read_csv('/content/exports/imputed_without_duplicates/testing_data/bfill/DBSCAN/application_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fK7gF9z4bpK5"
   },
   "outputs": [],
   "source": [
    "pd.read_csv('/content/exports/feature_engineered/testing_data/bfill/LOF/application_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1kIV59zktL5"
   },
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQTeP-byZ5Ze"
   },
   "outputs": [],
   "source": [
    "def aggregate_numerical(dataframe, group_by_column):\n",
    "    \"\"\"\n",
    "    Aggregates numerical columns in the dataframe by calculating the mean, grouped by the specified column.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): The input dataframe containing numerical columns to be aggregated.\n",
    "    group_by_column (str): The column name to group by.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new dataframe with the mean of numerical columns grouped by the specified column.\n",
    "    \"\"\"\n",
    "    # Select numerical columns, excluding the group_by_column\n",
    "    numerical_columns = dataframe.select_dtypes(include=['number']).columns\n",
    "    numerical_columns = [col for col in numerical_columns if col != group_by_column]\n",
    "    return dataframe.groupby(group_by_column)[numerical_columns].mean().reset_index()\n",
    "\n",
    "def merge_data(application_train, application_test, bureau, bureau_balance, previous_application, pos_cash_balance, installments_payments, credit_card_balance):\n",
    "    \"\"\"\n",
    "    Merges multiple dataframes into a single training and testing dataframe.\n",
    "\n",
    "    Args:\n",
    "    application_train (pd.DataFrame): Training application dataframe.\n",
    "    application_test (pd.DataFrame): Testing application dataframe.\n",
    "    bureau (pd.DataFrame): Bureau dataframe.\n",
    "    bureau_balance (pd.DataFrame): Bureau balance dataframe.\n",
    "    previous_application (pd.DataFrame): Previous application dataframe.\n",
    "    pos_cash_balance (pd.DataFrame): POS cash balance dataframe.\n",
    "    installments_payments (pd.DataFrame): Installments payments dataframe.\n",
    "    credit_card_balance (pd.DataFrame): Credit card balance dataframe.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing merged training dataframe, merged testing dataframe, and aggregated previous application dataframe.\n",
    "    \"\"\"\n",
    "    # Aggregate and merge bureau_balance with bureau\n",
    "    bureau_balance_agg = aggregate_numerical(bureau_balance, 'SK_ID_BUREAU')\n",
    "    bureau_balance_agg = bureau_balance_agg.add_suffix('_bureau_balance')\n",
    "    bureau_balance_agg.rename(columns={'SK_ID_BUREAU_bureau_balance': 'SK_ID_BUREAU'}, inplace=True)\n",
    "    bureau = bureau.merge(bureau_balance_agg, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "    # Ensure SK_ID_CURR is present before merging\n",
    "    if 'SK_ID_CURR' not in application_train.columns or 'SK_ID_CURR' not in bureau.columns:\n",
    "        raise KeyError(\"SK_ID_CURR column is missing in one of the dataframes.\")\n",
    "\n",
    "    # Merge application_train and application_test with bureau data\n",
    "    train_merged = application_train.merge(bureau, on='SK_ID_CURR', how='left', suffixes=('', '_bureau'))\n",
    "    test_merged = application_test.merge(bureau, on='SK_ID_CURR', how='left', suffixes=('', '_bureau'))\n",
    "\n",
    "    # Aggregate other balance and payment data by SK_ID_PREV\n",
    "    pos_cash_balance_agg = aggregate_numerical(pos_cash_balance, 'SK_ID_PREV')\n",
    "    installments_payments_agg = aggregate_numerical(installments_payments, 'SK_ID_PREV')\n",
    "    credit_card_balance_agg = aggregate_numerical(credit_card_balance, 'SK_ID_PREV')\n",
    "\n",
    "    # Merge aggregated data with previous_application\n",
    "    previous_application = previous_application.merge(pos_cash_balance_agg, on='SK_ID_PREV', how='left', suffixes=('', '_pos_cash'))\n",
    "    previous_application = previous_application.merge(installments_payments_agg, on='SK_ID_PREV', how='left', suffixes=('', '_installments'))\n",
    "    previous_application = previous_application.merge(credit_card_balance_agg, on='SK_ID_PREV', how='left', suffixes=('', '_credit_card'))\n",
    "\n",
    "    # Aggregate previous_application data by SK_ID_CURR\n",
    "    previous_application_agg = aggregate_numerical(previous_application, 'SK_ID_CURR')\n",
    "\n",
    "    # Merge aggregated previous_application data with train and test data\n",
    "    train_merged = train_merged.merge(previous_application_agg, on='SK_ID_CURR', how='left', suffixes=('', '_prev_app'))\n",
    "    test_merged = test_merged.merge(previous_application_agg, on='SK_ID_CURR', how='left', suffixes=('', '_prev_app'))\n",
    "\n",
    "    return train_merged, test_merged, previous_application_agg\n",
    "\n",
    "def export_to_csv_in_chunks(dataframe, filepath, chunksize):\n",
    "    \"\"\"\n",
    "    Exports a dataframe to a CSV file in chunks to manage memory usage.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): The dataframe to be exported.\n",
    "    filepath (str): The path where the CSV file will be saved.\n",
    "    chunksize (int): The number of rows per chunk to write at a time.\n",
    "    \"\"\"\n",
    "    dataframe.to_csv(filepath, index=False, chunksize=chunksize)\n",
    "\n",
    "def merge_and_export(input_base_path, output_base_path, testing=False, max_rows=None, chunk_size=10000, testing_data_sub_folder_name='test'):\n",
    "    \"\"\"\n",
    "    Merges multiple dataframes from the specified input path and exports the merged dataframes to the specified output path.\n",
    "\n",
    "    Args:\n",
    "    input_base_path (str): The base path where the input CSV files are located.\n",
    "    output_base_path (str): The base path where the output CSV files will be saved.\n",
    "    testing (bool): Whether to activate testing mode (limit the number of rows and modify output paths).\n",
    "    max_rows (int): The maximum number of rows to read from each input file (only applicable in testing mode).\n",
    "    chunk_size (int): The number of rows per chunk to write at a time.\n",
    "    testing_data_sub_folder_name (str): The sub-folder name to use in testing mode for saving output files.\n",
    "    \"\"\"\n",
    "    # Walk through the input directory to process CSV files\n",
    "    for root, dirs, files in tqdm(os.walk(input_base_path), desc=\"Processing directories\"):\n",
    "        if \"/outliers\" not in root:\n",
    "            dataframes = {}\n",
    "            # Read each CSV file and store in the dataframes dictionary\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    if testing and max_rows:\n",
    "                        df = df.head(max_rows)\n",
    "                    if 'bureau_balance' in file:\n",
    "                        dataframes['bureau_balance'] = df\n",
    "                    elif 'bureau' in file:\n",
    "                        dataframes['bureau'] = df\n",
    "                    elif 'POS_CASH_balance' in file:\n",
    "                        dataframes['POS_CASH_balance'] = df\n",
    "                    elif 'previous_application' in file:\n",
    "                        dataframes['previous_application'] = df\n",
    "                    elif 'credit_card_balance' in file:\n",
    "                        dataframes['credit_card_balance'] = df\n",
    "                    elif 'installments_payments' in file:\n",
    "                        dataframes['installments_payments'] = df\n",
    "                    elif 'application_train' in file:\n",
    "                        dataframes['application_train'] = df\n",
    "                    elif 'application_test' in file:\n",
    "                        dataframes['application_test'] = df\n",
    "\n",
    "            # Check if all required dataframes are present\n",
    "            required_keys = ['bureau', 'bureau_balance', 'POS_CASH_balance', 'previous_application', 'credit_card_balance', 'installments_payments', 'application_train', 'application_test']\n",
    "            if not all(key in dataframes for key in required_keys):\n",
    "                continue\n",
    "\n",
    "            bureau = dataframes['bureau']\n",
    "            bureau_balance = dataframes['bureau_balance']\n",
    "            POS_CASH_balance = dataframes['POS_CASH_balance']\n",
    "            previous_application = dataframes['previous_application']\n",
    "            credit_card_balance = dataframes['credit_card_balance']\n",
    "            installments_payments = dataframes['installments_payments']\n",
    "            app_train = dataframes['application_train']\n",
    "            app_test = dataframes['application_test']\n",
    "\n",
    "            # Merge dataframes\n",
    "            application_train_merged, application_test_merged, previous_application_agg = merge_data(app_train, app_test, bureau, bureau_balance, previous_application, POS_CASH_balance, installments_payments, credit_card_balance)\n",
    "\n",
    "            # Extract methods for naming the output files\n",
    "            imputation_method = root.split('/')[-2]\n",
    "            outlier_method = root.split('/')[-1]\n",
    "            # encoded_method = root.split('/')[-2]\n",
    "\n",
    "            # Modify path logic to avoid additional \"test\" folder\n",
    "            if testing:\n",
    "                print(f\"output_base_path={output_base_path}\")\n",
    "                print(f\"testing_data_sub_folder_name={testing_data_sub_folder_name}\")\n",
    "                print(f\"imputation_method={imputation_method}\")\n",
    "                print(f\"outlier_method={outlier_method}\")\n",
    "\n",
    "                output_train_path = os.path.join(output_base_path, testing_data_sub_folder_name, imputation_method, outlier_method, 'application_train.csv')\n",
    "                output_test_path = os.path.join(output_base_path, testing_data_sub_folder_name, imputation_method, outlier_method, 'application_test.csv')\n",
    "            else:\n",
    "                output_train_path = os.path.join(output_base_path, imputation_method, outlier_method, 'application_train.csv')\n",
    "                output_test_path = os.path.join(output_base_path, imputation_method, outlier_method, 'application_test.csv')\n",
    "\n",
    "            # Create directories if they do not exist\n",
    "            os.makedirs(os.path.dirname(output_train_path), exist_ok=True)\n",
    "            # Export merged training data\n",
    "            export_to_csv_in_chunks(application_train_merged, output_train_path, chunk_size)\n",
    "\n",
    "            # Clean up to free memory\n",
    "            del app_train, application_train_merged\n",
    "            gc.collect()\n",
    "\n",
    "            # Merge testing data in chunks\n",
    "            application_test_merged = app_test.merge(previous_application_agg, on='SK_ID_CURR', how='left', suffixes=('', '_prev_app'))\n",
    "\n",
    "            # Create directories if they do not exist\n",
    "            os.makedirs(os.path.dirname(output_test_path), exist_ok=True)\n",
    "            # Export merged testing data\n",
    "            export_to_csv_in_chunks(application_test_merged, output_test_path, chunk_size)\n",
    "\n",
    "            # Clean up to free memory\n",
    "            del app_test, application_test_merged\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpB4_SvfJ6nP"
   },
   "outputs": [],
   "source": [
    "dbcscan_bureau = pd.read_csv(\"/content/exports/imputed_without_outliers/testing_data/bfill/DBSCAN/bureau.csv\")\n",
    "application_test = pd.read_csv(\"/content/exports/imputed_without_outliers/testing_data/bfill/DBSCAN/application_test_processed.csv\")\n",
    "application_train = pd.read_csv(\"/content/exports/imputed_without_outliers/testing_data/bfill/DBSCAN/application_train_processed.csv\")\n",
    "credit_card_balance = pd.read_csv(\"/content/exports/imputed_without_outliers/testing_data/bfill/DBSCAN/credit_card_balance_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0r3Q90OKBjt"
   },
   "outputs": [],
   "source": [
    "dbcscan_bureau.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XW_gCCb7fYm9"
   },
   "outputs": [],
   "source": [
    "application_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKIkJTDHfaE3"
   },
   "outputs": [],
   "source": [
    "application_train.head()\n",
    "credit_card_balance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGRbHNSwkwWX"
   },
   "source": [
    "### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nPJ_swaZ7kB"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "merge_and_export(input_base_path=LOCAL_EXPORT_FEATURE_ENGINEERED_FOLDER_PATH, output_base_path=LOCAL_EXPORT_MERGED_DATAFRAMES_FOLDER_PATH, testing=TESTING_MODE, max_rows=TESTING_MODE_MAX_LINES, chunk_size=GENERAL_CHUNK_SIZE, testing_data_sub_folder_name=TESTING_MODE_SUB_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "95627792-157e-457a-88a8-3b3875c7e1d5",
    "_uuid": "46f5bf9a6de52e270aa911ffd895e704da5426ec",
    "id": "NErq_3mGyLXQ"
   },
   "source": [
    "## Label Encoding and One-Hot Encoding\n",
    "\n",
    "Let's implement the policy described above: for any categorical variable (`dtype == object`) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding.\n",
    "\n",
    "For label encoding, we use the Scikit-Learn `LabelEncoder` and for one-hot encoding, the pandas `get_dummies(df)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13KwrvMoOoWn"
   },
   "outputs": [],
   "source": [
    "def encode_and_align_chunked(input_dir, output_dir, target_columns=None, encoding_methods=['onehot'], chunk_size=10000, test_mode=False, max_rows=10000, testing_sub_folder_name=\"test_output\"):\n",
    "    \"\"\"\n",
    "    Processes and encodes chunks of CSV files, aligning training and testing data and applying specified encoding methods.\n",
    "\n",
    "    Args:\n",
    "    input_dir (str): The directory containing the input CSV files.\n",
    "    output_dir (str): The directory where the processed CSV files will be saved.\n",
    "    target_columns (list): List of target columns for encoding methods that require targets.\n",
    "    encoding_methods (list): List of encoding methods to apply (e.g., ['onehot', 'label']).\n",
    "    chunk_size (int): The number of rows per chunk to process at a time.\n",
    "    test_mode (bool): Whether to limit the number of rows for testing purposes.\n",
    "    max_rows (int): The maximum number of rows to process when test_mode is True.\n",
    "    testing_sub_folder_name (str): The sub-folder name for saving outputs in testing mode.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    def process_file(train_file, test_file, output_train_file, output_test_file, encoding_method, pbar):\n",
    "        \"\"\"\n",
    "        Processes a pair of training and testing files, applying encoding and alignment.\n",
    "\n",
    "        Args:\n",
    "        train_file (str): Path to the training CSV file.\n",
    "        test_file (str): Path to the testing CSV file.\n",
    "        output_train_file (str): Path to save the processed training CSV file.\n",
    "        output_test_file (str): Path to save the processed testing CSV file.\n",
    "        encoding_method (str): The encoding method to apply.\n",
    "        pbar (tqdm): Progress bar instance for tracking progress.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        train_header_written = False\n",
    "        test_header_written = False\n",
    "\n",
    "        # Initialize encoders\n",
    "        le = LabelEncoder()\n",
    "        oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        be = ce.BinaryEncoder()\n",
    "        he = ce.HashingEncoder()\n",
    "        loo = ce.LeaveOneOutEncoder()\n",
    "        woe = ce.WOEEncoder()\n",
    "        pe = ce.PolynomialEncoder()\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "        # Dictionary to hold target labels if target columns are specified\n",
    "        if target_columns:\n",
    "            train_labels_list = {target: [] for target in target_columns}\n",
    "            print(f\"Dictionary to hold target labels initialized: {train_labels_list}\")\n",
    "\n",
    "        try:\n",
    "            chunk_train_iterator = pd.read_csv(train_file, chunksize=chunk_size, on_bad_lines='skip')\n",
    "            chunk_test_iterator = pd.read_csv(test_file, chunksize=chunk_size, on_bad_lines='skip')\n",
    "\n",
    "            rows_processed = 0\n",
    "\n",
    "            for chunk_train, chunk_test in zip(chunk_train_iterator, chunk_test_iterator):\n",
    "                if test_mode and rows_processed >= max_rows:\n",
    "                    break\n",
    "\n",
    "                if chunk_train.empty or chunk_test.empty:\n",
    "                    continue\n",
    "\n",
    "                # Drop columns with all NaN values\n",
    "                chunk_train.dropna(axis=1, how='all', inplace=True)\n",
    "                chunk_test.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "                # Ensure both train and test chunks have common columns, excluding target columns from alignment\n",
    "                common_columns = list(set(chunk_train.columns).intersection(set(chunk_test.columns)))\n",
    "\n",
    "                if not common_columns:\n",
    "                    continue\n",
    "\n",
    "                chunk_train_common = chunk_train[common_columns]\n",
    "                chunk_test_common = chunk_test[common_columns]\n",
    "\n",
    "                # Print column names to debug\n",
    "                print(f\"Columns in training chunk from file {train_file}: {chunk_train.columns.tolist()}\")\n",
    "                print(f\"Columns in testing chunk from file {test_file}: {chunk_test.columns.tolist()}\")\n",
    "\n",
    "                # Verify presence of target columns\n",
    "                if target_columns:\n",
    "                    for target in target_columns:\n",
    "                        if target not in chunk_train.columns:\n",
    "                            raise ValueError(f\"Target column '{target}' not found in training data from file: {train_file}\")\n",
    "                    print(f\"Target columns verified: {target_columns}\")\n",
    "                    for target in target_columns:\n",
    "                        print(f\"First values of target column '{target}' in training data: {chunk_train[target].head().values}\")\n",
    "\n",
    "                # Apply encoding methods\n",
    "                if encoding_method == 'label':\n",
    "                    for col in chunk_train_common.columns:\n",
    "                        if chunk_train_common[col].dtype == 'object' or chunk_train_common[col].dtype.name == 'category':\n",
    "                            le.fit(chunk_train_common[col])\n",
    "                            chunk_train_common[col] = le.transform(chunk_train_common[col])\n",
    "                            chunk_test_common[col] = chunk_test_common[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
    "                            le.classes_ = np.append(le.classes_, '<unknown>')\n",
    "                            chunk_test_common[col] = le.transform(chunk_test_common[col])\n",
    "\n",
    "                elif encoding_method == 'onehot':\n",
    "                    categorical_columns = chunk_train_common.select_dtypes(include=['object']).columns\n",
    "                    if len(categorical_columns) > 0:\n",
    "                        ohe.fit(chunk_train_common[categorical_columns])\n",
    "                        train_encoded = pd.DataFrame(ohe.transform(chunk_train_common[categorical_columns]), columns=ohe.get_feature_names_out())\n",
    "                        test_encoded = pd.DataFrame(ohe.transform(chunk_test_common[categorical_columns]), columns=ohe.get_feature_names_out())\n",
    "                        chunk_train_common = chunk_train_common.drop(columns=categorical_columns)\n",
    "                        chunk_test_common = chunk_test_common.drop(columns=categorical_columns)\n",
    "                        chunk_train_common = pd.concat([chunk_train_common, train_encoded], axis=1)\n",
    "                        chunk_test_common = pd.concat([chunk_test_common, test_encoded], axis=1)\n",
    "\n",
    "                elif encoding_method == 'ordinal':\n",
    "                    categorical_columns = chunk_train_common.select_dtypes(include=['object']).columns\n",
    "                    chunk_train_common[categorical_columns] = oe.fit_transform(chunk_train_common[categorical_columns])\n",
    "                    chunk_test_common[categorical_columns] = oe.transform(chunk_test_common[categorical_columns])\n",
    "\n",
    "                elif encoding_method == 'frequency':\n",
    "                    for col in chunk_train_common.columns:\n",
    "                        if chunk_train_common[col].dtype == 'object':\n",
    "                            freq = chunk_train_common[col].value_counts() / len(chunk_train_common)\n",
    "                            chunk_train_common[col] = chunk_train_common[col].map(freq)\n",
    "                            chunk_test_common[col] = chunk_test_common[col].map(freq).fillna(0)\n",
    "\n",
    "                elif encoding_method == 'target':\n",
    "                    if target_columns:\n",
    "                        mean_target = chunk_train.groupby(target_columns).mean()\n",
    "                        for col in chunk_train_common.columns:\n",
    "                            if chunk_train_common[col].dtype == 'object':\n",
    "                                chunk_train_common[col] = chunk_train_common[col].map(mean_target[target_columns])\n",
    "                                chunk_test_common[col] = chunk_test_common[col].map(mean_target[target_columns]).fillna(chunk_train_common[col].mean())\n",
    "\n",
    "                elif encoding_method == 'binary':\n",
    "                    chunk_train_common = be.fit_transform(chunk_train_common)\n",
    "                    chunk_test_common = be.transform(chunk_test_common)\n",
    "\n",
    "                elif encoding_method == 'hashing':\n",
    "                    chunk_train_common = he.fit_transform(chunk_train_common)\n",
    "                    chunk_test_common = he.transform(chunk_test_common)\n",
    "\n",
    "                elif encoding_method == 'leaveoneout':\n",
    "                    if target_columns:\n",
    "                        chunk_train_common = loo.fit_transform(chunk_train_common, chunk_train[target_columns])\n",
    "                        chunk_test_common = loo.transform(chunk_test_common)\n",
    "\n",
    "                elif encoding_method == 'woe':\n",
    "                    if target_columns:\n",
    "                        chunk_train_common = woe.fit_transform(chunk_train_common, chunk_train[target_columns])\n",
    "                        chunk_test_common = woe.transform(chunk_test_common)\n",
    "\n",
    "                elif encoding_method == 'polynomial':\n",
    "                    chunk_train_common = pe.fit_transform(chunk_train_common)\n",
    "                    chunk_test_common = pe.transform(chunk_test_common)\n",
    "\n",
    "                # Collect target labels if specified\n",
    "                if target_columns:\n",
    "                    for target in target_columns:\n",
    "                        if target in chunk_train.columns:\n",
    "                            train_labels = chunk_train[target]\n",
    "                            train_labels_list[target].append(train_labels)\n",
    "\n",
    "                # Align train and test chunks to ensure they have the same columns\n",
    "                chunk_train_common, chunk_test_common = chunk_train_common.align(chunk_test_common, join='inner', axis=1)\n",
    "\n",
    "                # Drop columns with all NaN values after alignment\n",
    "                chunk_train_common.dropna(axis=1, how='all', inplace=True)\n",
    "                chunk_test_common.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "                if not all(chunk_train_common.columns == chunk_test_common.columns):\n",
    "                    continue\n",
    "\n",
    "                if chunk_train_common.empty or chunk_test_common.empty:\n",
    "                    continue\n",
    "\n",
    "                # Combine the common columns with the target columns for the training set\n",
    "                if target_columns:\n",
    "                    chunk_train = pd.concat([chunk_train_common, chunk_train[target_columns]], axis=1)\n",
    "                else:\n",
    "                    chunk_train = chunk_train_common\n",
    "\n",
    "                chunk_train.to_csv(output_train_file, mode='a', index=False, header=not train_header_written)\n",
    "                chunk_test_common.to_csv(output_test_file, mode='a', index=False, header=not test_header_written)\n",
    "                train_header_written = True\n",
    "                test_header_written = True\n",
    "\n",
    "                rows_processed += len(chunk_train_common)\n",
    "                pbar.update(len(chunk_train_common))\n",
    "                del chunk_train, chunk_test_common\n",
    "                gc.collect()\n",
    "\n",
    "            # If target columns are specified, append them to the output train file\n",
    "            if target_columns:\n",
    "                train_data = pd.read_csv(output_train_file, on_bad_lines='skip')\n",
    "                for target in target_columns:\n",
    "                    if target in train_labels_list:\n",
    "                        target_train = pd.concat(train_labels_list[target], ignore_index=True)\n",
    "                        train_data[target] = target_train\n",
    "                train_data.to_csv(output_train_file, index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Calculate total files for progress bar\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(input_dir) if any('application_train.csv' in file or 'application_test.csv' in file for file in files)])\n",
    "    print(f\"Total files to process: {total_files}\")\n",
    "    pbar = tqdm(total=total_files * chunk_size, desc=\"Processing files\")\n",
    "\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        print(f\"Checking directory: {root}\")\n",
    "        if \"/outliers\" not in root:\n",
    "            train_file = None\n",
    "            test_file = None\n",
    "\n",
    "            for file in files:\n",
    "                # Identify train and test files\n",
    "                if 'application_train.csv' in file:\n",
    "                    train_file = file\n",
    "                elif 'application_test.csv' in file:\n",
    "                    test_file = file\n",
    "\n",
    "            if not train_file or not test_file:\n",
    "                continue\n",
    "\n",
    "            input_train_file = os.path.join(root, train_file)\n",
    "            input_test_file = os.path.join(root, test_file)\n",
    "\n",
    "            for encoding_method in encoding_methods:\n",
    "                imputation_method = root.split('/')[-2]\n",
    "                outlier_method = root.split('/')[-1]\n",
    "\n",
    "                # Adjust path logic based on test mode\n",
    "                if test_mode:\n",
    "                    output_sub_dir = os.path.join(output_dir, testing_sub_folder_name, imputation_method, outlier_method, encoding_method)\n",
    "                else:\n",
    "                    output_sub_dir = os.path.join(output_dir, imputation_method, outlier_method, encoding_method)\n",
    "\n",
    "                # Ensure there are no duplicate subdirectory names\n",
    "                output_sub_dir = os.path.normpath(output_sub_dir)\n",
    "                os.makedirs(output_sub_dir, exist_ok=True)\n",
    "\n",
    "                output_train_file = os.path.join(output_sub_dir, train_file)\n",
    "                output_test_file = os.path.join(output_sub_dir, test_file)\n",
    "\n",
    "                # Check if files exist before processing\n",
    "                if not os.path.exists(input_train_file) or not os.path.exists(input_test_file):\n",
    "                    print(f\"File not found: {input_train_file} or {input_test_file}\")\n",
    "                    continue\n",
    "\n",
    "                process_file(input_train_file, input_test_file, output_train_file, output_test_file, encoding_method, pbar)\n",
    "\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfaaZN_pbH6T"
   },
   "source": [
    "### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIKUvmUobJL0"
   },
   "outputs": [],
   "source": [
    "encode_and_align_chunked(\n",
    "    input_dir=LOCAL_EXPORT_MERGED_DATAFRAMES_FOLDER_PATH,\n",
    "    output_dir=LOCAL_EXPORT_ENCODED_AND_ALIGNED_FOLDER_PATH,\n",
    "    target_columns=[\"TARGET\"],\n",
    "    encoding_methods=[\"onehot\", \"label\", \"ordinal\"],  # spécifiez les types d'encodage souhaités\n",
    "    chunk_size=GENERAL_CHUNK_SIZE,\n",
    "    test_mode=TESTING_MODE,\n",
    "    max_rows=TESTING_MODE_MAX_LINES,\n",
    "    testing_sub_folder_name=TESTING_MODE_SUB_FOLDER_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMIzu95qtX5M"
   },
   "outputs": [],
   "source": [
    "test_encoding_prev = pd.read_csv(\"/content/exports/encoded_and_aligned/testing_data/Isolation Forest/ffill/onehot/application_test_merged.csv\")\n",
    "test_encoding_prev_label = pd.read_csv(\"/content/exports/encoded_and_aligned/testing_data/Isolation Forest/ffill/label/application_test_merged.csv\")\n",
    "test_encoding_prev_ordinal = pd.read_csv(\"/content/exports/encoded_and_aligned/testing_data/Isolation Forest/ffill/ordinal/application_test_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "759LOKc8tfl7"
   },
   "outputs": [],
   "source": [
    "test_encoding_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxQNhJs4tx3d"
   },
   "outputs": [],
   "source": [
    "test_encoding_prev_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DGTC9ASt4J7"
   },
   "outputs": [],
   "source": [
    "test_encoding_prev_ordinal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "13918211-0e6b-4d72-955b-f997db19eea2",
    "_uuid": "4d7c8dd1d5bb5a0ef84cb78e6bff927249e62145",
    "id": "y8w_ZvgbyLXQ"
   },
   "source": [
    "## Back to Exploratory Data Analysis\n",
    "\n",
    "### Anomalies\n",
    "\n",
    "One problem we always want to be on the lookout for when doing EDA is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies quantitatively is by looking at the statistics of a column using the `describe` method. The numbers in the `DAYS_BIRTH` column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a60be93c2d7d63855e6d65c1109f408ad85da134",
    "id": "UdtrPidDyLXR"
   },
   "outputs": [],
   "source": [
    "(app_train['DAYS_BIRTH'] / -365).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "acb37a3e3f2e0b2fd581259788b9255398314157",
    "id": "_95tYT2xyLXR"
   },
   "source": [
    "Those ages look reasonable. There are no outliers for the age on either the high or low end. How about the days of employment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "600c59dd5d970d3ccfea3a6af0036d85958adc91",
    "id": "RXZ46vWSyLXR"
   },
   "outputs": [],
   "source": [
    "app_train['DAYS_EMPLOYED'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1cdd9dafce28e497e08062cd3b189ac353c04cd9",
    "id": "yo98-Q4VyLXR"
   },
   "source": [
    "That doesn't look right! The maximum value (besides being positive) is about 1000 years!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2878bfb3a2be4554f33e03e1a04d4c1978b52a08",
    "id": "obQef_OQyLXR"
   },
   "outputs": [],
   "source": [
    "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
    "plt.xlabel('Days Employment');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d28ca1e799c0a6113cc5e920297e1dc93d380af4",
    "id": "9rBoypmFyLXR"
   },
   "source": [
    "Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKAKPCxHfZ_k"
   },
   "source": [
    "## Manual check patch after encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2n3eCY1fgoz"
   },
   "source": [
    "### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKQv9d7ad_vt"
   },
   "outputs": [],
   "source": [
    "def handle_anomalies(df, column, anomaly_value):\n",
    "    # Create an anomalous flag column\n",
    "    # df[f'{column}_ANOM'] = df[column] == anomaly_value\n",
    "    print(f\"Anomalies found in column {column}: {(df[f'{column}'] == anomaly_value).sum()}\")\n",
    "    # Replace the anomalous values with np.nan\n",
    "    df[column].replace({anomaly_value: 0}, inplace=True)\n",
    "    print(f\"Anomalies found in column after patch {column}: {(df[f'{column}'] == anomaly_value).sum()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_files_in_directory(base_path, anomaly_column, anomaly_value, output_folder, max_rows=None, testing=False, testing_sub_path_name=\"testing_data\"):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path, nrows=max_rows)\n",
    "                # Handle anomalies\n",
    "                df = handle_anomalies(df, anomaly_column, anomaly_value)\n",
    "\n",
    "                # Determine the output path\n",
    "                relative_path = os.path.relpath(root, base_path)\n",
    "                imputation_method = root.split('/')[-3]\n",
    "                outlier_method = root.split('/')[-2]\n",
    "                encoding_method = root.split('/')[-1]\n",
    "\n",
    "                if testing:\n",
    "                    output_dir = os.path.join(output_folder, testing_sub_path_name, imputation_method, outlier_method, encoding_method)\n",
    "                else:\n",
    "                    output_dir = os.path.join(output_folder, imputation_method, outlier_method, encoding_method)\n",
    "\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                output_file_path = os.path.join(output_dir, file)\n",
    "\n",
    "                # Save the processed DataFrame\n",
    "                df.to_csv(output_file_path, index=False)\n",
    "                print(f\"Saved processed file to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVGyC0mgfehh"
   },
   "source": [
    "### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7feGVaOafWTk"
   },
   "outputs": [],
   "source": [
    "process_files_in_directory(\n",
    "        base_path=LOCAL_EXPORT_ENCODED_AND_ALIGNED_FOLDER_PATH,\n",
    "        anomaly_column=\"DAYS_EMPLOYED\",\n",
    "        anomaly_value=365243,\n",
    "        output_folder=LOCAL_EXPORT_MANUAL_CHECK_PATCH_FOLDER_PATH,\n",
    "        max_rows=TESTING_MODE_MAX_LINES,  # Set to None to process all rows\n",
    "        testing=TESTING_MODE,\n",
    "        testing_sub_path_name=TESTING_MODE_SUB_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1edfcf786aadb004f083e9896989a29e43bf80da",
    "id": "4t3vPD6DyLXU"
   },
   "source": [
    "Well that is extremely interesting! It turns out that the anomalies have a lower rate of default.\n",
    "\n",
    "Handling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (`np.nan`) and then create a new boolean column indicating whether or not the value was anomalous.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e23ec3cb89428f3dd994b572f718cc729740cfab",
    "id": "-GlgLNzcyLXU"
   },
   "outputs": [],
   "source": [
    "# Create an anomalous flag column\n",
    "app_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "# Replace the anomalous values with nan\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
    "plt.xlabel('Days Employment');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "839595437c0721e2480f6b4ee58f3060b222f166",
    "id": "2P6vCrc1yLXU"
   },
   "source": [
    "The distribution looks to be much more in line with what we would expect, and we also have created a new column to tell the model that these values were originally anomalous (becuase we will have to fill in the nans with some value, probably the median of the column). The other columns with `DAYS` in the dataframe look to be about what we expect with no obvious outliers.\n",
    "\n",
    "As an extremely important note, anything we do to the training data we also have to do to the testing data. Let's make sure to create the new column and fill in the existing column with `np.nan` in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0d7c77b2adecaa878f39cf86ffddcfbbe51a190",
    "id": "P4OaSAkEyLXV"
   },
   "outputs": [],
   "source": [
    "app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "print('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd656b392faad3b34ecfa448b55ad03e75449e0a",
    "id": "nBpTk-8zyLXV"
   },
   "source": [
    "### Correlations\n",
    "\n",
    "Now that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the `.corr` dataframe method.\n",
    "\n",
    "The correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some [general interpretations of the absolute value of the correlation coefficent](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf) are:\n",
    "\n",
    "\n",
    "* .00-.19 “very weak”\n",
    "*  .20-.39 “weak”\n",
    "*  .40-.59 “moderate”\n",
    "*  .60-.79 “strong”\n",
    "* .80-1.0 “very strong”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "02acdb8d-d95f-41b9-8ad1-e2b6cb26f398",
    "_uuid": "d39d15d64db1f2c9015c6f542911ef9a9cac119e",
    "id": "5wPP8_HhyLXV"
   },
   "outputs": [],
   "source": [
    "# Find correlations with the target and sort\n",
    "correlations = app_train.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display correlations\n",
    "print('Most Positive Correlations:\\n', correlations.tail(15))\n",
    "print('\\nMost Negative Correlations:\\n', correlations.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yAcdfPFrTv4"
   },
   "outputs": [],
   "source": [
    "correlations.to_csv(\"correlations_extrac.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8cfa409c-ec74-4fa4-8093-e7d00596c9c5",
    "_uuid": "67e1f0f22ec8e26c38827c24ca1e9409d73c9c64",
    "id": "eQgGgcYcyLXV"
   },
   "source": [
    "Let's take a look at some of more significant correlations: the `DAYS_BIRTH` is the most positive correlation. (except for `TARGET` because the correlation of a variable with itself is always 1!) Looking at the documentation, `DAYS_BIRTH` is the age in days of the client at the time of the loan in negative days (for whatever reason!). The correlation is positive, but the value of this feature is actually negative, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). That's a little confusing, so we will take the absolute value of the feature and then the correlation will be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f7b1cfb-9e5c-4720-9618-ad326940f3f3",
    "_uuid": "c1b831b6d1c3221efb123fbc1a4882aa1f598ec0",
    "id": "KbAj0znoyLXV"
   },
   "source": [
    "### Effect of Age on Repayment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b0ab583c-dfbb-4ff7-80e5-d747fc408499",
    "_uuid": "f705c7aa49486ec3bf119c4edc4e4af58861b88d",
    "id": "s9KM0tuSyLXV"
   },
   "outputs": [],
   "source": [
    "# Find the correlation of the positive days since birth and target\n",
    "app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\n",
    "app_train['DAYS_BIRTH'].corr(app_train['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3fde277c-ebf1-4eaf-a353-c18fc4b518a6",
    "_uuid": "2b95e2c33bdd50682e7105d0f27b9cc3ad5b482d",
    "id": "aLSaNAJkyLXV"
   },
   "source": [
    "As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often.\n",
    "\n",
    "Let's start looking at this variable. First, we can make a histogram of the age. We will put the x axis in years to make the plot a little more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35e36393-e388-488e-ba7a-7473169d3e6f",
    "_uuid": "739226c4594130d6aabeb25ffb8742c37657d7a4",
    "id": "LGuGqMlayLXW"
   },
   "outputs": [],
   "source": [
    "# Set the style of plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the distribution of ages in years\n",
    "plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\n",
    "plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "02f5d3c5-e527-430b-a38d-531aeb8f3dd1",
    "_uuid": "340680b4a4ecf310a6369808157b17cac7c13461",
    "id": "wtIOLfBEyLXW"
   },
   "source": [
    "By itself, the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. To visualize the effect of the age on the target, we will next make a [kernel density estimation plot](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) colored by the value of the target. A [kernel density estimate plot shows the distribution of a single variable](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn `kdeplot` for this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3982a18f-2731-4bb2-80c9-831b2377421f",
    "_uuid": "2e045e65f048789b577477356df4337c9e5e2087",
    "id": "R0uNcLS3yLXW"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# KDE plot of loans that were repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n",
    "\n",
    "# KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n",
    "\n",
    "# Labeling of plot\n",
    "plt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9749e164-efea-47d2-ab60-5a8b89ff0570",
    "_uuid": "57757e02285b8067b61e3f586174ad64bec78ac1",
    "id": "GBqSNjvUyLXW"
   },
   "source": [
    "The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket.\n",
    "\n",
    "To make this graph, first we `cut` the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4296e926-7245-40df-bb0a-f6e59d8e566a",
    "_uuid": "6c50572f095bff250bfed1993e2c53118277b5dd",
    "id": "qV-GWMbtyLXW"
   },
   "outputs": [],
   "source": [
    "# Age information into a separate dataframe\n",
    "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
    "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n",
    "\n",
    "# Bin the age data\n",
    "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n",
    "age_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "18873d6b-3877-4c77-830e-0f3e10e5e7fb",
    "_uuid": "7082483e5fd9114856926de28968e5ae0b478b36",
    "id": "iYMhZYOSyLXW"
   },
   "outputs": [],
   "source": [
    "# Group by the bin and calculate averages\n",
    "age_groups  = age_data.groupby('YEARS_BINNED').mean()\n",
    "age_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "004d1021-d73f-4356-9ef8-0464c95d1708",
    "_uuid": "823b5032f472b05ce079ae5a7680389f31ddd8b7",
    "id": "VccGoRogyLXW"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Graph the age bins and the average of the target as a bar plot\n",
    "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
    "\n",
    "# Plot labeling\n",
    "plt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\n",
    "plt.title('Failure to Repay by Age Group');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2dad060f-bcab-4fe3-aa19-29fbf3e6fdab",
    "_uuid": "eb2bd6392ed6d6f7e002bc8dbea6aab0f30487d9",
    "id": "q4411QOyyLXW"
   },
   "source": [
    "There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group.\n",
    "\n",
    "This is information that could be directly used by the bank: because younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This does not mean the bank should discriminate against younger clients, but it would be smart to take precautionary measures to help younger clients pay on time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4749204f-ec63-4eeb-8d25-9c80967348f1",
    "_uuid": "43a3bb87bdaa65509e9dc887492239ae06cd1c77",
    "id": "o8d5b04WyLXX"
   },
   "source": [
    "### Exterior Sources\n",
    "\n",
    "The 3 variables with the strongest negative correlations with the target are `EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3`.\n",
    "According to the documentation, these features represent a \"normalized score from external data source\". I'm not sure what this exactly means, but it may be a cumulative sort of credit rating made using numerous sources of data.\n",
    "\n",
    "Let's take a look at these variables.\n",
    "\n",
    "First, we can show the correlations of the `EXT_SOURCE` features with the target and with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e2ab3b7f-3a53-4495-a1de-31ad287f032a",
    "_uuid": "6197819149feaff75176e64e54c65ea6be3864fe",
    "id": "O6TzG1cLyLXX"
   },
   "outputs": [],
   "source": [
    "# Extract the EXT_SOURCE variables and show correlations\n",
    "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "ext_data_corrs = ext_data.corr()\n",
    "ext_data_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0479863d-cfa9-47ab-83e6-7d7877e3e939",
    "_uuid": "20b21a6b4e15a726c29596abeb01346dc416729c",
    "id": "vKFgs98ayLXX"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "\n",
    "# Heatmap of correlations\n",
    "sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
    "plt.title('Correlation Heatmap');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78bd5acc-003d-4795-a57a-a6c4fc9c8c5f",
    "_uuid": "6a592aa7c01858b268489ccb8fd00690cd26cd58",
    "id": "p4FtMjL0yLXX"
   },
   "source": [
    "All three `EXT_SOURCE` featureshave negative correlations with the target, indicating that as the value of the `EXT_SOURCE` increases, the client is more likely to repay the loan. We can also see that `DAYS_BIRTH` is positively correlated with `EXT_SOURCE_1` indicating that maybe one of the factors in this score is the client age.\n",
    "\n",
    "Next we can look at the distribution of each of these features colored by the value of the target. This will let us visualize the effect of this variable on the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5e2b6507-96d1-4f96-964f-d8241e321f09",
    "_uuid": "49afab6b3790abcc2dea04c483f462f39e536503",
    "id": "3EdEo9_byLXX"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 12))\n",
    "\n",
    "# iterate through the sources\n",
    "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
    "\n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n",
    "\n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % source)\n",
    "    plt.xlabel('%s' % source); plt.ylabel('Density');\n",
    "\n",
    "plt.tight_layout(h_pad = 2.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0ee531e8-f131-4ae3-b542-d4bf550d9bd5",
    "_uuid": "71ce5855665256dacfd7c52bceb11c68f5c58759",
    "id": "yYMaS7nIyLXX"
   },
   "source": [
    "`EXT_SOURCE_3` displays the greatest difference between the values of the target. We can clearly see that this feature has some relationship to the likelihood of an applicant to repay a loan. The relationship is not very strong (in fact they are all [considered very weak](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf), but these variables will still be useful for a machine learning model to predict whether or not an applicant will repay a loan on time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "53f486249f8afec0496d3de25120e57d956c2eb7",
    "id": "6L1w2hEmyLXX"
   },
   "source": [
    "## Pairs Plot\n",
    "\n",
    "As a final exploratory plot, we can make a pairs plot of the `EXT_SOURCE` variables and the `DAYS_BIRTH` variable. The [Pairs Plot](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166) is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle.\n",
    "\n",
    "If you don't understand this code, that's all right! Plotting in Python can be overly complex, and for anything beyond the simplest graphs, I usually find an existing implementation and adapt the code (don't repeat yourself)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7b185a4e-ac04-4ff2-b5cb-46eacf6a70b6",
    "_uuid": "9400f9d2810f4331005c9b91e040818279d1eaf8",
    "id": "ksziClDiyLXX"
   },
   "outputs": [],
   "source": [
    "# Copy the data for plotting\n",
    "plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n",
    "\n",
    "# Add in the age of the client in years\n",
    "plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
    "\n",
    "# Drop na values and limit to first 100000 rows\n",
    "plot_data = plot_data.dropna().loc[:100000, :]\n",
    "\n",
    "# Function to calculate correlation coefficient between two columns\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.2, .8), xycoords=ax.transAxes,\n",
    "                size = 20)\n",
    "\n",
    "# Create the pairgrid object\n",
    "grid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n",
    "                    hue = 'TARGET',\n",
    "                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n",
    "\n",
    "# Upper is a scatter plot\n",
    "grid.map_upper(plt.scatter, alpha = 0.2)\n",
    "\n",
    "# Diagonal is a histogram\n",
    "grid.map_diag(sns.kdeplot)\n",
    "\n",
    "# Bottom is density plot\n",
    "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
    "\n",
    "plt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "839f51f5-02f4-472d-9de4-aa2f760c171c",
    "_uuid": "88f9f486c74856bd87ff7699998088d9ee7fd926",
    "id": "HFokX8ZqyLXY"
   },
   "source": [
    "In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the `EXT_SOURCE_1` and the `DAYS_BIRTH` (or equivalently `YEARS_BIRTH`), indicating that this feature may take into account the age of the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8zwbnc54KwG"
   },
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    'DAYS_BIRTH',\n",
    "    'DAYS_EMPLOYED',\n",
    "    'NAME_FAMILY_STATUS',\n",
    "    'CODE_GENDER',\n",
    "    'CNT_CHILDREN',\n",
    "    'CNT_FAM_MEMBERS',\n",
    "    'NAME_INCOME_TYPE',\n",
    "    'OCCUPATION_TYPE',\n",
    "    'DAYS_CREDIT',\n",
    "    'DAYS_CREDIT_ENDDATE',\n",
    "    'DAYS_CREDIT_UPDATE',\n",
    "    'DAYS_DECISION',\n",
    "    'AMT_CREDIT',\n",
    "    'AMT_ANNUITY',\n",
    "    'AMT_GOODS_PRICE',\n",
    "    'DAYS_LAST_DUE',\n",
    "    'DAYS_LAST_DUE_1ST_VERSION',\n",
    "    'DAYS_TERMINATION',\n",
    "    'DAYS_ID_PUBLISH',\n",
    "    'DAYS_INSTALMENT',\n",
    "    'DAYS_LAST_PHONE_CHANGE',\n",
    "    'CNT_INSTALMENT',\n",
    "    'CNT_INSTALMENT_FUTURE',\n",
    "    'CNT_INSTALMENT_MATURE_CUM',\n",
    "    'CNT_PAYMENT',\n",
    "    'SK_DPD',\n",
    "    'SK_DPD_DEF',\n",
    "    'CREDIT_DAY_OVERDUE',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT_SUM',\n",
    "    'AMT_CREDIT_SUM_DEBT',\n",
    "    'AMT_CREDIT_SUM_LIMIT',\n",
    "    'AMT_CREDIT_SUM_OVERDUE',\n",
    "    'AMT_REQ_CREDIT_BUREAU_YEAR',\n",
    "    'REGION_RATING_CLIENT',\n",
    "    'REGION_RATING_CLIENT_W_CITY',\n",
    "    'REG_CITY_NOT_WORK_CITY',\n",
    "    'REG_CITY_NOT_LIVE_CITY',\n",
    "    'REG_REGION_NOT_WORK_REGION',\n",
    "    'REG_REGION_NOT_LIVE_REGION',\n",
    "    'LIVE_CITY_NOT_WORK_CITY',\n",
    "    'LIVE_REGION_NOT_WORK_REGION',\n",
    "    'FLAG_OWN_REALTY',\n",
    "    'OWN_CAR_AGE',\n",
    "    'FLAG_DOCUMENT_3',\n",
    "    'FLAG_EMP_PHONE',\n",
    "    'FLAG_WORK_PHONE',\n",
    "    'AGE_EMPLOYED_RATIO',\n",
    "    'CREDIT_INCOME_RATIO',\n",
    "    'ANNUITY_INCOME_RATIO',\n",
    "    'CREDIT_ANNUITY_RATIO',\n",
    "    'CHILDREN_RATIO',\n",
    "    'INCOME_PER_FAMILY_MEMBER'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLvGBjiGndxt"
   },
   "source": [
    "# Entraîner un modèle de forêt aléatoire pour obtenir l'importance des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cESl3ovpPUk"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNsbDP29qNPf"
   },
   "outputs": [],
   "source": [
    "dfs_test = pd.read_csv(f\"{LOCAL_EXPORT_FOLDER_PATH}/application_train_merged.csv\", chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jk9mFDP1qnp_"
   },
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(dfs_test):\n",
    "    if i == 1001:\n",
    "        break\n",
    "    model.fit(chunk.drop(columns=['TARGET']), chunk['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WdHHGfhoCv-"
   },
   "outputs": [],
   "source": [
    "chunksize = 10000  # Définir la taille du lot\n",
    "for i, chunk in enumerate(pd.read_csv(f\"{LOCAL_EXPORT_FOLDER_PATH}/application_train_merged.csv\", chunksize=chunksize)):\n",
    "    # Traitement du lot\n",
    "    # processed_chunk = process(chunk)\n",
    "    model.fit(app_train.drop(columns=['TARGET']), app_train['TARGET'])\n",
    "\n",
    "    # Exporter le lot traité en CSV\n",
    "    # processed_chunk.to_csv(f'${LOCAL_EXPORT_FOLDER_PATH}/processed_chunk_{i}.csv', index=False)\n",
    "\n",
    "    # Libérer la mémoire\n",
    "    del chunk\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBxj9iS4nf7v"
   },
   "outputs": [],
   "source": [
    "function training(app_train):\n",
    "    model.fit(app_train.drop(columns=['TARGET']), app_train['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7MBXQfPnmPS"
   },
   "source": [
    "# Importance des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgCTbdDinlGC"
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(model.feature_importances_, index=app_train.drop(columns=['TARGET']).columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvmQHtO-o641"
   },
   "source": [
    "\n",
    "# Entraîner le modèle avec les nouvelles features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2xVhVnko1rp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XHqXzA_oxtk"
   },
   "outputs": [],
   "source": [
    "features = app_train.drop(columns=['TARGET'])\n",
    "target = app_train['TARGET']\n",
    "model = RandomForestClassifier()\n",
    "scores = cross_val_score(model, features, target, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"Mean ROC AUC Score: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZS_NXiolfr0Q"
   },
   "source": [
    "\n",
    "Étape 4: Validation et Évaluation\n",
    "Valider et évaluer l'impact des nouvelles features sur la performance du modèle.\n",
    "\n",
    "4.1 Validation Croisée\n",
    "Utiliser la validation croisée pour évaluer la performance du modèle.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "\n",
    "\n",
    "features = app_train.drop(columns=['TARGET'])\n",
    "target = app_train['TARGET']\n",
    "model = RandomForestClassifier()\n",
    "scores = cross_val_score(model, features, target, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"Mean ROC AUC Score: {scores.mean()}\")\n",
    "4.2 Évaluation de la Performance\n",
    "Comparer les performances du modèle avec et sans les nouvelles features.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Modèle avec les nouvelles features\n",
    "model_with_features = RandomForestClassifier()\n",
    "model_with_features.fit(features, target)\n",
    "scores_with_features = cross_val_score(model_with_features, features, target, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Modèle sans les nouvelles features\n",
    "selected_features = ['DAYS_BIRTH', 'DAYS_EMPLOYED', 'AMT_CREDIT', 'AMT_ANNUITY', 'SK_DPD', 'CREDIT_DAY_OVERDUE', 'AMT_INCOME_TOTAL']\n",
    "features_without = app_train[selected_features]\n",
    "model_without_features = RandomForestClassifier()\n",
    "model_without_features.fit(features_without, target)\n",
    "scores_without_features = cross_val_score(model_without_features, features_without, target, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"Mean ROC AUC Score with features: {scores_with_features.mean()}\")\n",
    "print(f\"Mean ROC AUC Score without features: {scores_without_features.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "464705a1-7ecf-47ba-a1fe-9f870102eb85",
    "_uuid": "70322dd11709dcaaf879a56103fde8fc787b7d4c",
    "id": "s6ZV7oMFyLXY"
   },
   "source": [
    "## Polynomial Features\n",
    "\n",
    "One simple feature construction method is called [polynomial features](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). In this method, we make features that are powers of existing features as well as interaction terms between existing features. For example, we can create variables `EXT_SOURCE_1^2` and `EXT_SOURCE_2^2` and also variables such as `EXT_SOURCE_1` x `EXT_SOURCE_2`, `EXT_SOURCE_1` x `EXT_SOURCE_2^2`, `EXT_SOURCE_1^2` x   `EXT_SOURCE_2^2`, and so on. These features that are a combination of multiple individual variables are called [interaction terms](https://en.wikipedia.org/wiki/Interaction_(statistics) because they  capture the interactions between variables. In other words, while two variables by themselves  may not have a strong influence on the target, combining them together into a single interaction variable might show a relationship with the target. [Interaction terms are commonly used in statistical models](https://www.theanalysisfactor.com/interpreting-interactions-in-regression/) to capture the effects of multiple variables, but I do not see them used as often in machine learning. Nonetheless, we can try out a few to see if they might help our model to predict whether or not a client will repay a loan.\n",
    "\n",
    "Jake VanderPlas writes about [polynomial features in his excellent book Python for Data Science](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html) for those who want more information.\n",
    "\n",
    "In the following code, we create polynomial features using the `EXT_SOURCE` variables and the `DAYS_BIRTH` variable. [Scikit-Learn has a useful class called `PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) that creates the polynomials and the interaction terms up to a specified degree. We can use a degree of 3 to see the results (when we are creating polynomial features, we want to avoid using too high of a degree, both because the number of features scales exponentially with the degree, and because we can run into [problems with overfitting](http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHV2cO9A53cj"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_polynomial_features(input_file_train, input_file_test, output_file_train, output_file_test, features, degree=3, chunk_size=10000, target_column=None, id_column=None, add_features=True):\n",
    "    train_header_written = False\n",
    "    test_header_written = False\n",
    "\n",
    "    # Imputer pour gérer les valeurs manquantes\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Calculer le nombre total de lignes pour la barre de progression\n",
    "    total_lines_train = sum(1 for _ in open(input_file_train)) - 1  # -1 pour l'en-tête\n",
    "    total_lines_test = sum(1 for _ in open(input_file_test)) - 1  # -1 pour l'en-tête\n",
    "\n",
    "    # Lire les fichiers en chunks\n",
    "    chunk_train_iterator = pd.read_csv(input_file_train, chunksize=chunk_size)\n",
    "    chunk_test_iterator = pd.read_csv(input_file_test, chunksize=chunk_size)\n",
    "\n",
    "    with tqdm(total=total_lines_train, desc=\"Traitement des chunks\") as pbar:\n",
    "        for chunk_train, chunk_test in zip(chunk_train_iterator, chunk_test_iterator):\n",
    "            # Vérifier la présence des colonnes de features\n",
    "            missing_features_train = [feature for feature in features if feature not in chunk_train.columns]\n",
    "            missing_features_test = [feature for feature in features if feature not in chunk_test.columns]\n",
    "\n",
    "            if missing_features_train:\n",
    "                print(f\"Colonnes manquantes dans le chunk d'entraînement : {missing_features_train}\")\n",
    "                print(f\"Colonnes disponibles dans le chunk d'entraînement : {chunk_train.columns.to_list()}\")\n",
    "                pbar.update(len(chunk_train))\n",
    "                continue\n",
    "\n",
    "            if missing_features_test:\n",
    "                print(f\"Colonnes manquantes dans le chunk de test : {missing_features_test}\")\n",
    "                print(f\"Colonnes disponibles dans le chunk de test : {chunk_test.columns.to_list()}\")\n",
    "                pbar.update(len(chunk_train))\n",
    "                continue\n",
    "\n",
    "            # Séparer les features et la colonne cible si elle est spécifiée\n",
    "            target_train = None\n",
    "            if target_column and target_column in chunk_train.columns:\n",
    "                target_train = chunk_train[target_column]\n",
    "                chunk_train = chunk_train.drop(columns=[target_column])\n",
    "\n",
    "            # Séparer la colonne ID si elle est spécifiée\n",
    "            id_train = id_test = None\n",
    "            if id_column and id_column in chunk_train.columns:\n",
    "                id_train = chunk_train[id_column]\n",
    "                id_test = chunk_test[id_column]\n",
    "                chunk_train = chunk_train.drop(columns=[id_column])\n",
    "                chunk_test = chunk_test.drop(columns=[id_column])\n",
    "\n",
    "            # Sélectionner les features à transformer\n",
    "            chunk_train_features = chunk_train[features]\n",
    "            chunk_test_features = chunk_test[features]\n",
    "\n",
    "            # Vérifier que les DataFrames de features ne sont pas vides\n",
    "            if chunk_train_features.shape[1] == 0 or chunk_test_features.shape[1] == 0:\n",
    "                print(\"Les DataFrames de features sont vides après la sélection des colonnes spécifiées. Chunk ignoré.\")\n",
    "                pbar.update(len(chunk_train))\n",
    "                continue\n",
    "\n",
    "            # Imputer les valeurs manquantes\n",
    "            chunk_train_features = imputer.fit_transform(chunk_train_features)\n",
    "            chunk_test_features = imputer.transform(chunk_test_features)\n",
    "\n",
    "            # Créer l'objet PolynomialFeatures avec le degré spécifié\n",
    "            poly_transformer = PolynomialFeatures(degree=degree)\n",
    "\n",
    "            try:\n",
    "                # Entraîner et transformer les features\n",
    "                poly_train_features = poly_transformer.fit_transform(chunk_train_features)\n",
    "                poly_test_features = poly_transformer.transform(chunk_test_features)\n",
    "            except ValueError as e:\n",
    "                print(f\"Erreur lors de la transformation polynomiale : {e}\")\n",
    "                print(\"Chunk ignoré.\")\n",
    "                pbar.update(len(chunk_train))\n",
    "                continue\n",
    "\n",
    "            # Créer les DataFrames avec les nouvelles features polynomiales\n",
    "            poly_feature_names = poly_transformer.get_feature_names_out(features)\n",
    "            poly_train_features = pd.DataFrame(poly_train_features, columns=poly_feature_names)\n",
    "            poly_test_features = pd.DataFrame(poly_test_features, columns=poly_feature_names)\n",
    "\n",
    "            if add_features:\n",
    "                # Ajouter les nouvelles features polynomiales aux DataFrames originaux\n",
    "                chunk_train = pd.concat([chunk_train, poly_train_features], axis=1)\n",
    "                chunk_test = pd.concat([chunk_test, poly_test_features], axis=1)\n",
    "            else:\n",
    "                # Remplacer les features existantes par les nouvelles features polynomiales\n",
    "                chunk_train = poly_train_features\n",
    "                chunk_test = poly_test_features\n",
    "\n",
    "            # Réinsérer les colonnes ID si elles étaient spécifiées\n",
    "            if id_column and id_train is not None:\n",
    "                chunk_train[id_column] = id_train\n",
    "                chunk_test[id_column] = id_test\n",
    "\n",
    "            # Réinsérer la colonne cible si elle était spécifiée\n",
    "            if target_column and target_train is not None:\n",
    "                chunk_train[target_column] = target_train\n",
    "\n",
    "            # Exporter les chunks avec les features polynomiales\n",
    "            chunk_train.to_csv(output_file_train, mode='a', index=False, header=not train_header_written)\n",
    "            chunk_test.to_csv(output_file_test, mode='a', index=False, header=not test_header_written)\n",
    "            train_header_written = True\n",
    "            test_header_written = True\n",
    "\n",
    "            # Nettoyage de la mémoire\n",
    "            del poly_train_features, poly_test_features, chunk_train, chunk_test\n",
    "            gc.collect()\n",
    "\n",
    "            # Mettre à jour la barre de progression\n",
    "            pbar.update(chunk_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDFLXWzO559-"
   },
   "outputs": [],
   "source": [
    "features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AK8JP35y76KN"
   },
   "outputs": [],
   "source": [
    "input_file_tain_to_poly = f'{LOCAL_EXPORT_FOLDER_PATH}/application_train_merged_encoded.csv'\n",
    "input_file_test_to_poly = f'{LOCAL_EXPORT_FOLDER_PATH}/application_test_merged_encoded.csv'\n",
    "output_file_train_poly = f'{LOCAL_EXPORT_FOLDER_PATH}/application_train_merged_with_new_features_encoded_poly_add.csv'\n",
    "output_file_test_poly = f'{LOCAL_EXPORT_FOLDER_PATH}/application_test_merged_with_new_features_encoded_poly_add.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBJDHnf99uCe"
   },
   "outputs": [],
   "source": [
    "target_column = 'TARGET'\n",
    "id_column = 'SK_ID_CURR'\n",
    "generate_polynomial_features(input_file_tain_to_poly, input_file_test_to_poly, output_file_train_poly, output_file_test_poly, features, degree=3, target_column=target_column, id_column=id_column, add_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQK4AVUeCVPv"
   },
   "source": [
    "### Verify of correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zy2lnpuCCX2K"
   },
   "outputs": [],
   "source": [
    "# Calcul des corrélations avec plusieurs colonnes cibles\n",
    "def calculate_correlations_with_targets(df, target_columns):\n",
    "    correlations = {}\n",
    "    for target in target_columns:\n",
    "        correlations[target] = df.corr()[target].abs().sort_values(ascending=False)\n",
    "    return correlations\n",
    "\n",
    "# Sélection des top features par corrélation avec plusieurs colonnes cibles\n",
    "def select_top_features_by_correlation(df, target_columns, top_n):\n",
    "    all_corrs = calculate_correlations_with_targets(df, target_columns)\n",
    "    top_features = set()\n",
    "    for target, corrs in all_corrs.items():\n",
    "        top_features.update(corrs.index[1:top_n+1])  # Exclure la colonne cible\n",
    "    return df[list(top_features)]\n",
    "\n",
    "# Exemple d'utilisation pour top 100 features par corrélation\n",
    "target_columns = ['TARGET']\n",
    "df_train_top100_corr = select_top_features_by_correlation(df_train_poly, target_columns, 100)\n",
    "df_test_top100_corr = df_test_poly[df_train_top100_corr.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwNlzm37IkGN"
   },
   "source": [
    "## Best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hV0rBamVCdA_"
   },
   "outputs": [],
   "source": [
    "# Exporter en CSV\n",
    "export_to_csv_in_chunks(df_train_top100_corr, f'{LOCAL_EXPORT_FOLDER_PATH}/application_train_merged_encoded_with_features_poly_add_top100_corr.csv')\n",
    "export_to_csv_in_chunks(df_test_top100_corr, f'{LOCAL_EXPORT_FOLDER_PATH}/application_test_merged_encoded_with_features_poly_add_top100_corr.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCK0GIZ2ImZc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.feature_selection import RFE, mutual_info_classif, chi2, f_classif, SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib  # To save the trained model\n",
    "\n",
    "def calculate_total_rows(file_path):\n",
    "    return sum(1 for _ in open(file_path)) - 1  # subtract 1 for header\n",
    "\n",
    "def impute_missing_values(chunk):\n",
    "    imputer = SimpleImputer(strategy='mean')  # You can change the strategy as needed\n",
    "    try:\n",
    "        imputed_chunk = pd.DataFrame(imputer.fit_transform(chunk), columns=chunk.columns)\n",
    "    except ValueError as e:\n",
    "        print(f\"Imputation error: {e}\")\n",
    "        return None\n",
    "    return imputed_chunk\n",
    "\n",
    "def calculate_feature_importance_via_chunks(input_csv, target_column, method, chunk_size=10000):\n",
    "    aggregated_importances = None\n",
    "    total_chunks = 0\n",
    "    total_rows = calculate_total_rows(input_csv)\n",
    "    processed_rows = 0\n",
    "\n",
    "    for chunk in pd.read_csv(input_csv, chunksize=chunk_size):\n",
    "        print(f\"Columns in chunk before imputation: {chunk.columns}\")  # Debug: print columns\n",
    "        chunk = impute_missing_values(chunk)\n",
    "\n",
    "        if chunk is None or chunk.shape[1] != len(chunk.columns):\n",
    "            print(f\"Skipping problematic chunk with shape {chunk.shape if chunk is not None else 'None'}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Columns in chunk after imputation: {chunk.columns}\")  # Debug: print columns\n",
    "\n",
    "        if target_column not in chunk.columns:\n",
    "            print(f\"Target column '{target_column}' not found in the columns: {chunk.columns}\")\n",
    "            continue\n",
    "\n",
    "        if chunk[target_column].nunique() < 2:\n",
    "            print(f\"Skipping chunk with less than 2 classes in target column: {chunk[target_column].unique()}\")\n",
    "            continue\n",
    "\n",
    "        X_train = chunk.drop(columns=[target_column])\n",
    "        y_train = chunk[target_column]\n",
    "\n",
    "        if method == 'random_forest':\n",
    "            model = RandomForestClassifier()\n",
    "            model.fit(X_train, y_train)\n",
    "            feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "        elif method == 'rfe':\n",
    "            model = LogisticRegression(max_iter=1000)\n",
    "            rfe = RFE(model, n_features_to_select=20)\n",
    "            rfe.fit(X_train, y_train)\n",
    "            feature_importances = pd.Series(rfe.ranking_, index=X_train.columns).sort_values()\n",
    "        elif method == 'lasso':\n",
    "            model = Lasso(alpha=0.01)\n",
    "            model.fit(X_train, y_train)\n",
    "            feature_importances = pd.Series(model.coef_, index=X_train.columns).abs()\n",
    "        elif method == 'mutual_info':\n",
    "            mutual_info = mutual_info_classif(X_train, y_train)\n",
    "            feature_importances = pd.Series(mutual_info, index=X_train.columns)\n",
    "        elif method == 'chi2':\n",
    "            chi_scores, _ = chi2(X_train, y_train)\n",
    "            feature_importances = pd.Series(chi_scores, index=X_train.columns)\n",
    "        elif method == 'f_classif':\n",
    "            f_values, _ = f_classif(X_train, y_train)\n",
    "            feature_importances = pd.Series(f_values, index=X_train.columns)\n",
    "        elif method == 'select_from_model':\n",
    "            selector = SelectFromModel(RandomForestClassifier(), max_features=100)\n",
    "            selector.fit(X_train, y_train)\n",
    "            feature_importances = pd.Series(selector.estimator_.feature_importances_, index=X_train.columns)\n",
    "\n",
    "        if aggregated_importances is None:\n",
    "            aggregated_importances = feature_importances\n",
    "        else:\n",
    "            aggregated_importances = aggregated_importances.add(feature_importances, fill_value=0)\n",
    "\n",
    "        total_chunks += 1\n",
    "        processed_rows += chunk_size\n",
    "        progress = (processed_rows / total_rows) * 100\n",
    "        print(f\"Progress: {min(progress, 100):.2f}%\")\n",
    "\n",
    "    aggregated_importances /= total_chunks\n",
    "    sorted_importances = aggregated_importances.sort_values(ascending=False)\n",
    "    top_features = sorted_importances.head(100)\n",
    "\n",
    "    return top_features\n",
    "\n",
    "def save_top_features(top_features, output_csv):\n",
    "    top_features.to_csv(output_csv, header=True)\n",
    "\n",
    "def save_filtered_dataset(input_csv, top_features, output_csv, chunk_size=10000):\n",
    "    top_features = top_features.index\n",
    "    total_rows = calculate_total_rows(input_csv)\n",
    "    processed_rows = 0\n",
    "\n",
    "    first_chunk = True\n",
    "    for chunk in pd.read_csv(input_csv, chunksize=chunk_size):\n",
    "        chunk = impute_missing_values(chunk)\n",
    "\n",
    "        if chunk is None or chunk.shape[1] != len(chunk.columns):\n",
    "            print(f\"Skipping problematic chunk with shape {chunk.shape if chunk is not None else 'None'}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Columns in chunk: {chunk.columns}\")  # Debug: print columns\n",
    "        filtered_chunk = chunk[top_features]\n",
    "        filtered_chunk.to_csv(output_csv, mode='a', header=first_chunk, index=False)\n",
    "        first_chunk = False\n",
    "\n",
    "        processed_rows += chunk_size\n",
    "        progress = (processed_rows / total_rows) * 100\n",
    "        print(f\"Progress: {min(progress, 100):.2f}%\")\n",
    "\n",
    "def retrain_model_with_top_features(train_csv, test_csv, top_features_csv, target_column, model_output, chunk_size=10000):\n",
    "    top_features = pd.read_csv(top_features_csv, index_col=0).index\n",
    "\n",
    "    X_train = pd.DataFrame()\n",
    "    y_train = pd.Series(dtype='int')\n",
    "\n",
    "    total_train_rows = calculate_total_rows(train_csv)\n",
    "    processed_train_rows = 0\n",
    "\n",
    "    for chunk in pd.read_csv(train_csv, chunksize=chunk_size):\n",
    "        chunk = impute_missing_values(chunk)\n",
    "\n",
    "        if chunk is None or chunk.shape[1] != len(chunk.columns):\n",
    "            print(f\"Skipping problematic chunk with shape {chunk.shape if chunk is not None else 'None'}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Columns in chunk: {chunk.columns}\")  # Debug: print columns\n",
    "        if target_column not in chunk.columns:\n",
    "            print(f\"Target column '{target_column}' not found in the columns: {chunk.columns}\")\n",
    "            continue\n",
    "\n",
    "        if chunk[target_column].nunique() < 2:\n",
    "            print(f\"Skipping chunk with less than 2 classes in target column: {chunk[target_column].unique()}\")\n",
    "            continue\n",
    "\n",
    "        X_chunk = chunk[top_features]\n",
    "        y_chunk = chunk[target_column]\n",
    "\n",
    "        X_train = pd.concat([X_train, X_chunk], ignore_index=True)\n",
    "        y_train = pd.concat([y_train, y_chunk], ignore_index=True)\n",
    "\n",
    "        processed_train_rows += chunk_size\n",
    "        progress = (processed_train_rows / total_train_rows) * 100\n",
    "        print(f\"Training Progress: {min(progress, 100):.2f}%\")\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    joblib.dump(model, model_output)\n",
    "    print(f'Model saved to {model_output}')\n",
    "\n",
    "    X_test = pd.DataFrame()\n",
    "    y_test = pd.Series(dtype='int')\n",
    "\n",
    "    total_test_rows = calculate_total_rows(test_csv)\n",
    "    processed_test_rows = 0\n",
    "\n",
    "    for chunk in pd.read_csv(test_csv, chunksize=chunk_size):\n",
    "        chunk = impute_missing_values(chunk)\n",
    "\n",
    "        if chunk is None or chunk.shape[1] != len(chunk.columns):\n",
    "            print(f\"Skipping problematic chunk with shape {chunk.shape if chunk is not None else 'None'}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Columns in chunk: {chunk.columns}\")  # Debug: print columns\n",
    "        if target_column not in chunk.columns:\n",
    "            print(f\"Target column '{target_column}' not found in the columns: {chunk.columns}\")\n",
    "            continue\n",
    "\n",
    "        if chunk[target_column].nunique() < 2:\n",
    "            print(f\"Skipping chunk with less than 2 classes in target column: {chunk[target_column].unique()}\")\n",
    "            continue\n",
    "\n",
    "        X_chunk = chunk[top_features]\n",
    "        y_chunk = chunk[target_column]\n",
    "\n",
    "        X_test = pd.concat([X_test, X_chunk], ignore_index=True)\n",
    "        y_test = pd.concat([y_test, y_chunk], ignore_index=True)\n",
    "\n",
    "        processed_test_rows += chunk_size\n",
    "        progress = (processed_test_rows / total_test_rows) * 100\n",
    "        print(f\"Testing Progress: {min(progress, 100):.2f}%\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with top features: {accuracy}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_feature_selection_and_retraining(train_csv, test_csv, target_column, methods, chunk_size=10000):\n",
    "    for method in methods:\n",
    "        top_features = calculate_feature_importance_via_chunks(train_csv, target_column, method, chunk_size)\n",
    "        output_csv = f'top_features_{method}.csv'\n",
    "        save_top_features(top_features, output_csv)\n",
    "        print(f'Top features saved to {output_csv} for method {method}')\n",
    "\n",
    "        filtered_train_csv = f'filtered_train_{method}.csv'\n",
    "        save_filtered_dataset(train_csv, top_features, filtered_train_csv, chunk_size)\n",
    "        print(f'Filtered training data saved to {filtered_train_csv} for method {method}')\n",
    "\n",
    "        filtered_test_csv = f'filtered_test_{method}.csv'\n",
    "        save_filtered_dataset(test_csv, top_features, filtered_test_csv, chunk_size)\n",
    "        print(f'Filtered test data saved to {filtered_test_csv} for method {method}')\n",
    "\n",
    "        model_output = f'model_{method}.joblib'\n",
    "        retrain_model_with_top_features(train_csv, test_csv, output_csv, target_column, model_output, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M871e9IDRwKQ"
   },
   "outputs": [],
   "source": [
    "test12312  = pd.read_csv(f'{LOCAL_EXPORT_FOLDER_PATH}/application_train_merged_with_new_features_encoded_poly_add.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7Jc4EA6SCHM"
   },
   "outputs": [],
   "source": [
    "test12312.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Up0M6T4COb9f"
   },
   "outputs": [],
   "source": [
    "feature_selection_and_training_input_train_csv = f'{LOCAL_EXPORT_FOLDER_PATH}/application_train_merged_with_new_features_encoded_poly_add.csv'\n",
    "feature_selection_and_training_output_test_csv = f'{LOCAL_EXPORT_FOLDER_PATH}/application_test_merged_with_new_features_encoded_poly_add.csv'\n",
    "target_column = '1'\n",
    "methods = ['random_forest', 'rfe', 'lasso', 'mutual_info', 'chi2', 'f_classif', 'select_from_model']\n",
    "\n",
    "run_feature_selection_and_retraining(feature_selection_and_training_input_train_csv, feature_selection_and_training_output_test_csv, target_column, methods, chunk_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOxfTvvCM_JG"
   },
   "outputs": [],
   "source": [
    "top_features_random_forest = pd.read_csv(f'/content/top_features_random_forest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h141x45dncEq"
   },
   "source": [
    "#### Random Forest:\n",
    "\n",
    "Description : Utilise l'importance des features déterminée par un modèle de forêt aléatoire pour sélectionner les features les plus importantes.\n",
    "Formule : Importance d'une feature = Réduction moyenne de l'impureté (Gini ou Entropie) sur tous les arbres.\n",
    "\n",
    "#### Recursive Feature Elimination (RFE):\n",
    "\n",
    "Description : Sélectionne les features en itérant sur le modèle, éliminant les features les moins importantes à chaque étape.\n",
    "Formule : Basé sur le score du modèle après l'élimination de chaque feature, en boucle.\n",
    "\n",
    "#### Lasso (Least Absolute Shrinkage and Selection Operator):\n",
    "\n",
    "Description : Utilise la régularisation L1 qui force certains coefficients de features à devenir exactement zéro, effectuant ainsi une sélection de features.\n",
    "Formule :\n",
    "min\n",
    "⁡\n",
    "{\n",
    "1\n",
    "2\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑋\n",
    "𝑖\n",
    "𝛽\n",
    ")\n",
    "2\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "∣\n",
    "𝛽\n",
    "𝑗\n",
    "∣\n",
    "}\n",
    "min{\n",
    "2n\n",
    "1\n",
    "​\n",
    " ∑\n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y\n",
    "i\n",
    "​\n",
    " −X\n",
    "i\n",
    "​\n",
    " β)\n",
    "2\n",
    " +λ∑\n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β\n",
    "j\n",
    "​\n",
    " ∣}\n",
    "\n",
    "#### Mutual Information:\n",
    "\n",
    "Description : Mesure la dépendance mutuelle entre chaque feature et la variable cible.\n",
    "Formule :\n",
    "𝐼\n",
    "(\n",
    "𝑋\n",
    ";\n",
    "𝑌\n",
    ")\n",
    "=\n",
    "∑\n",
    "𝑥\n",
    "∈\n",
    "𝑋\n",
    "∑\n",
    "𝑦\n",
    "∈\n",
    "𝑌\n",
    "𝑝\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "log\n",
    "⁡\n",
    "𝑝\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "𝑝\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "𝑝\n",
    "(\n",
    "𝑦\n",
    ")\n",
    "I(X;Y)=∑\n",
    "x∈X\n",
    "​\n",
    " ∑\n",
    "y∈Y\n",
    "​\n",
    " p(x,y)log\n",
    "p(x)p(y)\n",
    "p(x,y)\n",
    "​\n",
    "\n",
    "#### Chi-squared (Chi2):\n",
    "\n",
    "Description : Test statistique pour mesurer l'indépendance entre deux variables catégorielles.\n",
    "Formule :\n",
    "𝜒\n",
    "2\n",
    "=\n",
    "∑\n",
    "(\n",
    "𝑂\n",
    "𝑖\n",
    "−\n",
    "𝐸\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "𝐸\n",
    "𝑖\n",
    "χ\n",
    "2\n",
    " =∑\n",
    "E\n",
    "i\n",
    "​\n",
    "\n",
    "(O\n",
    "i\n",
    "​\n",
    " −E\n",
    "i\n",
    "​\n",
    " )\n",
    "2\n",
    "\n",
    "​\n",
    "\n",
    "\n",
    "#### ANOVA F-test (f_classif):\n",
    "\n",
    "Description : Utilise l'ANOVA pour comparer les variances entre les groupes et au sein des groupes.\n",
    "Formule :\n",
    "𝐹\n",
    "=\n",
    "variance entre les groupes\n",
    "variance au sein du groupe\n",
    "F=\n",
    "variance au sein du groupe\n",
    "variance entre les groupes\n",
    "​\n",
    "\n",
    "#### SelectFromModel:\n",
    "\n",
    "Description : Utilise l'importance des features déterminée par un modèle (comme Lasso, Random Forest, etc.) pour sélectionner les features.\n",
    "Formule : Basé sur les coefficients ou importances des features déterminés par le modèle sélectionné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNRMVfJANUB6"
   },
   "outputs": [],
   "source": [
    "top_features_random_forest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e5b0efd9-67ac-4aa0-91e9-2141a87a6a8a",
    "_uuid": "a63d53dcac14c4ac2e31ea9c5e16b5d161c2415b",
    "collapsed": true,
    "id": "WQlSIaLUyLXY",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make a new dataframe for polynomial features\n",
    "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "\n",
    "# imputer for handling missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "poly_target = poly_features['TARGET']\n",
    "\n",
    "poly_features = poly_features.drop(columns = ['TARGET'])\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2be7c1ab-d1e5-40f2-b8e7-e2b2ce1e2f9a",
    "_uuid": "72c5ecaae9c6ff038d16cbd9208f1abb69912631",
    "id": "eiqqENbcyLXY"
   },
   "outputs": [],
   "source": [
    "# Train the polynomial features\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape: ', poly_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a7833b1e-714c-4988-8cbf-757d01290d8f",
    "_uuid": "4d837e47bada5411ffce06266605f043c6ffe19e",
    "id": "2U1Aw7z5yLXY"
   },
   "source": [
    "This creates a considerable number of new features. To get the names we have to use the polynomial features `get_feature_names` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7465d1e6-d360-4029-afa7-67cb34f60249",
    "_uuid": "121f98d2ec9c81c5dabb911dc68562d0b2b6d737",
    "id": "F9csJh9DyLXY"
   },
   "outputs": [],
   "source": [
    "poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7eaeb645-bb25-4ac5-884f-d1231ab6d88f",
    "_uuid": "4e68b80b2738ef46863b53b7b781f299d602d316",
    "id": "niFjgGBNyLXZ"
   },
   "source": [
    "There are 35 features with individual features raised to powers up to degree 3 and interaction terms. Now, we can see whether any of these new features are correlated with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "95725a63-f8f2-4680-8f7a-4252f04e7f7f",
    "_uuid": "e712923de757457bb87a35ecaccd27007b351e6c",
    "id": "hLyY9rTiyLXZ"
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of the features\n",
    "poly_features = pd.DataFrame(poly_features,\n",
    "                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2',\n",
    "                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "# Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display most negative and most positive\n",
    "print(poly_corrs.head(10))\n",
    "print(poly_corrs.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "971de432-c65e-4c9a-a3c0-c923ed27ddcb",
    "_uuid": "082ac97a068afed6758ed191acf5ab485e39230c",
    "id": "V-RYJF8EyLXZ"
   },
   "source": [
    "Several of the new variables have a greater (in terms of absolute magnitude) correlation with the target than the original features. When we build machine learning models, we can try with and without these features to determine if they actually help the model learn.\n",
    "\n",
    "We will add these features to a copy of the training and testing data and then evaluate models with and without the features. Many times in machine learning, the only way to know if an approach will work is to try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ed758ed436a86f92a8ee574999aa91089242ca7a",
    "id": "XBnFRY0fyLXZ"
   },
   "outputs": [],
   "source": [
    "# Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test,\n",
    "                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2',\n",
    "                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "# Merge polynomial features into training dataframe\n",
    "poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n",
    "app_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n",
    "app_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
    "\n",
    "# Print out the new shapes\n",
    "print('Training data with polynomial features shape: ', app_train_poly.shape)\n",
    "print('Testing data with polynomial features shape:  ', app_test_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b27fad1522263c32b57a8127c84ad0e08ff9d8f",
    "id": "jA3R9z-ryLXZ"
   },
   "source": [
    "## Domain Knowledge Features\n",
    "\n",
    "Maybe it's not entirely correct to call this \"domain knowledge\" because I'm not a credit expert, but perhaps we could call this \"attempts at applying limited financial knowledge\". In this frame of mind, we can make a couple features that attempt to capture what we think may be important for telling whether a client will default on a loan. Here I'm going to use five features that were inspired by [this script](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) by Aguiar:\n",
    "\n",
    "* `CREDIT_INCOME_PERCENT`: the percentage of the credit amount relative to a client's income\n",
    "* `ANNUITY_INCOME_PERCENT`: the percentage of the loan annuity relative to a client's income\n",
    "* `CREDIT_TERM`:  the length of the payment in months (since the annuity is the monthly amount due\n",
    "* `DAYS_EMPLOYED_PERCENT`: the percentage of the days employed relative to the client's age\n",
    "\n",
    "Again, thanks to Aguiar and [his great script](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) for exploring these features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8d4b165b45da6c3120911de18e9348d8726c70c",
    "collapsed": true,
    "id": "dvDGISfMyLXZ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "app_train_domain = app_train.copy()\n",
    "app_test_domain = app_test.copy()\n",
    "\n",
    "app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n",
    "app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d017103871bd4935a8c29599d6be33e0e74b2f83",
    "collapsed": true,
    "id": "wAPy7CfGyLXZ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n",
    "app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e917d654c05bd0ca3251d4f51c8176d82fe613f",
    "id": "1i1m9Z-8yLXZ"
   },
   "source": [
    "#### Visualize New Variables\n",
    "\n",
    "We should explore these __domain knowledge__ variables visually in a graph. For all of these, we will make the same KDE plot colored by the value of the `TARGET`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9c10d7f55b4c636335f815762b93598fe4acb0a",
    "id": "r03fIlzoyLXa"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 20))\n",
    "# iterate through the new features\n",
    "for i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n",
    "\n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(4, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n",
    "\n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % feature)\n",
    "    plt.xlabel('%s' % feature); plt.ylabel('Density');\n",
    "\n",
    "plt.tight_layout(h_pad = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e27d400f3ec5447cfe5e908952351f271d521784",
    "id": "JBr4RSGhyLXa"
   },
   "source": [
    "It's hard to say ahead of time if these new features will be useful. The only way to tell for sure is to try them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6QrvylkkWy9"
   },
   "outputs": [],
   "source": [
    "xgboost\n",
    "lgbm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ebb64e63-6222-4509-a43c-302c6435ce09",
    "_uuid": "8bf057e523b2d99833f6dc9d95fe6141fb4e325a",
    "id": "m3VSRwKSyLXa"
   },
   "source": [
    "# Baseline\n",
    "\n",
    "For a naive baseline, we could guess the same value for all examples on the testing set.  We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess 0.5 for all observations on the test set. This  will get us a Reciever Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition ([random guessing on a classification task will score a 0.5](https://stats.stackexchange.com/questions/266387/can-auc-roc-be-between-0-0-5)).\n",
    "\n",
    "Since we already know what score we are going to get, we don't really need to make a naive baseline guess. Let's use a slightly more sophisticated model for our actual baseline: Logistic Regression.\n",
    "\n",
    "## Logistic Regression Implementation\n",
    "\n",
    "Here I will focus on implementing the model rather than explaining the details, but for those who want to learn more about the theory of machine learning algorithms, I recommend both [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) and [Hands-On Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do). Both of these books present the theory and also the code needed to make the models (in R and Python respectively). They both teach with the mindset that the best way to learn is by doing, and they are very effective!\n",
    "\n",
    "To get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values (imputation) and normalizing the range of the features (feature scaling). The following code performs both of these preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60ef8744-ca3a-4810-8439-2835fbfc1833",
    "_uuid": "784ae2f91cf7792702595a9973ba773b2acdec00",
    "id": "pvtQc0dayLXa"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "\n",
    "# Drop the target from the training data\n",
    "if 'TARGET' in app_train:\n",
    "    train = app_train.drop(columns = ['TARGET'])\n",
    "else:\n",
    "    train = app_train.copy()\n",
    "\n",
    "# Feature names\n",
    "features = list(train.columns)\n",
    "\n",
    "# Copy of the testing data\n",
    "test = app_test.copy()\n",
    "\n",
    "# Median imputation of missing values\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "imputer.fit(train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(app_test)\n",
    "\n",
    "# Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "print('Training data shape: ', train.shape)\n",
    "print('Testing data shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1bcfab25-cc1c-4553-9473-96fcfeb2a61a",
    "_uuid": "364f0835a46f7a7bb7be487b54d92f5ff50ed341",
    "id": "MZPAPkj3yLXa"
   },
   "source": [
    "We will use [`LogisticRegression`from Scikit-Learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for our first model. The only change we will make from the default model settings is to lower the [regularization parameter](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), C, which controls the amount of overfitting (a lower value should decrease overfitting). This will get us slightly better results than the default `LogisticRegression`, but it still will set a low bar for any future models.\n",
    "\n",
    "Here we use the familiar Scikit-Learn modeling syntax: we first create the model, then we train the model using `.fit` and then we make predictions on the testing data using `.predict_proba` (remember that we want probabilities and not a 0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6462ff85-e3b6-4a5f-b95c-9416841413b1",
    "_uuid": "9e8aba9401e8367f9902d710ba49e820294870e1",
    "id": "kFxPOsVsyLXa"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Make the model with the specified regularization parameter\n",
    "log_reg = LogisticRegression(C = 0.0001)\n",
    "\n",
    "# Train on the training data\n",
    "log_reg.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fe98191a-da57-4d50-8d56-7d8077fc6c26",
    "_uuid": "0ad71fb750fac4af2845f30b0af73f5817e46101",
    "id": "tWEH8PCDyLXa"
   },
   "source": [
    "Now that the model has been trained, we can use it to make predictions. We want to predict the probabilities of not paying a loan, so we use the model `predict.proba` method. This returns an m x 2 array where m is the number of observations. The first column is the probability of the target being 0 and the second column is the probability of the target being 1 (so for a single row, the two columns must sum to 1). We want the probability the loan is not repaid, so we will select the second column.\n",
    "\n",
    "The following code makes the predictions and selects the correct column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "80c77c89-3fa9-4311-b441-412a4fbb1480",
    "_uuid": "2138782ddbfc9a803dc99a938460fc27d15972a9",
    "collapsed": true,
    "id": "tgj76S8QyLXa",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "# Make sure to select the second column only\n",
    "log_reg_pred = log_reg.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3a2612e95b13a94a13f679c1754b6c4fb28c332d",
    "id": "LuczORZEyLXa"
   },
   "source": [
    "The predictions must be in the format shown in the `sample_submission.csv` file, where there are only two columns: `SK_ID_CURR` and `TARGET`. We will create a dataframe in this format from the test set and the predictions called `submit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09a3d281e4c7ee6820f402e32f31775851113089",
    "id": "h3vPiFloyLXb"
   },
   "outputs": [],
   "source": [
    "# Submission dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = log_reg_pred\n",
    "\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a1bf4f54df8b37a71a7732e61a7bfebafd8be11",
    "id": "MC3G7DdRyLXb"
   },
   "source": [
    "The predictions represent a probability between 0 and 1 that the loan will not be repaid. If we were using these predictions to classify applicants, we could set a probability threshold for determining that a loan is risky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77204f15-c3b9-4c67-8d93-173fa3afceaa",
    "_uuid": "fcaf338e52d8f42f119b31d437b516e336e787ec",
    "collapsed": true,
    "id": "eUiKafSXyLXb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Save the submission to a csv file\n",
    "submit.to_csv('log_reg_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11e18bd5c4e75b931f90a22bc6ff84441a13570c",
    "id": "--3NvHueyLXb"
   },
   "source": [
    "The submission has now been saved to the virtual environment in which our notebook is running. To access the submission, at the end of the notebook, we will hit the blue Commit & Run button at the upper right of the kernel. This runs the entire notebook and then lets us download any files that are created during the run.\n",
    "\n",
    "Once we run the notebook, the files created are available in the Versions tab under the Output sub-tab. From here, the submission files can be submitted to the competition or downloaded. Since there are several models in this notebook, there will be multiple output files.\n",
    "\n",
    "__The logistic regression baseline should score around 0.671 when submitted.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "462ea34f-3f66-490a-a61f-24a991271f69",
    "_uuid": "92687ac866441f6ee2919aa5e5c935490c172afc",
    "id": "AdXbzZ0fyLXb"
   },
   "source": [
    "## Improved Model: Random Forest\n",
    "\n",
    "To try and beat the poor performance of our baseline, we can update the algorithm. Let's try using a Random Forest on the same training data to see how that affects performance. The Random Forest is a much more powerful model especially when we use hundreds of trees. We will use 100 trees in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6643479e-7980-431c-a6a2-9087acdb0f42",
    "_uuid": "cf05e2318904b8f3575ae233c185cd995fd07643",
    "collapsed": true,
    "id": "wlMrwmFwyLXb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Make the random forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "020f0856-8f24-4b22-bca5-aac7f137f032",
    "_uuid": "52258a9b89b3069bc1d82829107e8e7c1ef05fd6",
    "collapsed": true,
    "id": "yneNf8ORyLXb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train on the training data\n",
    "random_forest.fit(train, train_labels)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importance_values = random_forest.feature_importances_\n",
    "feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "25145966-669e-426d-89a3-98e30b861057",
    "_uuid": "1da4b02502388d2b8a2bc5376027c5bef50272f3",
    "collapsed": true,
    "id": "gQa6PtXIyLXb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make a submission dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = predictions\n",
    "\n",
    "# Save the submission dataframe\n",
    "submit.to_csv('random_forest_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf6f600ed10c511dd26d4bd5efa7997ab8d6916a",
    "id": "1iWwTXK-yLXb"
   },
   "source": [
    "These predictions will also be available when we run the entire notebook.\n",
    "\n",
    "__This model should score around 0.678 when submitted.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43d979aed7cdfd6d7bd6a995b5756a384bd2b7dc",
    "id": "lx_V9Q74yLXb"
   },
   "source": [
    "### Make Predictions using Engineered Features\n",
    "\n",
    "The only way to see if the Polynomial Features and Domain knowledge improved the model is to train a test a model on these features! We can then compare the submission performance to that for the model without these features to gauge the effect of our feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9d49008fb73b8d15c797850c64d5e6f81375163",
    "collapsed": true,
    "id": "1ddu_WZSyLXc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "poly_features_names = list(app_train_poly.columns)\n",
    "\n",
    "# Impute the polynomial features\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "poly_features = imputer.fit_transform(app_train_poly)\n",
    "poly_features_test = imputer.transform(app_test_poly)\n",
    "\n",
    "# Scale the polynomial features\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "poly_features = scaler.fit_transform(poly_features)\n",
    "poly_features_test = scaler.transform(poly_features_test)\n",
    "\n",
    "random_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7d3f3b6cdf8231832c56224c8a694056e456593",
    "collapsed": true,
    "id": "b1IzcjrFyLXc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train on the training data\n",
    "random_forest_poly.fit(poly_features, train_labels)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd923eed057b6d61354db27473d9a36f1411dd5c",
    "collapsed": true,
    "id": "lRzdHBUhyLXc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make a submission dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = predictions\n",
    "\n",
    "# Save the submission dataframe\n",
    "submit.to_csv('random_forest_baseline_engineered.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec50627c874a9d78d6789e01a47e829c820f9615",
    "id": "fzNbeB3XyLXc"
   },
   "source": [
    "This model scored 0.678 when submitted to the competition, exactly the same as that without the engineered features. Given these results, it does not appear that our feature construction helped in this case.\n",
    "\n",
    "#### Testing Domain Features\n",
    "\n",
    "Now we can test the domain features we made by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04b93e7d3629c1a5ba27a6eed037900862dc039d",
    "collapsed": true,
    "id": "LWvbBAi2yLXc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "app_train_domain = app_train_domain.drop(columns = 'TARGET')\n",
    "\n",
    "domain_features_names = list(app_train_domain.columns)\n",
    "\n",
    "# Impute the domainnomial features\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "domain_features = imputer.fit_transform(app_train_domain)\n",
    "domain_features_test = imputer.transform(app_test_domain)\n",
    "\n",
    "# Scale the domainnomial features\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "domain_features = scaler.fit_transform(domain_features)\n",
    "domain_features_test = scaler.transform(domain_features_test)\n",
    "\n",
    "random_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Train on the training data\n",
    "random_forest_domain.fit(domain_features, train_labels)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importance_values_domain = random_forest_domain.feature_importances_\n",
    "feature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27598fb499df4c3282be63356422e4a6f6d6dd17",
    "collapsed": true,
    "id": "gUzzVAq3yLXc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make a submission dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = predictions\n",
    "\n",
    "# Save the submission dataframe\n",
    "submit.to_csv('random_forest_baseline_domain.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "240fb8ba2b5fe73da4d021543fd64baa104fb418",
    "id": "2ItnPLzPyLXc"
   },
   "source": [
    "This scores 0.679 when submitted which probably shows that the engineered features do not help in this model (however they do help in the Gradient Boosting Model at the end of the notebook).\n",
    "\n",
    "In later notebooks, we will do more [feature engineering](https://docs.featuretools.com/index.html) by using the information from the other data sources. From experience, this will definitely help our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b742ed91-9dd6-4a7b-af5e-1d6e7128beb2",
    "_uuid": "b1805834b4d4eae38db4f68502aade956fc1e10f",
    "id": "eNc4Cur_yLXc"
   },
   "source": [
    "## Model Interpretation: Feature Importances\n",
    "\n",
    "As a simple method to see which variables are the most relevant, we can look at the feature importances of the random forest. Given the correlations we saw in the exploratory data analysis, we should expect that the most important features are the `EXT_SOURCE` and the `DAYS_BIRTH`. We may use these feature importances as a method of dimensionality reduction in future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a90e9368-5f7d-4179-a5cc-1025f32c6a81",
    "_uuid": "b912337a5f35f495398d8ae8b8576ceb7062fe50",
    "collapsed": true,
    "id": "4qe5yXpTyLXc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    \"\"\"\n",
    "    Plot importances returned by a model. This can work with any measure of\n",
    "    feature importance provided that higher importance is better.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): feature importances. Must have the features in a column\n",
    "        called `features` and the importances in a column called `importance\n",
    "\n",
    "    Returns:\n",
    "        shows a plot of the 15 most importance features\n",
    "\n",
    "        df (dataframe): feature importances sorted by importance (highest to lowest)\n",
    "        with a column for normalized importance\n",
    "        \"\"\"\n",
    "\n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "\n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))),\n",
    "            df['importance_normalized'].head(15),\n",
    "            align = 'center', edgecolor = 'k')\n",
    "\n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "\n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1084ad42-bc44-438b-b2fd-5fd7a1c1b363",
    "_uuid": "37309c4a94b248ad85fa7a0825f01830a818ba92",
    "collapsed": true,
    "id": "2bf350PAyLXd",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Show the feature importances for the default features\n",
    "feature_importances_sorted = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "524c6aa12acc80e7018750e7a8897dc6b4bacf18",
    "id": "M7Ck9476yLXd"
   },
   "source": [
    "As expected, the most important features are those dealing with `EXT_SOURCE` and `DAYS_BIRTH`. We see that there are only a handful of features with a significant importance to the model, which suggests we may be able to drop many of the features without a decrease in performance (and we may even see an increase in performance.) Feature importances are not the most sophisticated method to interpret a model or perform dimensionality reduction, but they let us start to understand what factors our model takes into account when it makes predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "516e4b2eedeec2ff441f1ff034fbe4a73374bba2",
    "collapsed": true,
    "id": "XbDfuxBWyLXd",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a2cdf24d5ecc01539d10902a9c7af6a13096086",
    "id": "pR4IggFZyLXd"
   },
   "source": [
    "We see that all four of our hand-engineered features made it into the top 15 most important! This should give us confidence that our domain knowledge was at least partially on track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd407ca0f7c5c50ee71fe5c8532eabeb92c15c50",
    "id": "zzsJ6gCpyLXd"
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we saw how to get started with a Kaggle machine learning competition. We first made sure to understand the data, our task, and the metric by which our submissions will be judged. Then, we performed a fairly simple EDA to try and identify relationships, trends, or anomalies that may help our modeling. Along the way, we performed necessary preprocessing steps such as encoding categorical variables, imputing missing values, and scaling features to a range. Then, we constructed new features out of the existing data to see if doing so could help our model.\n",
    "\n",
    "Once the data exploration, data preparation, and feature engineering was complete, we implemented a baseline model upon which we hope to improve. Then we built a second slightly more complicated model to beat our first score. We also carried out an experiment to determine the effect of adding the engineering variables.\n",
    "\n",
    "We followed the general outline of a [machine learning project](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420):\n",
    "\n",
    "1.  Understand the problem and the data\n",
    "2. Data cleaning and formatting (this was mostly done for us)\n",
    "3. Exploratory Data Analysis\n",
    "4. Baseline model\n",
    "5.  Improved model\n",
    "6. Model interpretation (just a little)\n",
    "\n",
    "Machine learning competitions do differ slightly from typical data science problems in that we are concerned only with achieving the best performance on a single metric and do not care about the interpretation. However, by attempting to understand how our models make decisions, we can try to improve them or examine the mistakes in order to correct the errors. In future notebooks we will look at incorporating more sources of data, building more complex models (by following the code of others), and improving our scores.\n",
    "\n",
    "I hope this notebook was able to get you up and running in this machine learning competition and that you are now ready to go out on your own - with help from the community - and start working on some great problems!\n",
    "\n",
    "__Running the notebook__: now that we are at the end of the notebook, you can hit the blue Commit & Run button to execute all the code at once. After the run is complete (this should take about 10 minutes), you can then access the files that were created by going to the versions tab and then the output sub-tab. The submission files can be directly submitted to the competition from this tab or they can be downloaded to a local machine and saved. The final part is to share the share the notebook: go to the settings tab and change the visibility to Public. This allows the entire world to see your work!\n",
    "\n",
    "### Follow-up Notebooks\n",
    "\n",
    "For those looking to keep working on this problem, I have a series of follow-up notebooks:\n",
    "\n",
    "* [Manual Feature Engineering Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)\n",
    "* [Manual Feature Engineering Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)\n",
    "* [Introduction to Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)\n",
    "* [Advanced Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory)\n",
    "* [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)\n",
    "* [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)\n",
    "\n",
    "As always, I welcome feedback and constructive criticism. I write for Towards Data Science at https://medium.com/@williamkoehrsen/ and can be reached on Twitter at https://twitter.com/koehrsen_will\n",
    "\n",
    "Will\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d12452cd-347e-4269-b3d4-f5f0589f4c5c",
    "_uuid": "a8bc307f9be27bfabbc3891deddbd94293ca03fa",
    "id": "VBkE2Xo-yLXd"
   },
   "source": [
    "# Just for Fun: Light Gradient Boosting Machine\n",
    "\n",
    "Now (if you want, this part is entirely optional) we can step off the deep end and use a real machine learning model: the [gradient boosting machine](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/) using the [LightGBM library](http://lightgbm.readthedocs.io/en/latest/Quick-Start.html)! The Gradient Boosting Machine is currently the leading model for learning on structured datasets (especially on Kaggle) and we will probably need some form of this model to do well in the competition. Don't worry, even if this code looks intimidating, it's just a series of small steps that build up to a complete model. I added this code just to show what may be in store for this project, and because it gets us a slightly better score on the leaderboard. In future notebooks we will see how to work with more advanced models (which mostly means adapting existing code to make it work better), feature engineering, and feature selection. See you in the next notebook!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60208a3f-947f-42d9-8f46-2159afd2eb7d",
    "_uuid": "2719663ed461422fce26b5dd55a31ab9718df47a",
    "collapsed": true,
    "id": "tPU_QviayLXd",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "def model(features, test_features, encoding = 'ohe', n_folds = 5):\n",
    "\n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation.\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame):\n",
    "            dataframe of training features to use\n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame):\n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model.\n",
    "        encoding (str, default = 'ohe'):\n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame):\n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame):\n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame):\n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "\n",
    "    # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "\n",
    "\n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "\n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "\n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "\n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "\n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "\n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "\n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "\n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary',\n",
    "                                   class_weight = 'balanced', learning_rate = 0.05,\n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1,\n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "\n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "\n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "\n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "\n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "\n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores})\n",
    "\n",
    "    return submission, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "89e02dcbb23e47e3504ed1f61431b182e2011ba5",
    "collapsed": true,
    "id": "5IPbbQtiyLXd",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "submission, fi, metrics = model(app_train, app_test)\n",
    "print('Baseline metrics')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca59467edd1060e5f7587a77a31dcd7331ce90ec",
    "collapsed": true,
    "id": "ht5NNS9DyLXe",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fi_sorted = plot_feature_importances(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d71f9d7b9b322824704eec9dc82e38a480d4f76c",
    "collapsed": true,
    "id": "nmARDfbXyLXe",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('baseline_lgb.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2aca0b9ea31dfef1ca3221dc6424fe31e829cbbf",
    "id": "9NjypQQSyLXe"
   },
   "source": [
    "This submission should score about 0.735 on the leaderboard. We will certainly best that in future work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd53d758d2838c9b99b9ae44780514d13373b717",
    "collapsed": true,
    "id": "4rIzzH_7yLXe",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "app_train_domain['TARGET'] = train_labels\n",
    "\n",
    "# Test the domain knolwedge features\n",
    "submission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\n",
    "print('Baseline with domain knowledge features metrics')\n",
    "print(metrics_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58a2d9b330a223733e3673b24433c41122d3b611",
    "collapsed": true,
    "id": "WY3nCTjIyLXe",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fi_sorted = plot_feature_importances(fi_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "947f49722aa5ae1df696c311d7413e91ba51e1b9",
    "id": "Bc2dgsHHyLXe"
   },
   "source": [
    "Again, we see tha some of our features made it into the most important. Going forward, we will need to think about whatother domain knowledge features may be useful for this problem (or we should consult someone who knows more about the financial industry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7dfc9123c7e231826be54a1c022e373a1ee68f51",
    "collapsed": true,
    "id": "C4cCeFQSyLXe",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "submission_domain.to_csv('baseline_lgb_domain_features.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "033b8d192a40127ea99d6f4fb13c624dd64c6611",
    "id": "huimVAHbyLXe"
   },
   "source": [
    "This model scores about 0.754 when submitted to the public leaderboard indicating that the domain features do improve the performance! [Feature engineering](https://en.wikipedia.org/wiki/Feature_engineering) is going to be a critical part of this competition (as it is for all machine learning problems)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7e9a1149953069853d4d83ec46f22084dce8711",
    "collapsed": true,
    "id": "69dOjDnSyLXe",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
