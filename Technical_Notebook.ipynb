{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:36:19.717059200Z",
     "start_time": "2024-06-19T21:36:19.687834700Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zip_directory(folder_path, zip_path):\n",
    "    \"\"\"\n",
    "    Zips the specified folder and saves it to the specified zip file path.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder to be zipped.\n",
    "        zip_path (str): The path where the zip file should be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the folder path exists\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"The folder path {folder_path} does not exist or is not a directory.\")\n",
    "\n",
    "    # Create a ZipFile object\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Walk through the directory\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                # Create the full path to the file\n",
    "                full_path = os.path.join(root, file)\n",
    "                # Add file to the zip file\n",
    "                arcname = os.path.relpath(full_path, start=folder_path)\n",
    "                zipf.write(full_path, arcname=arcname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbea0ab7c5b237d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Compression of resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95cc61b7ef1fbab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:38:12.922913900Z",
     "start_time": "2024-06-19T21:36:22.850771800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "zip_directory('./resources', './resources.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "660498668ba91f11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:45:16.594967800Z",
     "start_time": "2024-06-19T21:45:16.560643500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "zip_directory('./exports', './exports.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01f6838c64975a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:42:56.083375600Z",
     "start_time": "2024-06-19T21:42:53.035314800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement git (from versions: none)\n",
      "ERROR: No matching distribution found for git\n"
     ]
    }
   ],
   "source": [
    " pip install git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f96edf4dcfc3291e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:42:41.674088300Z",
     "start_time": "2024-06-19T21:42:41.621346400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'git'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgit\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpush_exports_to_github\u001b[39m(repo_path, exports_folder, commit_message, branch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    Pushes the changes in the specified exports folder to GitHub.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m        branch (str): The branch to push the changes to. Default is 'main'.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'git'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import git\n",
    "\n",
    "def push_exports_to_github(repo_path, exports_folder, commit_message, branch='main'):\n",
    "    \"\"\"\n",
    "    Pushes the changes in the specified exports folder to GitHub.\n",
    "\n",
    "    Args:\n",
    "        repo_path (str): The path to the local git repository.\n",
    "        exports_folder (str): The path to the exports folder relative to the repository.\n",
    "        commit_message (str): The commit message for the changes.\n",
    "        branch (str): The branch to push the changes to. Default is 'main'.\n",
    "    \"\"\"\n",
    "    # Ensure the repository path exists\n",
    "    if not os.path.isdir(repo_path):\n",
    "        raise ValueError(f\"The repository path {repo_path} does not exist or is not a directory.\")\n",
    "\n",
    "    # Ensure the exports folder path exists\n",
    "    exports_path = os.path.join(repo_path, exports_folder)\n",
    "    if not os.path.isdir(exports_path):\n",
    "        raise ValueError(f\"The exports folder path {exports_path} does not exist or is not a directory.\")\n",
    "\n",
    "    try:\n",
    "        # Initialize the repository\n",
    "        repo = git.Repo(repo_path)\n",
    "\n",
    "        # Stage changes in the exports folder\n",
    "        repo.git.add(exports_path)\n",
    "\n",
    "        # Commit the changes\n",
    "        repo.index.commit(commit_message)\n",
    "\n",
    "        # Push the changes to the specified branch\n",
    "        origin = repo.remote(name='origin')\n",
    "        origin.push(refspec=f'{branch}:{branch}')\n",
    "\n",
    "        print(f\"Changes from '{exports_folder}' pushed to GitHub branch '{branch}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# Make sure your repo_path points to the local git repository\n",
    "repo_path = '/path/to/your/local/repo'\n",
    "exports_folder = 'exports'  # This should be the relative path from the repo root\n",
    "commit_message = 'Add files from exports directory'\n",
    "push_exports_to_github(repo_path, exports_folder, commit_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b6728b9bd3413",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Update Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c57ab600557ff6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main ad8e175] Notebooks updates\n",
      " 9 files changed, 3910 insertions(+), 1362 deletions(-)\n",
      " rewrite Fusilier_Antoine_2_notebook_modelization_032024.ipynb (66%)\n",
      " rewrite Technical_Notebook.ipynb (66%)\n",
      " create mode 100644 catboost_info/catboost_training.json\n",
      " create mode 100644 catboost_info/learn/events.out.tfevents\n",
      " create mode 100644 catboost_info/learn_error.tsv\n",
      " create mode 100644 catboost_info/time_left.tsv\n",
      "Uploading LFS objects: 100% (1/1), 224 B | 0 B/s, done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: \n",
      "remote: GitHub found 1 vulnerability on lessons-data-ai-engineer/project_4-home_credit_default_risk's default branch (1 moderate). To find out more, visit:        \n",
      "remote:      https://github.com/lessons-data-ai-engineer/project_4-home_credit_default_risk/security/dependabot/1        \n",
      "remote: \n",
      "To https://github.com/lessons-data-ai-engineer/project_4-home_credit_default_risk.git\n",
      "   94066c3..ad8e175  main -> main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from subprocess import check_call, CalledProcessError\n",
    "\n",
    "COMMIT_MESSAGE = \"Notebooks updates\"\n",
    "\n",
    "def git_add_commit_push():\n",
    "    try:\n",
    "        check_call([\"git\", \"add\", \".\"])\n",
    "        check_call([\"git\", \"commit\", \"-m\", COMMIT_MESSAGE])\n",
    "        check_call([\"git\", \"push\"])\n",
    "    except CalledProcessError as e:\n",
    "        print(f\"Error during git operations: {e}\")\n",
    "\n",
    "def main():\n",
    "    git_add_commit_push()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76be65d1-3046-4ec1-beb3-6235b5ca77d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading LFS objects: 100% (486/486), 114 MB | 0 B/s, done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: \n",
      "remote: GitHub found 1 vulnerability on lessons-data-ai-engineer/project_4-home_credit_default_risk's default branch (1 moderate). To find out more, visit:        \n",
      "remote:      https://github.com/lessons-data-ai-engineer/project_4-home_credit_default_risk/security/dependabot/1        \n",
      "remote: \n",
      "To https://github.com/lessons-data-ai-engineer/project_4-home_credit_default_risk.git\n",
      "   894c36e..9f8d803  main -> main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_call([\"git\", \"push\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b43f1db-4e6d-48ed-9d2b-e64a07dc64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def git_command(command):\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        return result.stdout.decode(), result.stderr.decode()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return e.stdout.decode(), e.stderr.decode()\n",
    "\n",
    "def git_pull():\n",
    "    print(\"Stashing any local changes...\")\n",
    "    stdout, stderr = git_command([\"git\", \"stash\"])\n",
    "    if stderr:\n",
    "        print(f\"Error during git stash: {stderr}\")\n",
    "        return\n",
    "\n",
    "    print(\"Pulling latest changes from the repository...\")\n",
    "    stdout, stderr = git_command([\"git\", \"pull\"])\n",
    "    print(stdout)\n",
    "    if 'CONFLICT' in stdout or 'CONFLICT' in stderr:\n",
    "        print(\"Merge conflicts detected. Attempting to resolve automatically...\")\n",
    "        stdout, stderr = git_command([\"git\", \"merge\", \"--abort\"])\n",
    "        if stderr:\n",
    "            print(f\"Error during merge abort: {stderr}\")\n",
    "            return\n",
    "        stdout, stderr = git_command([\"git\", \"pull\", \"--strategy-option=theirs\"])\n",
    "        if stderr:\n",
    "            print(f\"Error during git pull with merge strategy: {stderr}\")\n",
    "            return\n",
    "\n",
    "    print(\"Applying stashed changes...\")\n",
    "    stdout, stderr = git_command([\"git\", \"stash\", \"pop\"])\n",
    "    if stderr:\n",
    "        print(f\"Error during git stash pop: {stderr}\")\n",
    "        return\n",
    "\n",
    "    print(\"Repository successfully updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7ab206-2fc3-4f46-bc22-d841a6482625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stashing any local changes...\n",
      "Pulling latest changes from the repository...\n",
      "Auto-merging Fusilier_Antoine_1_notebook_exploratory_analysis_and_cleaning_and_feature_enginering_022024.ipynb\n",
      "CONFLICT (content): Merge conflict in Fusilier_Antoine_1_notebook_exploratory_analysis_and_cleaning_and_feature_enginering_022024.ipynb\n",
      "Automatic merge failed; fix conflicts and then commit the result.\n",
      "\n",
      "Merge conflicts detected. Attempting to resolve automatically...\n",
      "Applying stashed changes...\n",
      "Error during git stash pop: No stash entries found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "git_pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3aeb712-0d4c-4731-ac85-e482164558a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.0 MB 68.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 36.4 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488513 sha256=327326550d4b02d8c63a0f787d425f0a7124bc0b4a6a862d46632e0d31c4decf\n",
      "  Stored in directory: /root/.cache/pip/wheels/da/78/6d/54350e0243f65f77dccf6ebe2ed5559faf6900559e904fb957\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c33529-b206-48bc-aa2c-e0c8322c8cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Configurer Spark pour accéder à HDFS\u001b[39;00m\n\u001b[1;32m      5\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHDFS Access\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Exemple de lecture d'un fichier HDFS\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configurer Spark pour accéder à HDFS\n",
    "conf = SparkConf().setAppName(\"HDFS Access\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Exemple de lecture d'un fichier HDFS\n",
    "hdfs_url = \"hdfs://45.93.138.139:9000/path/to/your/file\"\n",
    "df = spark.read.text(hdfs_url)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb20e4d-4347-494a-a6c9-2b0bd72d1bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
