{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import optuna\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports\n",
    "LOCAL_EXPORT_FOLDER_PATH='/content/exports'\n",
    "# Exports > Manual check path \n",
    "LOCAL_EXPORT_MANUAL_CHECK_PATCH_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/manual_check_patch'\n",
    "TARGET_COLUMNS=['TARGET',]\n",
    "LOCAL_EXPORT_MODELIZATION_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/modelization'\n",
    "MLFLOW_EXPERIMENT_NAME = 'generic_model_experiment'\n",
    "LOCAL_EXPORT_MODELIZATION_EVALUATION_RESULT_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/modelization_evaluation'\n",
    "\n",
    "# Export > General Settings\n",
    "TESTING_MODE=True\n",
    "TESTING_MODE_MAX_LINES=1000\n",
    "TESTING_MODE_SUB_FOLDER_NAME='testing_data'\n",
    "GENERAL_CHUNK_SIZE=100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_head_of_files(base_path, file_extension='csv', chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Parcours tous les fichiers dans le répertoire donné et affiche les premières lignes de chaque fichier CSV.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Le chemin du répertoire de base où se trouvent les fichiers.\n",
    "        file_extension (str): L'extension des fichiers à traiter (par défaut 'csv').\n",
    "        chunk_size (int): Taille des chunks pour lire les fichiers partiellement.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith(file_extension):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                \n",
    "                # Lire les premières lignes du fichier CSV\n",
    "                for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "                    print(chunk.head())\n",
    "                    break  # On ne lit qu'un seul chunk pour obtenir le head\n",
    "                print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_head_of_files(base_path=LOCAL_EXPORT_MANUAL_CHECK_PATCH_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des modèles et leurs hyperparamètres\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {\n",
    "            'C': np.logspace(-3, 3, 7),\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'colsample_bytree': [0.3, 0.7]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': lgb.LGBMClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'num_leaves': [31, 62, 127],\n",
    "            'boosting_type': ['gbdt', 'dart','goss']\n",
    "        }\n",
    "    },\n",
    "    # 'CatBoost': {\n",
    "    #     'model': cb.CatBoostClassifier(verbose=0),\n",
    "    #     'params': {\n",
    "    #         'iterations': [100, 200, 300],\n",
    "    #         'learning_rate': [0.01, 0.05, 0.1],\n",
    "    #         'depth': [3, 4, 5],\n",
    "    #         'l2_leaf_reg': [3, 5, 7]\n",
    "    #     }\n",
    "    # },\n",
    "    # 'SVM': {\n",
    "    #     'model': SVC(probability=True),\n",
    "    #     'params': {\n",
    "    #         'C': np.logspace(-3, 3, 7),\n",
    "    #         'kernel': ['linear', 'rbf', 'poly'],\n",
    "    #         'degree': [3, 4, 5]\n",
    "    #     }\n",
    "    # },\n",
    "    # 'KNN': {\n",
    "    #     'model': KNeighborsClassifier(),\n",
    "    #     'params': {\n",
    "    #         'n_neighbors': [5, 10, 20],\n",
    "    #         'weights': ['uniform', 'distance'],\n",
    "    #         'metric': ['euclidean', 'manhattan']\n",
    "    #     }\n",
    "    # },\n",
    "    # 'Neural Network': {\n",
    "    #     'model': MLPClassifier(max_iter=500),\n",
    "    #     'params': {\n",
    "    #         'hidden_layer_sizes': ['50,50', '100', '100,50'],\n",
    "    #         'activation': ['tanh', 'relu'],\n",
    "    #         'alpha': [0.0001, 0.001, 0.01]\n",
    "    #     }\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'optimisation des hyperparamètres avec Optuna\n",
    "def objective(trial, X_train, y_train):\n",
    "    classifier_name = trial.suggest_categorical('classifier', list(models.keys()))\n",
    "    classifier_info = models[classifier_name]\n",
    "    classifier = classifier_info['model']\n",
    "    params = classifier_info['params']\n",
    "\n",
    "    trial_params = {}\n",
    "    for param, values in params.items():\n",
    "        if param == 'hidden_layer_sizes':\n",
    "            hidden_layer_size_str = trial.suggest_categorical(param, values)\n",
    "            trial_params[param] = tuple(map(int, hidden_layer_size_str.split(',')))\n",
    "        elif isinstance(values[0], int):\n",
    "            trial_params[param] = trial.suggest_int(param, min(values), max(values))\n",
    "        elif isinstance(values[0], float):\n",
    "            trial_params[param] = trial.suggest_float(param, min(values), max(values))\n",
    "        else:\n",
    "            trial_params[param] = trial.suggest_categorical(param, values)\n",
    "    \n",
    "    classifier.set_params(**trial_params)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    score = cross_val_score(pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring='accuracy', error_score='raise')\n",
    "    accuracy = score.mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna optimize hyperameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'optimisation des hyperparamètres avec Optuna\n",
    "def objective(trial, X_train, y_train):\n",
    "    classifier_name = trial.suggest_categorical('classifier', list(models.keys()))\n",
    "    classifier_info = models[classifier_name]\n",
    "    classifier = classifier_info['model']\n",
    "    params = classifier_info['params']\n",
    "\n",
    "    trial_params = {}\n",
    "    for param, values in params.items():\n",
    "        if param == 'hidden_layer_sizes':\n",
    "            hidden_layer_size_str = trial.suggest_categorical(param, values)\n",
    "            trial_params[param] = tuple(map(int, hidden_layer_size_str.split(',')))\n",
    "        elif isinstance(values[0], int):\n",
    "            trial_params[param] = trial.suggest_int(param, min(values), max(values))\n",
    "        elif isinstance(values[0], float):\n",
    "            trial_params[param] = trial.suggest_float(param, min(values), max(values))\n",
    "        else:\n",
    "            trial_params[param] = trial.suggest_categorical(param, values)\n",
    "    \n",
    "    classifier.set_params(**trial_params)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    score = cross_val_score(pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring='accuracy', error_score='raise')\n",
    "    accuracy = score.mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-entrainment method for best increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour ré-entraîner un modèle jusqu'à atteindre une amélioration significative\n",
    "def retrain_model(best_pipeline, X_train, y_train, X_test, y_test, threshold=0.01, max_iter=10):\n",
    "    previous_score = 0\n",
    "    for iteration in range(max_iter):\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        y_pred = best_pipeline.predict(X_test)\n",
    "        current_score = accuracy_score(y_test, y_pred)\n",
    "        improvement = current_score - previous_score\n",
    "        if improvement < threshold:\n",
    "            break\n",
    "        previous_score = current_score\n",
    "        print(f\"Iteration {iteration + 1}, Accuracy: {current_score}, Improvement: {improvement}\")\n",
    "    return best_pipeline, current_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal to entrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "B6QYxuCQVGP1"
   },
   "outputs": [],
   "source": [
    "# Fonction d'optimisation des hyperparamètres avec Optuna\n",
    "def objective(trial, X_train, y_train, output_dir, best_classifier_name):\n",
    "    classifier_name = trial.suggest_categorical('classifier', list(models.keys()))\n",
    "    classifier_info = models[classifier_name]\n",
    "    classifier = classifier_info['model']\n",
    "    params = classifier_info['params']\n",
    "\n",
    "    trial_params = {}\n",
    "    for param, values in params.items():\n",
    "        if isinstance(values[0], int):\n",
    "            trial_params[param] = trial.suggest_int(param, min(values), max(values))\n",
    "        elif isinstance(values[0], float):\n",
    "            trial_params[param] = trial.suggest_float(param, min(values), max(values))\n",
    "        else:\n",
    "            trial_params[param] = trial.suggest_categorical(param, values)\n",
    "    \n",
    "    classifier.set_params(**trial_params)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    score = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring='accuracy')\n",
    "    accuracy = score.mean()\n",
    "    \n",
    "    # Enregistrer le modèle et les paramètres après chaque essai\n",
    "    model_path = os.path.join(output_dir, f'best_{best_classifier_name}_model.pkl')\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    \n",
    "    params_path = os.path.join(output_dir, f'best_{best_classifier_name}_params.json')\n",
    "    with open(params_path, 'w') as f:\n",
    "        json.dump(trial_params, f)\n",
    "    \n",
    "    print(f\"Model and parameters saved: {model_path}, {params_path}\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Fonction pour ré-entraîner un modèle jusqu'à atteindre une amélioration significative\n",
    "def retrain_model(best_pipeline, X_train, y_train, threshold=0.01, max_iter=10):\n",
    "    previous_score = 0\n",
    "    for iteration in range(max_iter):\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        y_pred = best_pipeline.predict(X_train)\n",
    "        current_score = accuracy_score(y_train, y_pred)\n",
    "        improvement = current_score - previous_score\n",
    "        if improvement < threshold:\n",
    "            break\n",
    "        previous_score = current_score\n",
    "        print(f\"Iteration {iteration + 1}, Accuracy: {current_score}, Improvement: {improvement}\")\n",
    "    return best_pipeline, current_score\n",
    "\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.replace(r'[^\\w\\s]', '_', regex=True)\n",
    "    return df\n",
    "\n",
    "def train_and_evaluate_models(base_path, output_folder, target_columns, max_features=5, testing=False, chunk_size=1000, testing_sub_path_name='test'):\n",
    "    all_scores = {}\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(base_path) if any(f.endswith('application_train.csv') for f in files)])\n",
    "    pbar = tqdm(total=total_files, desc=\"Processing files\")\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith('application_train.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                test_file_path = file_path.replace('application_train.csv', 'application_test.csv')\n",
    "\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                # Lire les fichiers CSV par chunks\n",
    "                for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "                    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "                    # Nettoyer les noms de colonnes\n",
    "                    chunk = clean_column_names(chunk)\n",
    "                    test_data = clean_column_names(test_data)\n",
    "\n",
    "                    for target_column in target_columns:\n",
    "                        print(f\"Using target column: {target_column}\")\n",
    "\n",
    "                        # Calculer les corrélations et sélectionner les meilleures caractéristiques\n",
    "                        correlations = chunk.corr()[target_column].abs().sort_values(ascending=False)\n",
    "                        top_features = correlations.index[1:max_features+1].tolist()\n",
    "\n",
    "                        # Séparation des features et de la cible\n",
    "                        X_train = chunk[top_features]\n",
    "                        y_train = chunk[target_column]\n",
    "\n",
    "                        X_test = test_data[top_features]\n",
    "                        y_test = test_data[target_column] if target_column in test_data.columns else None\n",
    "\n",
    "                        # Vérifier le nombre de classes dans y_train\n",
    "                        if y_train.nunique() < 2:\n",
    "                            print(f\"Skipping optimization for {target_column} as it contains only one class in the training data.\")\n",
    "                            continue\n",
    "\n",
    "                        # Déterminer le chemin de sortie\n",
    "                        relative_path = os.path.relpath(root, base_path)\n",
    "                        output_dir = os.path.join(output_folder, relative_path, target_column)\n",
    "\n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "\n",
    "                        # Optimisation des hyperparamètres avec Optuna\n",
    "                        study = optuna.create_study(direction='maximize')\n",
    "                        study.optimize(lambda trial: objective(trial, X_train, y_train, output_dir, target_column), n_trials=100)\n",
    "\n",
    "                        print('Number of finished trials: ', len(study.trials))\n",
    "                        print('Best trial:')\n",
    "                        trial = study.best_trial\n",
    "\n",
    "                        print('  Value: ', trial.value)\n",
    "                        print('  Params: ')\n",
    "                        for key, value in trial.params.items():\n",
    "                            print('    {}: {}'.format(key, value))\n",
    "\n",
    "                        # Enregistrer le meilleur modèle et ses paramètres\n",
    "                        best_classifier_name = trial.params['classifier']\n",
    "                        best_classifier_info = models[best_classifier_name]\n",
    "                        best_classifier = best_classifier_info['model']\n",
    "                        best_params = {k: v for k, v in trial.params.items() if k != 'classifier'}\n",
    "\n",
    "                        best_classifier.set_params(**best_params)\n",
    "\n",
    "                        # Création du pipeline avec le meilleur modèle\n",
    "                        best_pipeline = Pipeline(steps=[\n",
    "                            ('classifier', best_classifier)\n",
    "                        ])\n",
    "\n",
    "                        # Retrain the model with the entire training dataset\n",
    "                        best_pipeline, _ = retrain_model(best_pipeline, X_train, y_train)\n",
    "\n",
    "                        # Sauvegarder le meilleur modèle et ses paramètres\n",
    "                        best_model_path = os.path.join(output_dir, f'best_{best_classifier_name}_model.pkl')\n",
    "                        joblib.dump(best_pipeline, best_model_path)\n",
    "                        best_params_path = os.path.join(output_dir, f'best_{best_classifier_name}_params.json')\n",
    "                        with open(best_params_path, 'w') as f:\n",
    "                            json.dump(trial.params, f)\n",
    "\n",
    "                        # Enregistrer les données de test\n",
    "                        test_data_path = os.path.join(output_dir, 'test_data.csv')\n",
    "                        test_data[top_features].to_csv(test_data_path, index=False)\n",
    "\n",
    "                        # Enregistrement des données d'entrainement \n",
    "                        train_data_path = os.path.join(output_dir, 'train_data.csv')\n",
    "                        chunk.to_csv(train_data_path, index=False)\n",
    "\n",
    "                        if y_test is not None:\n",
    "                            # Évaluation du modèle\n",
    "                            y_pred = best_pipeline.predict(X_test)\n",
    "                            y_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]  # Only use the probability for the positive class\n",
    "                            accuracy = accuracy_score(y_test, y_pred)\n",
    "                            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "                            print(f\"Accuracy: {accuracy}\")\n",
    "                            print(f\"ROC AUC: {roc_auc}\")\n",
    "                            print(confusion_matrix(y_test, y_pred))\n",
    "                            print(classification_report(y_test, y_pred))\n",
    "\n",
    "                            # Logging avec mlflow\n",
    "                            mlflow.set_experiment('credit_scoring')\n",
    "                            with mlflow.start_run():\n",
    "                                mlflow.log_params(trial.params)\n",
    "                                mlflow.log_metric('accuracy', accuracy)\n",
    "                                mlflow.log_metric('roc_auc', roc_auc)\n",
    "                                mlflow.sklearn.log_model(best_pipeline, 'model')\n",
    "                                mlflow.log_artifact(file_path)\n",
    "                                mlflow.log_artifact(test_file_path)\n",
    "                                mlflow.log_artifact(test_data_path)\n",
    "\n",
    "                        print(f'Model saved at {best_model_path}')\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Visualisation des scores de validation croisée\n",
    "    model_names = list(all_scores.keys())\n",
    "    model_scores = [score for scores in all_scores.values() for score in scores]\n",
    "    model_names_repeated = [model for model in model_names for _ in range(len(all_scores[model]))]\n",
    "\n",
    "    plot_cross_val_scores(model_scores, model_names_repeated, output_folder)\n",
    "\n",
    "# Fonction de visualisation pour les scores de validation croisée\n",
    "def plot_cross_val_scores(model_scores, model_names, output_dir):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x=model_names, y=model_scores)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Cross-Validation Score')\n",
    "    plt.title('Model Comparison - Cross-Validation Scores')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(output_dir, 'cross_val_scores.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling modelization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_models(\n",
    "    base_path=LOCAL_EXPORT_MANUAL_CHECK_PATCH_FOLDER_PATH, \n",
    "    output_folder=LOCAL_EXPORT_MODELIZATION_FOLDER_PATH, \n",
    "    target_columns=TARGET_COLUMNS, \n",
    "    testing=TESTING_MODE, \n",
    "    max_features=6,\n",
    "    chunk_size=GENERAL_CHUNK_SIZE, \n",
    "    testing_sub_path_name=TESTING_MODE_SUB_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = pd.read_csv(\"./exports/manual_check_patch/testing_data/bfill/DBSCAN/ordinal/application_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1[\"TARGET\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluation des Modèles\n",
    "\n",
    "After training the models, the evaluate_models function is used to evaluate the performance of each model. This function calculates several performance metrics and also computes the business score based on the given formula.\n",
    "\n",
    "Business Score Calculation\n",
    "The business score is calculated using the following formula:\n",
    "\n",
    "Business Score\n",
    "=\n",
    "(\n",
    "Benefit\n",
    "×\n",
    "PPV\n",
    ")\n",
    "−\n",
    "(\n",
    "Cost\n",
    "×\n",
    "False Positive Rate\n",
    ")\n",
    "Business Score=(Benefit×PPV)−(Cost×False Positive Rate)\n",
    "\n",
    "Where:\n",
    "\n",
    "Benefit: The benefit obtained from a correct prediction.\n",
    "PPV (Positive Predictive Value): \n",
    "TP\n",
    "TP\n",
    "+\n",
    "FP\n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " , where TP is the number of true positives and FP is the number of false positives.\n",
    "Cost: The cost associated with an incorrect prediction.\n",
    "False Positive Rate: \n",
    "FP\n",
    "Total Negatives\n",
    "Total Negatives\n",
    "FP\n",
    "​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_test, y_pred_proba, model_name, output_dir):\n",
    "    from sklearn.metrics import RocCurveDisplay\n",
    "    RocCurveDisplay.from_predictions(y_test, y_pred_proba)\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.savefig(os.path.join(output_dir, f'roc_curve_{model_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, model_name, output_dir):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.savefig(os.path.join(output_dir, f'confusion_matrix_{model_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_models(base_path, output_folder, target_columns, chunk_size=1000, benefit=1.0, cost=1.0):\n",
    "    # Créer le dossier output_folder s'il n'existe pas\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "    performance_metrics = []\n",
    "\n",
    "    # Traverse base_path and find .pkl files\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.pkl'):\n",
    "                print(f\"Analyse of : {file}\")\n",
    "                model_path = os.path.join(root, file)\n",
    "                model_name = file.split('_')[1] if len(file.split('_')) > 1 else \"unknown_model\"\n",
    "                target_column = next((col for col in target_columns if col in root), None)\n",
    "\n",
    "                if target_column:\n",
    "                    print(f\"Analyzing model: {model_name} for target: {target_column}\")\n",
    "                    try:\n",
    "                        # Charger le modèle et les données de test\n",
    "                        best_pipeline = joblib.load(model_path)\n",
    "                        test_file_path = os.path.join(os.path.dirname(model_path), 'train_data.csv')\n",
    "                        \n",
    "                        if not os.path.exists(test_file_path):\n",
    "                            print(f\"Test file not found: {test_file_path}\")\n",
    "                            continue\n",
    "                        \n",
    "                        test_data = pd.read_csv(test_file_path, chunksize=chunk_size)\n",
    "                        for chunk in test_data:\n",
    "                            if target_column not in chunk.columns:\n",
    "                                print(f\"Target column {target_column} not in chunk columns\")\n",
    "                                continue\n",
    "\n",
    "                            X_test = chunk.drop(target_column, axis=1)\n",
    "                            y_test = chunk[target_column]\n",
    "\n",
    "                            # Aligner les colonnes des données de test avec celles d'entraînement\n",
    "                            try:\n",
    "                                X_test = X_test.reindex(columns=best_pipeline.feature_names_in_, fill_value=0)\n",
    "                            except AttributeError:\n",
    "                                # Si feature_names_in_ n'est pas disponible\n",
    "                                # Charger les colonnes d'entraînement sauvegardées (si disponibles)\n",
    "                                pass\n",
    "\n",
    "                            # Prédictions\n",
    "                            y_pred = best_pipeline.predict(X_test)\n",
    "                            y_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                            # Calcul des métriques de performance\n",
    "                            accuracy = accuracy_score(y_test, y_pred)\n",
    "                            precision = precision_score(y_test, y_pred)\n",
    "                            recall = recall_score(y_test, y_pred)\n",
    "                            f1 = f1_score(y_test, y_pred)\n",
    "                            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "                            # Matrice de confusion\n",
    "                            cm = confusion_matrix(y_test, y_pred)\n",
    "                            TP = cm[1, 1]\n",
    "                            FP = cm[0, 1]\n",
    "                            TN = cm[0, 0]\n",
    "                            FN = cm[1, 0]\n",
    "\n",
    "                            # Calcul du VPP et Taux de faux positifs\n",
    "                            VPP = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "                            taux_fp = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "\n",
    "                            # Calcul du Score Métier\n",
    "                            score_metier = (benefit * VPP) - (cost * taux_fp)\n",
    "\n",
    "                            performance_metrics.append({\n",
    "                                'model': model_name,\n",
    "                                'target': target_column,\n",
    "                                'accuracy': accuracy,\n",
    "                                'precision': precision,\n",
    "                                'recall': recall,\n",
    "                                'f1_score': f1,\n",
    "                                'roc_auc': roc_auc,\n",
    "                                'score_metier': score_metier\n",
    "                            })\n",
    "\n",
    "                            print(f\"Accuracy: {accuracy}\")\n",
    "                            print(f\"Precision: {precision}\")\n",
    "                            print(f\"Recall: {recall}\")\n",
    "                            print(f\"F1 Score: {f1}\")\n",
    "                            print(f\"ROC AUC: {roc_auc}\")\n",
    "                            print(f\"Score Métier: {score_metier}\")\n",
    "\n",
    "                            # Visualisation des résultats\n",
    "                            plot_roc_curve(y_test, y_pred_proba, model_name, root)\n",
    "                            plot_confusion_matrix(y_test, y_pred, model_name, root)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing model {model_path}: {e}\")\n",
    "\n",
    "    if performance_metrics:\n",
    "        # Convertir les métriques de performance en DataFrame\n",
    "        performance_df = pd.DataFrame(performance_metrics)\n",
    "        performance_df.to_csv(os.path.join(output_folder, 'model_performance_metrics.csv'), index=False)\n",
    "        print(\"Performance metrics exported.\")\n",
    "        return performance_df\n",
    "    else:\n",
    "        print(\"No performance metrics to export.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(\n",
    "    base_path=LOCAL_EXPORT_MODELIZATION_FOLDER_PATH, \n",
    "    output_folder=LOCAL_EXPORT_MODELIZATION_EVALUATION_RESULT_FOLDER_PATH, \n",
    "    target_columns=[\"TARGET\"], \n",
    "    chunk_size=GENERAL_CHUNK_SIZE, \n",
    "    benefit=1.0, \n",
    "    cost=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sélection du Modèle Final et Hyperparameter Tuning\n",
    "\n",
    "After evaluating the models, the best performing model can be selected and further tuned using hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_tune_best_model(performance_df, base_path, output_folder, target_column):\n",
    "    best_model_row = performance_df.loc[performance_df['roc_auc'].idxmax()]\n",
    "    best_model_name = best_model_row['model']\n",
    "    print(f\"Best model: {best_model_name} with ROC AUC: {best_model_row['roc_auc']}\")\n",
    "\n",
    "    model_path = os.path.join(output_folder, best_model_name, target_column, f'best_{best_model_name}_model.pkl')\n",
    "    best_pipeline = joblib.load(model_path)\n",
    "\n",
    "    # Tuning des hyperparamètres du meilleur modèle avec Optuna\n",
    "    X_train = pd.read_csv(os.path.join(base_path, 'application_train.csv')).drop(target_column, axis=1)\n",
    "    y_train = pd.read_csv(os.path.join(base_path, 'application_train.csv'))[target_column]\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100)\n",
    "\n",
    "    print('Number of finished trials: ', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('  Value: ', trial.value)\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))\n",
    "\n",
    "    best_classifier = models[best_model_name]['model']\n",
    "    best_params = {k: v for k, v in trial.params.items() if k != 'classifier'}\n",
    "    best_classifier.set_params(**best_params)\n",
    "\n",
    "    best_pipeline = Pipeline(steps=[\n",
    "        ('classifier', best_classifier)\n",
    "    ])\n",
    "\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "    joblib.dump(best_pipeline, model_path)\n",
    "    print(f\"Model saved at {model_path}\")\n",
    "\n",
    "    return best_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interprétabilité et Explicabilité\n",
    "Using SHAP to explain the predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_model(best_pipeline, X_train, output_folder):\n",
    "    explainer = shap.TreeExplainer(best_pipeline.named_steps['classifier'])\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "    shap.summary_plot(shap_values, X_train)\n",
    "    plt.savefig(os.path.join(output_folder, 'shap_summary_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "    shap.initjs()\n",
    "    shap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:])\n",
    "    plt.savefig(os.path.join(output_folder, 'shap_force_plot.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(performance_df):\n",
    "    best_model_row = performance_df.loc[performance_df['roc_auc'].idxmax()]\n",
    "    best_model_name = best_model_row['model']\n",
    "    summary = f\"Best model: {best_model_name}\\n\"\n",
    "    summary += f\"Accuracy: {best_model_row['accuracy']}\\n\"\n",
    "    summary += f\"Precision: {best_model_row['precision']}\\n\"\n",
    "    summary += f\"Recall: {best_model_row['recall']}\\n\"\n",
    "    summary += f\"F1 Score: {best_model_row['f1_score']}\\n\"\n",
    "    summary += f\"ROC AUC: {best_model_row['roc_auc']}\\n\"\n",
    "    summary += f\"Business Score: {best_model_row['business_score']}\\n\"\n",
    "    \n",
    "    print(summary)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'model_summary.txt'), 'w') as f:\n",
    "        f.write(summary)\n",
    "\n",
    "    print(\"Summary saved.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
