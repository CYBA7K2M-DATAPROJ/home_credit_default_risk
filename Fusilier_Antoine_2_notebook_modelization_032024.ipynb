{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import joblib\n",
    "import optuna\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports\n",
    "LOCAL_EXPORT_FOLDER_PATH='/content/exports'\n",
    "# Exports > Manual check path \n",
    "LOCAL_EXPORT_MANUAL_CHECK_PATCH_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/manual_check_patch'\n",
    "TARGET_COLUMNS=['TARGET',]\n",
    "LOCAL_EXPORT_MODELIZATION_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/modelization'\n",
    "MLFLOW_EXPERIMENT_NAME = 'generic_model_experiment'\n",
    "# Export > General Settings\n",
    "TESTING_MODE=True\n",
    "TESTING_MODE_MAX_LINES=1000\n",
    "TESTING_MODE_SUB_FOLDER_NAME='testing_data'\n",
    "GENERAL_CHUNK_SIZE=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des modèles et leurs hyperparamètres\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {\n",
    "            'C': np.logspace(-3, 3, 7),\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'colsample_bytree': [0.3, 0.7]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': lgb.LGBMClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'num_leaves': [31, 62, 127],\n",
    "            'boosting_type': ['gbdt', 'dart']\n",
    "        }\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'model': cb.CatBoostClassifier(verbose=0),\n",
    "        'params': {\n",
    "            'iterations': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'depth': [3, 4, 5],\n",
    "            'l2_leaf_reg': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(probability=True),\n",
    "        'params': {\n",
    "            'C': np.logspace(-3, 3, 7),\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'degree': [3, 4, 5]\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [5, 10, 20],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan']\n",
    "        }\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'model': MLPClassifier(max_iter=500),\n",
    "        'params': {\n",
    "            'hidden_layer_sizes': ['50,50', '100', '100,50'],\n",
    "            'activation': ['tanh', 'relu'],\n",
    "            'alpha': [0.0001, 0.001, 0.01]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'optimisation des hyperparamètres avec Optuna\n",
    "def objective(trial, X_train, y_train):\n",
    "    classifier_name = trial.suggest_categorical('classifier', list(models.keys()))\n",
    "    classifier_info = models[classifier_name]\n",
    "    classifier = classifier_info['model']\n",
    "    params = classifier_info['params']\n",
    "\n",
    "    trial_params = {}\n",
    "    for param, values in params.items():\n",
    "        if param == 'hidden_layer_sizes':\n",
    "            hidden_layer_size_str = trial.suggest_categorical(param, values)\n",
    "            trial_params[param] = tuple(map(int, hidden_layer_size_str.split(',')))\n",
    "        elif isinstance(values[0], int):\n",
    "            trial_params[param] = trial.suggest_int(param, min(values), max(values))\n",
    "        elif isinstance(values[0], float):\n",
    "            trial_params[param] = trial.suggest_float(param, min(values), max(values))\n",
    "        else:\n",
    "            trial_params[param] = trial.suggest_categorical(param, values)\n",
    "    \n",
    "    classifier.set_params(**trial_params)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    score = cross_val_score(pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring='accuracy', error_score='raise')\n",
    "    accuracy = score.mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6QYxuCQVGP1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Fonction d'optimisation des hyperparamètres avec Optuna\n",
    "def objective(trial, X_train, y_train):\n",
    "    classifier_name = trial.suggest_categorical('classifier', list(models.keys()))\n",
    "    classifier_info = models[classifier_name]\n",
    "    classifier = classifier_info['model']\n",
    "    params = classifier_info['params']\n",
    "\n",
    "    trial_params = {}\n",
    "    for param, values in params.items():\n",
    "        if param == 'hidden_layer_sizes':\n",
    "            hidden_layer_size_str = trial.suggest_categorical(param, values)\n",
    "            trial_params[param] = tuple(map(int, hidden_layer_size_str.split(',')))\n",
    "        elif isinstance(values[0], int):\n",
    "            trial_params[param] = trial.suggest_int(param, min(values), max(values))\n",
    "        elif isinstance(values[0], float):\n",
    "            trial_params[param] = trial.suggest_float(param, min(values), max(values))\n",
    "        else:\n",
    "            trial_params[param] = trial.suggest_categorical(param, values)\n",
    "    \n",
    "    classifier.set_params(**trial_params)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    score = cross_val_score(pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring='accuracy', error_score='raise')\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Fonction pour ré-entraîner un modèle jusqu'à atteindre une amélioration significative\n",
    "def retrain_model(best_pipeline, X_train, y_train, X_test, y_test, threshold=0.01, max_iter=10):\n",
    "    previous_score = 0\n",
    "    for iteration in range(max_iter):\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        y_pred = best_pipeline.predict(X_test)\n",
    "        current_score = accuracy_score(y_test, y_pred)\n",
    "        improvement = current_score - previous_score\n",
    "        if improvement < threshold:\n",
    "            break\n",
    "        previous_score = current_score\n",
    "        print(f\"Iteration {iteration + 1}, Accuracy: {current_score}, Improvement: {improvement}\")\n",
    "    return best_pipeline, current_score\n",
    "\n",
    "# Fonction principale pour entraîner et évaluer les modèles\n",
    "def train_and_evaluate_models(base_path, output_folder, target_columns, max_features=5, testing=False, chunk_size=1000, testing_sub_path_name='test'):\n",
    "    all_scores = {}\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(base_path) if any(f.endswith('application_train.csv') for f in files)])\n",
    "    pbar = tqdm(total=total_files, desc=\"Processing files\")\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith('application_train.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                test_file_path = file_path.replace('application_train.csv', 'application_test.csv')\n",
    "\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                # Lire les fichiers CSV par chunks\n",
    "                for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "                    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "                    for target_column in target_columns:\n",
    "                        print(f\"Using target column: {target_column}\")\n",
    "\n",
    "                        # Calculer les corrélations et sélectionner les meilleures caractéristiques\n",
    "                        correlations = chunk.corr()[target_column].abs().sort_values(ascending=False)\n",
    "                        top_features = correlations.index[1:max_features+1].tolist()\n",
    "\n",
    "                        # Séparation des features et de la cible\n",
    "                        X_train = chunk[top_features]\n",
    "                        y_train = chunk[target_column]\n",
    "                        \n",
    "                        if target_column in test_data.columns:\n",
    "                            X_test = test_data[top_features]\n",
    "                            y_test = test_data[target_column]\n",
    "                        else:\n",
    "                            X_test = test_data\n",
    "                            y_test = None\n",
    "                            print(f\"Target column {target_column} not found in test data. Skipping evaluation.\")\n",
    "\n",
    "                        # Optimisation des hyperparamètres avec Optuna\n",
    "                        study = optuna.create_study(direction='maximize')\n",
    "                        study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100)\n",
    "\n",
    "                        print('Number of finished trials: ', len(study.trials))\n",
    "                        print('Best trial:')\n",
    "                        trial = study.best_trial\n",
    "\n",
    "                        print('  Value: ', trial.value)\n",
    "                        print('  Params: ')\n",
    "                        for key, value in trial.params.items():\n",
    "                            print('    {}: {}'.format(key, value))\n",
    "\n",
    "                        # Entraîner le meilleur modèle\n",
    "                        best_classifier_name = trial.params['classifier']\n",
    "                        best_classifier_info = models[best_classifier_name]\n",
    "                        best_classifier = best_classifier_info['model']\n",
    "                        best_params = {k: v for k, v in trial.params.items() if k != 'classifier'}\n",
    "\n",
    "                        best_classifier.set_params(**best_params)\n",
    "\n",
    "                        # Création du pipeline avec le meilleur modèle\n",
    "                        best_pipeline = Pipeline(steps=[\n",
    "                            ('classifier', best_classifier)\n",
    "                        ])\n",
    "\n",
    "                        best_pipeline, best_score = retrain_model(best_pipeline, X_train, y_train, X_test, y_test)\n",
    "\n",
    "                        # Stocker les scores de validation croisée\n",
    "                        if best_classifier_name not in all_scores:\n",
    "                            all_scores[best_classifier_name] = []\n",
    "                        all_scores[best_classifier_name].extend(study.trials_dataframe().value.values)\n",
    "\n",
    "                        # Déterminer le chemin de sortie\n",
    "                        relative_path = os.path.relpath(root, base_path)\n",
    "                        if testing:\n",
    "                            output_dir = os.path.join(output_folder, testing_sub_path_name, relative_path, target_column)\n",
    "                        else:\n",
    "                            output_dir = os.path.join(output_folder, relative_path, target_column)\n",
    "                            \n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "\n",
    "                        model_path = os.path.join(output_dir, f'best_{best_classifier_name}_model.pkl')\n",
    "                        joblib.dump(best_pipeline, model_path)\n",
    "\n",
    "                        if y_test is not None:\n",
    "                            # Évaluation du modèle\n",
    "                            y_pred = best_pipeline.predict(X_test)\n",
    "                            y_pred_proba = best_pipeline.predict_proba(X_test)\n",
    "                            accuracy = accuracy_score(y_test, y_pred)\n",
    "                            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "                            print(f\"Accuracy: {accuracy}\")\n",
    "                            print(f\"ROC AUC: {roc_auc}\")\n",
    "                            print(confusion_matrix(y_test, y_pred))\n",
    "                            print(classification_report(y_test, y_pred))\n",
    "\n",
    "                            # Logging avec mlflow\n",
    "                            mlflow.set_experiment('credit_scoring')\n",
    "                            with mlflow.start_run():\n",
    "                                mlflow.log_params(trial.params)\n",
    "                                mlflow.log_metric('accuracy', accuracy)\n",
    "                                mlflow.log_metric('roc_auc', roc_auc)\n",
    "                                mlflow.sklearn.log_model(best_pipeline, 'model')\n",
    "                                mlflow.log_artifact(file_path)\n",
    "                                mlflow.log_artifact(test_file_path)\n",
    "\n",
    "                            print(f'Model saved at {model_path}')\n",
    "\n",
    "                        else:\n",
    "                            print(f\"Skipping model evaluation for {target_column} as target column is not in test data.\")\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Visualisation des scores de validation croisée\n",
    "    model_names = list(all_scores.keys())\n",
    "    model_scores = [score for scores in all_scores.values() for score in scores]\n",
    "    model_names_repeated = [model for model in model_names for _ in range(len(all_scores[model]))]\n",
    "\n",
    "    plot_cross_val_scores(model_scores, model_names_repeated, output_folder)\n",
    "\n",
    "# Fonction de visualisation pour les scores de validation croisée\n",
    "def plot_cross_val_scores(model_scores, model_names, output_dir):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x=model_names, y=model_scores)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Cross-Validation Score')\n",
    "    plt.title('Model Comparison - Cross-Validation Scores')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(output_dir, 'cross_val_scores.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing = pd.read_csv(\"/content/exports/manual_check_patch/testing_data/mean/LOF/ordinal/application_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
       "      <th>FLAG_DOCUMENT_9</th>\n",
       "      <th>TOTAL_CREDIT_BUREAU_REQUESTS</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>HOUSETYPE_MODE</th>\n",
       "      <th>LIVINGAPARTMENTS_MODE</th>\n",
       "      <th>FLAG_CONT_MOBILE</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>BASEMENTAREA_AVG</th>\n",
       "      <th>...</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>APARTMENTS_MODE</th>\n",
       "      <th>APARTMENTS_MEDI</th>\n",
       "      <th>BASEMENTAREA_MEDI</th>\n",
       "      <th>COMMONAREA_MODE</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>YEARS_ID_PUBLISH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>YEARS_BUILD_MODE</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38419.155</td>\n",
       "      <td>0.007264</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097535</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.08623</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.109244</td>\n",
       "      <td>0.113539</td>\n",
       "      <td>0.085864</td>\n",
       "      <td>0.037053</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6.882192</td>\n",
       "      <td>0</td>\n",
       "      <td>0.757553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67500.000</td>\n",
       "      <td>0.007264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097535</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.08623</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.085864</td>\n",
       "      <td>0.037053</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.841096</td>\n",
       "      <td>-2717</td>\n",
       "      <td>0.757553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108000.000</td>\n",
       "      <td>0.007264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097535</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.08623</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.109244</td>\n",
       "      <td>0.113539</td>\n",
       "      <td>0.085864</td>\n",
       "      <td>0.037053</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.591781</td>\n",
       "      <td>-1317</td>\n",
       "      <td>0.757553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000.000</td>\n",
       "      <td>0.007264</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097535</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.08623</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.109244</td>\n",
       "      <td>0.113539</td>\n",
       "      <td>0.085864</td>\n",
       "      <td>0.037053</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.860274</td>\n",
       "      <td>-2038</td>\n",
       "      <td>0.757553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135000.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.037053</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.931507</td>\n",
       "      <td>-4286</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AMT_INCOME_TOTAL  NONLIVINGAPARTMENTS_MODE  FLAG_DOCUMENT_9  \\\n",
       "0         38419.155                  0.007264                0   \n",
       "1         67500.000                  0.007264                0   \n",
       "2        108000.000                  0.007264                0   \n",
       "3         90000.000                  0.007264                0   \n",
       "4        135000.000                  0.000000                0   \n",
       "\n",
       "   TOTAL_CREDIT_BUREAU_REQUESTS  FLAG_EMAIL  HOUSETYPE_MODE  \\\n",
       "0                             2           0             0.0   \n",
       "1                             1           0             0.0   \n",
       "2                             1           0             0.0   \n",
       "3                             2           0             0.0   \n",
       "4                             0           0             0.0   \n",
       "\n",
       "   LIVINGAPARTMENTS_MODE  FLAG_CONT_MOBILE  REGION_RATING_CLIENT  \\\n",
       "0               0.097535                 1                     2   \n",
       "1               0.097535                 1                     2   \n",
       "2               0.097535                 1                     2   \n",
       "3               0.097535                 1                     2   \n",
       "4               0.064000                 1                     2   \n",
       "\n",
       "   BASEMENTAREA_AVG  ...  REGION_RATING_CLIENT_W_CITY  APARTMENTS_MODE  \\\n",
       "0           0.08623  ...                            2         0.109244   \n",
       "1           0.08623  ...                            2         0.084000   \n",
       "2           0.08623  ...                            2         0.109244   \n",
       "3           0.08623  ...                            2         0.109244   \n",
       "4           0.08000  ...                            2         0.074000   \n",
       "\n",
       "   APARTMENTS_MEDI  BASEMENTAREA_MEDI  COMMONAREA_MODE  ORGANIZATION_TYPE  \\\n",
       "0         0.113539           0.085864         0.037053               36.0   \n",
       "1         0.083000           0.085864         0.037053                3.0   \n",
       "2         0.113539           0.085864         0.037053                6.0   \n",
       "3         0.113539           0.085864         0.037053               15.0   \n",
       "4         0.073000           0.080000         0.037053               25.0   \n",
       "\n",
       "   YEARS_ID_PUBLISH  DAYS_EMPLOYED  YEARS_BUILD_MODE  TARGET  \n",
       "0          6.882192              0          0.757553       0  \n",
       "1          8.841096          -2717          0.757553       0  \n",
       "2         10.591781          -1317          0.757553       0  \n",
       "3         10.860274          -2038          0.757553       0  \n",
       "4          4.931507          -4286          0.712000       0  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-01 11:56:38,644] A new study created in memory with name: no-name-6cc39c00-1d89-4c6f-9329-db733cd2b64b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /content/exports/manual_check_patch/testing_data/mean/LOF/ordinal/application_train.csv\n",
      "Using target column: TARGET\n",
      "Target column TARGET not found in test data. Skipping evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-01 11:56:39,862] Trial 0 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 170, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 11:56:40,856] Trial 1 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 196, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 11:56:41,806] Trial 2 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 195, 'max_depth': 22, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 11:56:42,662] Trial 3 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 241, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 11:56:43,084] Trial 4 finished with value: 0.8863636363636364 and parameters: {'classifier': 'Gradient Boosting', 'n_estimators': 131, 'learning_rate': 0.03343617096554268, 'max_depth': 5, 'subsample': 0.9296377303173755}. Best is trial 0 with value: 0.9181818181818182.\n",
      "/usr/local/lib/python3.8/dist-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.8/dist-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.8/dist-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "[I 2024-07-01 11:56:43,130] Trial 5 finished with value: 0.9136363636363635 and parameters: {'classifier': 'Neural Network', 'hidden_layer_sizes': (100, 50), 'activation': 'relu', 'alpha': 0.0014805743872506154}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 11:56:43,371] Trial 6 finished with value: 0.9181818181818182 and parameters: {'classifier': 'XGBoost', 'n_estimators': 122, 'learning_rate': 0.045911606132492115, 'max_depth': 4, 'colsample_bytree': 0.3610887681298402}. Best is trial 0 with value: 0.9181818181818182.\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:158: UserWarning: [11:56:43] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:158: UserWarning: [11:56:43] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:158: UserWarning: [11:56:43] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:158: UserWarning: [11:56:43] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:158: UserWarning: [11:56:43] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_models(\n",
    "    base_path=LOCAL_EXPORT_MANUAL_CHECK_PATCH_FOLDER_PATH, \n",
    "    output_folder=LOCAL_EXPORT_MODELIZATION_FOLDER_PATH, \n",
    "    target_columns=TARGET_COLUMNS, \n",
    "    testing=TESTING_MODE, \n",
    "    chunk_size=GENERAL_CHUNK_SIZE, \n",
    "    testing_sub_path_name=TESTING_MODE_SUB_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'optimisation des hyperparamètres avec Optuna\n",
    "def objective(trial, X_train, y_train):\n",
    "    classifier_name = trial.suggest_categorical('classifier', list(models.keys()))\n",
    "    classifier_info = models[classifier_name]\n",
    "    classifier = classifier_info['model']\n",
    "    params = classifier_info['params']\n",
    "\n",
    "    trial_params = {}\n",
    "    for param, values in params.items():\n",
    "        if param == 'hidden_layer_sizes':\n",
    "            hidden_layer_size_str = trial.suggest_categorical(param, values)\n",
    "            trial_params[param] = tuple(map(int, hidden_layer_size_str.split(',')))\n",
    "        elif isinstance(values[0], int):\n",
    "            trial_params[param] = trial.suggest_int(param, min(values), max(values))\n",
    "        elif isinstance(values[0], float):\n",
    "            trial_params[param] = trial.suggest_float(param, min(values), max(values))\n",
    "        else:\n",
    "            trial_params[param] = trial.suggest_categorical(param, values)\n",
    "    \n",
    "    classifier.set_params(**trial_params)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    score = cross_val_score(pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring='accuracy', error_score='raise')\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Fonction pour ré-entraîner un modèle jusqu'à atteindre une amélioration significative\n",
    "def retrain_model(best_pipeline, X_train, y_train, X_test, y_test, threshold=0.01, max_iter=10):\n",
    "    previous_score = 0\n",
    "    for iteration in range(max_iter):\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        y_pred = best_pipeline.predict(X_test)\n",
    "        current_score = accuracy_score(y_test, y_pred)\n",
    "        improvement = current_score - previous_score\n",
    "        if improvement < threshold:\n",
    "            break\n",
    "        previous_score = current_score\n",
    "        print(f\"Iteration {iteration + 1}, Accuracy: {current_score}, Improvement: {improvement}\")\n",
    "    return best_pipeline, current_score\n",
    "\n",
    "# Fonction principale pour entraîner et évaluer les modèles\n",
    "def train_and_evaluate_models(base_path, output_folder, target_columns, max_features=5, testing=False, chunk_size=1000, testing_sub_path_name='test'):\n",
    "    all_scores = {}\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(base_path) if any(f.endswith('application_train.csv') for f in files)])\n",
    "    pbar = tqdm(total=total_files, desc=\"Processing files\")\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith('application_train.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                test_file_path = file_path.replace('application_train.csv', 'application_test.csv')\n",
    "\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                # Lire les fichiers CSV par chunks\n",
    "                for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "                    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "                    for target_column in target_columns:\n",
    "                        print(f\"Using target column: {target_column}\")\n",
    "\n",
    "                        # Calculer les corrélations et sélectionner les meilleures caractéristiques\n",
    "                        correlations = chunk.corr()[target_column].abs().sort_values(ascending=False)\n",
    "                        top_features = correlations.index[1:max_features+1].tolist()\n",
    "\n",
    "                        # Séparation des features et de la cible\n",
    "                        X_train = chunk[top_features]\n",
    "                        y_train = chunk[target_column]\n",
    "                        \n",
    "                        if target_column in test_data.columns:\n",
    "                            X_test = test_data[top_features]\n",
    "                            y_test = test_data[target_column]\n",
    "                        else:\n",
    "                            X_test = test_data\n",
    "                            y_test = None\n",
    "                            print(f\"Target column {target_column} not found in test data. Skipping evaluation.\")\n",
    "\n",
    "                        # Optimisation des hyperparamètres avec Optuna\n",
    "                        study = optuna.create_study(direction='maximize')\n",
    "                        study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100)\n",
    "\n",
    "                        print('Number of finished trials: ', len(study.trials))\n",
    "                        print('Best trial:')\n",
    "                        trial = study.best_trial\n",
    "\n",
    "                        print('  Value: ', trial.value)\n",
    "                        print('  Params: ')\n",
    "                        for key, value in trial.params.items():\n",
    "                            print('    {}: {}'.format(key, value))\n",
    "\n",
    "                        # Entraîner le meilleur modèle\n",
    "                        best_classifier_name = trial.params['classifier']\n",
    "                        best_classifier_info = models[best_classifier_name]\n",
    "                        best_classifier = best_classifier_info['model']\n",
    "                        best_params = {k: v for k, v in trial.params.items() if k != 'classifier'}\n",
    "\n",
    "                        best_classifier.set_params(**best_params)\n",
    "\n",
    "                        # Création du pipeline avec le meilleur modèle\n",
    "                        best_pipeline = Pipeline(steps=[\n",
    "                            ('classifier', best_classifier)\n",
    "                        ])\n",
    "\n",
    "                        best_pipeline, best_score = retrain_model(best_pipeline, X_train, y_train, X_test, y_test)\n",
    "\n",
    "                        # Stocker les scores de validation croisée\n",
    "                        all_scores[best_classifier_name] = study.trials_dataframe().value.values\n",
    "\n",
    "                        # Déterminer le chemin de sortie\n",
    "                        relative_path = os.path.relpath(root, base_path)\n",
    "                        if testing:\n",
    "                            output_dir = os.path.join(output_folder, testing_sub_path_name, relative_path, target_column)\n",
    "                        else:\n",
    "                            output_dir = os.path.join(output_folder, relative_path, target_column)\n",
    "                            \n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "\n",
    "                        model_path = os.path.join(output_dir, f'best_{best_classifier_name}_model.pkl')\n",
    "                        joblib.dump(best_pipeline, model_path)\n",
    "\n",
    "                        if y_test is not None:\n",
    "                            # Évaluation du modèle\n",
    "                            y_pred = best_pipeline.predict(X_test)\n",
    "                            y_pred_proba = best_pipeline.predict_proba(X_test)\n",
    "                            accuracy = accuracy_score(y_test, y_pred)\n",
    "                            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "                            print(f\"Accuracy: {accuracy}\")\n",
    "                            print(f\"ROC AUC: {roc_auc}\")\n",
    "                            print(confusion_matrix(y_test, y_pred))\n",
    "                            print(classification_report(y_test, y_pred))\n",
    "\n",
    "                            # Logging avec mlflow\n",
    "                            mlflow.set_experiment('credit_scoring')\n",
    "                            with mlflow.start_run():\n",
    "                                mlflow.log_params(trial.params)\n",
    "                                mlflow.log_metric('accuracy', accuracy)\n",
    "                                mlflow.log_metric('roc_auc', roc_auc)\n",
    "                                mlflow.sklearn.log_model(best_pipeline, 'model')\n",
    "                                mlflow.log_artifact(file_path)\n",
    "                                mlflow.log_artifact(test_file_path)\n",
    "\n",
    "                            print(f'Model saved at {model_path}')\n",
    "\n",
    "                        else:\n",
    "                            print(f\"Skipping model evaluation for {target_column} as target column is not in test data.\")\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Visualisation des scores de validation croisée\n",
    "    model_names = list(all_scores.keys())\n",
    "    model_scores = [score for scores in all_scores.values() for score in scores]\n",
    "    model_names_repeated = [model for model in model_names for _ in range(len(all_scores[model]))]\n",
    "\n",
    "    plot_cross_val_scores(model_scores, model_names_repeated, output_folder)\n",
    "\n",
    "# Fonction de visualisation pour les scores de validation croisée\n",
    "def plot_cross_val_scores(model_scores, model_names, output_dir):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x=model_names, y=model_scores)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Cross-Validation Score')\n",
    "    plt.title('Model Comparison - Cross-Validation Scores')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(output_dir, 'cross_val_scores.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
