{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import joblib\n",
    "import optuna\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports\n",
    "LOCAL_EXPORT_FOLDER_PATH='/content/exports'\n",
    "# Exports > Manual check path \n",
    "LOCAL_EXPORT_MANUAL_CHECK_PATCH_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/manual_check_patch'\n",
    "TARGET_COLUMNS=['TARGET',]\n",
    "LOCAL_EXPORT_MODELIZATION_FOLDER_PATH=LOCAL_EXPORT_FOLDER_PATH+'/modelization'\n",
    "MLFLOW_EXPERIMENT_NAME = 'generic_model_experiment'\n",
    "# Export > General Settings\n",
    "TESTING_MODE=True\n",
    "TESTING_MODE_MAX_LINES=1000\n",
    "TESTING_MODE_SUB_FOLDER_NAME='testing_data'\n",
    "GENERAL_CHUNK_SIZE=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_head_of_files(base_path, file_extension='csv', chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Parcours tous les fichiers dans le répertoire donné et affiche les premières lignes de chaque fichier CSV.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Le chemin du répertoire de base où se trouvent les fichiers.\n",
    "        file_extension (str): L'extension des fichiers à traiter (par défaut 'csv').\n",
    "        chunk_size (int): Taille des chunks pour lire les fichiers partiellement.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith(file_extension):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                \n",
    "                # Lire les premières lignes du fichier CSV\n",
    "                for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "                    print(chunk.head())\n",
    "                    break  # On ne lit qu'un seul chunk pour obtenir le head\n",
    "                print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_head_of_files(base_path=LOCAL_EXPORT_MANUAL_CHECK_PATCH_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des modèles et leurs hyperparamètres\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {\n",
    "            'C': np.logspace(-3, 3, 7),\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    # 'Gradient Boosting': {\n",
    "    #     'model': GradientBoostingClassifier(),\n",
    "    #     'params': {\n",
    "    #         'n_estimators': [100, 200, 300],\n",
    "    #         'learning_rate': [0.01, 0.05, 0.1],\n",
    "    #         'max_depth': [3, 4, 5],\n",
    "    #         'subsample': [0.8, 0.9, 1.0]\n",
    "    #     }\n",
    "    # },\n",
    "    # 'XGBoost': {\n",
    "    #     'model': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    #     'params': {\n",
    "    #         'n_estimators': [100, 200, 300],\n",
    "    #         'learning_rate': [0.01, 0.05, 0.1],\n",
    "    #         'max_depth': [3, 4, 5],\n",
    "    #         'colsample_bytree': [0.3, 0.7]\n",
    "    #     }\n",
    "    # },\n",
    "    # 'LightGBM': {\n",
    "    #     'model': lgb.LGBMClassifier(),\n",
    "    #     'params': {\n",
    "    #         'n_estimators': [100, 200, 300],\n",
    "    #         'learning_rate': [0.01, 0.05, 0.1],\n",
    "    #         'num_leaves': [31, 62, 127],\n",
    "    #         'boosting_type': ['gbdt', 'dart']\n",
    "    #     }\n",
    "    # },\n",
    "    # 'CatBoost': {\n",
    "    #     'model': cb.CatBoostClassifier(verbose=0),\n",
    "    #     'params': {\n",
    "    #         'iterations': [100, 200, 300],\n",
    "    #         'learning_rate': [0.01, 0.05, 0.1],\n",
    "    #         'depth': [3, 4, 5],\n",
    "    #         'l2_leaf_reg': [3, 5, 7]\n",
    "    #     }\n",
    "    # },\n",
    "    # 'SVM': {\n",
    "    #     'model': SVC(probability=True),\n",
    "    #     'params': {\n",
    "    #         'C': np.logspace(-3, 3, 7),\n",
    "    #         'kernel': ['linear', 'rbf', 'poly'],\n",
    "    #         'degree': [3, 4, 5]\n",
    "    #     }\n",
    "    # },\n",
    "    # 'KNN': {\n",
    "    #     'model': KNeighborsClassifier(),\n",
    "    #     'params': {\n",
    "    #         'n_neighbors': [5, 10, 20],\n",
    "    #         'weights': ['uniform', 'distance'],\n",
    "    #         'metric': ['euclidean', 'manhattan']\n",
    "    #     }\n",
    "    # },\n",
    "    # 'Neural Network': {\n",
    "    #     'model': MLPClassifier(max_iter=500),\n",
    "    #     'params': {\n",
    "    #         'hidden_layer_sizes': ['50,50', '100', '100,50'],\n",
    "    #         'activation': ['tanh', 'relu'],\n",
    "    #         'alpha': [0.0001, 0.001, 0.01]\n",
    "    #     }\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'optimisation des hyperparamètres avec Optuna\n",
    "def objective(trial, X_train, y_train):\n",
    "    classifier_name = trial.suggest_categorical('classifier', list(models.keys()))\n",
    "    classifier_info = models[classifier_name]\n",
    "    classifier = classifier_info['model']\n",
    "    params = classifier_info['params']\n",
    "\n",
    "    trial_params = {}\n",
    "    for param, values in params.items():\n",
    "        if param == 'hidden_layer_sizes':\n",
    "            hidden_layer_size_str = trial.suggest_categorical(param, values)\n",
    "            trial_params[param] = tuple(map(int, hidden_layer_size_str.split(',')))\n",
    "        elif isinstance(values[0], int):\n",
    "            trial_params[param] = trial.suggest_int(param, min(values), max(values))\n",
    "        elif isinstance(values[0], float):\n",
    "            trial_params[param] = trial.suggest_float(param, min(values), max(values))\n",
    "        else:\n",
    "            trial_params[param] = trial.suggest_categorical(param, values)\n",
    "    \n",
    "    classifier.set_params(**trial_params)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    score = cross_val_score(pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring='accuracy', error_score='raise')\n",
    "    accuracy = score.mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna optimize hyperameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'optimisation des hyperparamètres avec Optuna\n",
    "def objective(trial, X_train, y_train):\n",
    "    classifier_name = trial.suggest_categorical('classifier', list(models.keys()))\n",
    "    classifier_info = models[classifier_name]\n",
    "    classifier = classifier_info['model']\n",
    "    params = classifier_info['params']\n",
    "\n",
    "    trial_params = {}\n",
    "    for param, values in params.items():\n",
    "        if param == 'hidden_layer_sizes':\n",
    "            hidden_layer_size_str = trial.suggest_categorical(param, values)\n",
    "            trial_params[param] = tuple(map(int, hidden_layer_size_str.split(',')))\n",
    "        elif isinstance(values[0], int):\n",
    "            trial_params[param] = trial.suggest_int(param, min(values), max(values))\n",
    "        elif isinstance(values[0], float):\n",
    "            trial_params[param] = trial.suggest_float(param, min(values), max(values))\n",
    "        else:\n",
    "            trial_params[param] = trial.suggest_categorical(param, values)\n",
    "    \n",
    "    classifier.set_params(**trial_params)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    score = cross_val_score(pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring='accuracy', error_score='raise')\n",
    "    accuracy = score.mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-entrainment method for best increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour ré-entraîner un modèle jusqu'à atteindre une amélioration significative\n",
    "def retrain_model(best_pipeline, X_train, y_train, X_test, y_test, threshold=0.01, max_iter=10):\n",
    "    previous_score = 0\n",
    "    for iteration in range(max_iter):\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        y_pred = best_pipeline.predict(X_test)\n",
    "        current_score = accuracy_score(y_test, y_pred)\n",
    "        improvement = current_score - previous_score\n",
    "        if improvement < threshold:\n",
    "            break\n",
    "        previous_score = current_score\n",
    "        print(f\"Iteration {iteration + 1}, Accuracy: {current_score}, Improvement: {improvement}\")\n",
    "    return best_pipeline, current_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal to entrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "B6QYxuCQVGP1"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(base_path, output_folder, target_columns, max_features=5, testing=False, chunk_size=1000, testing_sub_path_name='test'):\n",
    "    all_scores = {}\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(base_path) if any(f.endswith('application_train.csv') for f in files)])\n",
    "    pbar = tqdm(total=total_files, desc=\"Processing files\")\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith('application_train.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                test_file_path = file_path.replace('application_train.csv', 'application_test.csv')\n",
    "\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                # Lire les fichiers CSV par chunks\n",
    "                for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "                    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "                    for target_column in target_columns:\n",
    "                        print(f\"Using target column: {target_column}\")\n",
    "\n",
    "                        # Calculer les corrélations et sélectionner les meilleures caractéristiques\n",
    "                        correlations = chunk.corr()[target_column].abs().sort_values(ascending=False)\n",
    "                        top_features = correlations.index[1:max_features+1].tolist()\n",
    "\n",
    "                        # Séparation des features et de la cible\n",
    "                        X_train = chunk[top_features]\n",
    "                        y_train = chunk[target_column]\n",
    "                        \n",
    "                        if target_column in test_data.columns:\n",
    "                            X_test = test_data[top_features]\n",
    "                            y_test = test_data[target_column]\n",
    "                        else:\n",
    "                            X_test = test_data\n",
    "                            y_test = None\n",
    "                            print(f\"Target column {target_column} not found in test data. Skipping evaluation.\")\n",
    "\n",
    "                        # Optimisation des hyperparamètres avec Optuna\n",
    "                        study = optuna.create_study(direction='maximize')\n",
    "                        study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100)\n",
    "\n",
    "                        print('Number of finished trials: ', len(study.trials))\n",
    "                        print('Best trial:')\n",
    "                        trial = study.best_trial\n",
    "\n",
    "                        print('  Value: ', trial.value)\n",
    "                        print('  Params: ')\n",
    "                        for key, value in trial.params.items():\n",
    "                            print('    {}: {}'.format(key, value))\n",
    "\n",
    "                        # Entraîner le meilleur modèle\n",
    "                        best_classifier_name = trial.params['classifier']\n",
    "                        best_classifier_info = models[best_classifier_name]\n",
    "                        best_classifier = best_classifier_info['model']\n",
    "                        best_params = {k: v for k, v in trial.params.items() if k != 'classifier'}\n",
    "\n",
    "                        best_classifier.set_params(**best_params)\n",
    "\n",
    "                        # Création du pipeline avec le meilleur modèle\n",
    "                        best_pipeline = Pipeline(steps=[\n",
    "                            ('classifier', best_classifier)\n",
    "                        ])\n",
    "\n",
    "                        # Ensure the feature names match\n",
    "                        X_test = X_test[X_train.columns]\n",
    "\n",
    "                        if y_test is not None:\n",
    "                            best_pipeline, best_score = retrain_model(best_pipeline, X_train, y_train, X_test, y_test)\n",
    "\n",
    "                            # Stocker les scores de validation croisée\n",
    "                            if best_classifier_name not in all_scores:\n",
    "                                all_scores[best_classifier_name] = []\n",
    "                            all_scores[best_classifier_name].extend(study.trials_dataframe().value.values)\n",
    "\n",
    "                            # Déterminer le chemin de sortie\n",
    "                            relative_path = os.path.relpath(root, base_path)\n",
    "                            if testing:\n",
    "                                output_dir = os.path.join(output_folder, testing_sub_path_name, relative_path, target_column)\n",
    "                            else:\n",
    "                                output_dir = os.path.join(output_folder, relative_path, target_column)\n",
    "                                \n",
    "                            if not os.path.exists(output_dir):\n",
    "                                os.makedirs(output_dir)\n",
    "\n",
    "                            model_path = os.path.join(output_dir, f'best_{best_classifier_name}_model.pkl')\n",
    "                            joblib.dump(best_pipeline, model_path)\n",
    "\n",
    "                            # Évaluation du modèle\n",
    "                            y_pred = best_pipeline.predict(X_test)\n",
    "                            y_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]  # Only use the probability for the positive class\n",
    "                            accuracy = accuracy_score(y_test, y_pred)\n",
    "                            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "                            print(f\"Accuracy: {accuracy}\")\n",
    "                            print(f\"ROC AUC: {roc_auc}\")\n",
    "                            print(confusion_matrix(y_test, y_pred))\n",
    "                            print(classification_report(y_test, y_pred))\n",
    "\n",
    "                            # Logging avec mlflow\n",
    "                            mlflow.set_experiment('credit_scoring')\n",
    "                            with mlflow.start_run():\n",
    "                                mlflow.log_params(trial.params)\n",
    "                                mlflow.log_metric('accuracy', accuracy)\n",
    "                                mlflow.log_metric('roc_auc', roc_auc)\n",
    "                                mlflow.sklearn.log_model(best_pipeline, 'model')\n",
    "                                mlflow.log_artifact(file_path)\n",
    "                                mlflow.log_artifact(test_file_path)\n",
    "\n",
    "                            print(f'Model saved at {model_path}')\n",
    "\n",
    "                        else:\n",
    "                            print(f\"Skipping model evaluation for {target_column} as target column is not in test data.\")\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Visualisation des scores de validation croisée\n",
    "    model_names = list(all_scores.keys())\n",
    "    model_scores = [score for scores in all_scores.values() for score in scores]\n",
    "    model_names_repeated = [model for model in model_names for _ in range(len(all_scores[model]))]\n",
    "\n",
    "    plot_cross_val_scores(model_scores, model_names_repeated, output_folder)\n",
    "\n",
    "# Fonction de visualisation pour les scores de validation croisée\n",
    "def plot_cross_val_scores(model_scores, model_names, output_dir):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x=model_names, y=model_scores)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Cross-Validation Score')\n",
    "    plt.title('Model Comparison - Cross-Validation Scores')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(output_dir, 'cross_val_scores.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling modelization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing files:   0%|          | 0/270 [00:00<?, ?it/s]\u001b[A\u001b[A[I 2024-07-01 14:53:55,743] A new study created in memory with name: no-name-f1c6a080-9b10-4c3b-9526-0598f0532f76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /content/exports/manual_check_patch/testing_data/mean/LOF/ordinal/application_train.csv\n",
      "Using target column: TARGET\n",
      "Target column TARGET not found in test data. Skipping evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-01 14:53:57,741] Trial 0 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 132, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9045454545454545.\n",
      "[I 2024-07-01 14:53:58,528] Trial 1 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 252.58557981046323, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:53:59,345] Trial 2 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 402.74466048795546, 'solver': 'newton-cg'}. Best is trial 1 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:00,174] Trial 3 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 118, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:00,202] Trial 4 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 418.7249275577628, 'solver': 'liblinear'}. Best is trial 1 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:00,615] Trial 5 finished with value: 0.909090909090909 and parameters: {'classifier': 'Random Forest', 'n_estimators': 257, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:00,643] Trial 6 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 591.8707138381571, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:01,065] Trial 7 finished with value: 0.909090909090909 and parameters: {'classifier': 'Random Forest', 'n_estimators': 264, 'max_depth': 25, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:01,366] Trial 8 finished with value: 0.909090909090909 and parameters: {'classifier': 'Random Forest', 'n_estimators': 213, 'max_depth': 23, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:01,432] Trial 9 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 454.7733937618157, 'solver': 'newton-cg'}. Best is trial 1 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:01,464] Trial 10 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 67.54226617115381, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:01,495] Trial 11 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 15.667717518061863, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:01,528] Trial 12 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 48.42204728792461, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:01,559] Trial 13 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 212.81740804105837, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:01,601] Trial 14 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 205.3182679398, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:01,622] Trial 15 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 769.0675684967634, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:01,667] Trial 16 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 210.97763012290145, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:01,711] Trial 17 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 239.40347177985387, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:01,735] Trial 18 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 94.28589435449686, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:01,768] Trial 19 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 937.722664470223, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:01,852] Trial 20 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 297.28230395666435, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:01,926] Trial 21 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 368.4071258500459, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:02,010] Trial 22 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 577.2767336645204, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:02,093] Trial 23 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 119.31726528439572, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:02,176] Trial 24 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 350.3264867867303, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:02,207] Trial 25 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 559.8358824666514, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:02,527] Trial 26 finished with value: 0.9 and parameters: {'classifier': 'Random Forest', 'n_estimators': 177, 'max_depth': 29, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:02,570] Trial 27 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 138.66177677046556, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:02,654] Trial 28 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 708.57091586281, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:03,159] Trial 29 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 298, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:03,196] Trial 30 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 286.26048654598026, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:03,401] Trial 31 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 100, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:03,691] Trial 32 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 161, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:03,904] Trial 33 finished with value: 0.909090909090909 and parameters: {'classifier': 'Random Forest', 'n_estimators': 103, 'max_depth': 29, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:04,286] Trial 34 finished with value: 0.9136363636363637 and parameters: {'classifier': 'Random Forest', 'n_estimators': 208, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:04,723] Trial 35 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 152, 'max_depth': 23, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:04,967] Trial 36 finished with value: 0.9136363636363637 and parameters: {'classifier': 'Random Forest', 'n_estimators': 130, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:05,372] Trial 37 finished with value: 0.9136363636363637 and parameters: {'classifier': 'Random Forest', 'n_estimators': 233, 'max_depth': 26, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:05,415] Trial 38 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 469.4470063295472, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:05,757] Trial 39 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 183, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:05,854] Trial 40 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 140.07099815450988, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:05,892] Trial 41 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 365.26646732548323, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:05,930] Trial 42 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 423.89431682982615, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:05,966] Trial 43 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 393.64790031414833, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:06,002] Trial 44 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 298.84174370872495, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:06,048] Trial 45 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 526.8193660064292, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:06,084] Trial 46 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 500.88695949542193, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:06,121] Trial 47 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 631.7162754973818, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:06,210] Trial 48 finished with value: 0.9272727272727274 and parameters: {'classifier': 'Logistic Regression', 'C': 10.642962699457762, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:06,296] Trial 49 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 31.622674500430566, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:06,385] Trial 50 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 24.67513538264093, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:06,463] Trial 51 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 0.6490311524982388, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:06,547] Trial 52 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 66.19626707525248, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:06,644] Trial 53 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 41.07101074505228, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:06,719] Trial 54 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 52.16982495507078, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:06,806] Trial 55 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 49.58666437508919, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:06,891] Trial 56 finished with value: 0.9272727272727274 and parameters: {'classifier': 'Logistic Regression', 'C': 7.985981193087895, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:06,966] Trial 57 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 166.48858913309297, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,042] Trial 58 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 1.146976017232734, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,126] Trial 59 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 86.03934210162093, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,212] Trial 60 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 42.43047336388324, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,289] Trial 61 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 87.21299009048965, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,374] Trial 62 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 42.08684585286085, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,460] Trial 63 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 170.1566689283426, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,545] Trial 64 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 103.99878762188047, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,631] Trial 65 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 26.69110534244205, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,708] Trial 66 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 4.811315482620532, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,792] Trial 67 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 171.11340683277996, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,889] Trial 68 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 81.15011727234531, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:07,978] Trial 69 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 56.6085546336876, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,065] Trial 70 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 127.1413056827821, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,152] Trial 71 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 46.449243810009264, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,240] Trial 72 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 62.14730745350095, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,326] Trial 73 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 121.74159997147646, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,412] Trial 74 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 30.624727529441458, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,499] Trial 75 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 968.067195773838, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,575] Trial 76 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 205.7664637514884, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,610] Trial 77 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 96.58961188114957, 'solver': 'lbfgs'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,694] Trial 78 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 155.22719220434368, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,770] Trial 79 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 2.3442109838706813, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,806] Trial 80 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 65.63519589851269, 'solver': 'lbfgs'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,893] Trial 81 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 1.986702569118009, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:08,979] Trial 82 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 29.35864589998447, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,064] Trial 83 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 115.84322522940084, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,149] Trial 84 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 31.31723074426143, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,236] Trial 85 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 830.1053583469952, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,311] Trial 86 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 76.61480884570287, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,357] Trial 87 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 254.8211133989559, 'solver': 'lbfgs'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,443] Trial 88 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 26.23258552492308, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,529] Trial 89 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 59.36841090029852, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,619] Trial 90 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 97.71817388923834, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,705] Trial 91 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 44.45168462527829, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,779] Trial 92 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 0.28347909117757375, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,862] Trial 93 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 45.48300087848771, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,926] Trial 94 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 136.24874961197986, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:09,960] Trial 95 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 80.35667036603017, 'solver': 'lbfgs'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:10,412] Trial 96 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 28.69382509836379, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:416: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "[I 2024-07-01 14:54:11,222] Trial 97 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 101.21032397836973, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:14,309] Trial 98 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 283, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 48 with value: 0.9272727272727274.\n",
      "[I 2024-07-01 14:54:14,395] Trial 99 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 184.63781730849774, 'solver': 'newton-cg'}. Best is trial 48 with value: 0.9272727272727274.\n",
      "\n",
      "\n",
      "Processing files:   0%|          | 1/270 [00:18<1:23:50, 18.70s/it]\u001b[A\u001b[A[I 2024-07-01 14:54:14,445] A new study created in memory with name: no-name-ec3c20ee-d673-4845-b0f5-13f00177c595\n",
      "[I 2024-07-01 14:54:14,474] Trial 0 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 86.50957482717335, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  100\n",
      "Best trial:\n",
      "  Value:  0.9272727272727274\n",
      "  Params: \n",
      "    classifier: Logistic Regression\n",
      "    C: 10.642962699457762\n",
      "    solver: newton-cg\n",
      "Skipping model evaluation for TARGET as target column is not in test data.\n",
      "Processing file: /content/exports/manual_check_patch/testing_data/mean/LOF/label/application_train.csv\n",
      "Using target column: TARGET\n",
      "Target column TARGET not found in test data. Skipping evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-01 14:54:14,868] Trial 1 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 244, 'max_depth': 13, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:14,897] Trial 2 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 858.8050057226584, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:14,925] Trial 3 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 440.4666323634905, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:15,196] Trial 4 finished with value: 0.9 and parameters: {'classifier': 'Random Forest', 'n_estimators': 151, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:15,470] Trial 5 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 162, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:15,497] Trial 6 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 887.3373285087307, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:15,911] Trial 7 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 269, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:16,244] Trial 8 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 214, 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:16,313] Trial 9 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 745.4553459298659, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:16,358] Trial 10 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 14.864198921428809, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,391] Trial 11 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 0.48076035410430507, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,424] Trial 12 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 27.608925625512462, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,459] Trial 13 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 251.75593619635168, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,532] Trial 14 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 235.45011598106572, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,557] Trial 15 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 203.8059947216713, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,594] Trial 16 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 517.201761324814, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,629] Trial 17 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 126.95276330345641, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,663] Trial 18 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 452.1807860093661, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,689] Trial 19 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 346.17783307155713, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:416: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "[I 2024-07-01 14:54:16,773] Trial 20 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 636.8439798213649, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,807] Trial 21 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 993.011378263405, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,850] Trial 22 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 132.86740954229307, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,882] Trial 23 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 750.9040293415738, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,926] Trial 24 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 108.42181041437728, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:16,959] Trial 25 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 324.8724901086508, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:17,158] Trial 26 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 106, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:17,191] Trial 27 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 786.5703406636823, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:17,274] Trial 28 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 607.5386485532256, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:17,766] Trial 29 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 292, 'max_depth': 23, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:17,800] Trial 30 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 63.215272230610026, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:17,835] Trial 31 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 408.50773880898885, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:17,871] Trial 32 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 635.2378007042338, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:17,905] Trial 33 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 533.3951729129251, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:18,216] Trial 34 finished with value: 0.909090909090909 and parameters: {'classifier': 'Random Forest', 'n_estimators': 187, 'max_depth': 30, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:18,241] Trial 35 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 184.19032942716325, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:18,461] Trial 36 finished with value: 0.9136363636363637 and parameters: {'classifier': 'Random Forest', 'n_estimators': 121, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:18,495] Trial 37 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 319.61623975801655, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:18,530] Trial 38 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 871.322516873272, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:18,901] Trial 39 finished with value: 0.9136363636363637 and parameters: {'classifier': 'Random Forest', 'n_estimators': 224, 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:18,936] Trial 40 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 950.9039881922276, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:19,227] Trial 41 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 167, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:19,508] Trial 42 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 141, 'max_depth': 23, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:19,862] Trial 43 finished with value: 0.909090909090909 and parameters: {'classifier': 'Random Forest', 'n_estimators': 187, 'max_depth': 17, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:20,169] Trial 44 finished with value: 0.9136363636363637 and parameters: {'classifier': 'Random Forest', 'n_estimators': 163, 'max_depth': 24, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:21,158] Trial 45 finished with value: 0.909090909090909 and parameters: {'classifier': 'Random Forest', 'n_estimators': 242, 'max_depth': 18, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:21,319] Trial 46 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 836.3591279491086, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:21,994] Trial 47 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 91.0564047368397, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:22,287] Trial 48 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 699.2943241816906, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:22,511] Trial 49 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 3.8434834937581286, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:24,378] Trial 50 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 130, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:24,632] Trial 51 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 895.4195271942104, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:24,712] Trial 52 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 925.6240533881056, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:24,746] Trial 53 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 817.9279962364766, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:24,781] Trial 54 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 46.72455541922142, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:24,865] Trial 55 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 704.9823054420184, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:24,910] Trial 56 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 273.10323886458303, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:24,934] Trial 57 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 152.0501070057581, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:24,979] Trial 58 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 430.175180235493, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:25,013] Trial 59 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 568.962560358338, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:25,057] Trial 60 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 992.7579033675672, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:25,514] Trial 61 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 294, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:25,957] Trial 62 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 268, 'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:26,384] Trial 63 finished with value: 0.9136363636363637 and parameters: {'classifier': 'Random Forest', 'n_estimators': 269, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:26,722] Trial 64 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 201, 'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:27,063] Trial 65 finished with value: 0.9136363636363637 and parameters: {'classifier': 'Random Forest', 'n_estimators': 175, 'max_depth': 21, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:27,149] Trial 66 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 479.6728922999565, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:27,187] Trial 67 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 195.43632772313737, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:27,222] Trial 68 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 251.87099173157327, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:27,606] Trial 69 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 241, 'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:27,639] Trial 70 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 373.3030526610404, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:416: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "[I 2024-07-01 14:54:27,714] Trial 71 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 745.0148560176295, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:27,788] Trial 72 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 841.3269846794694, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "[I 2024-07-01 14:54:27,873] Trial 73 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 782.4537165797121, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:27,959] Trial 74 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 695.7902188527174, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:27,993] Trial 75 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 0.8028966532135229, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:28,350] Trial 76 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 215, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:28,373] Trial 77 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 946.8577770185849, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "[I 2024-07-01 14:54:28,459] Trial 78 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 890.8033072679897, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:28,875] Trial 79 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 268, 'max_depth': 21, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:28,932] Trial 80 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 775.9080906453037, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:28,979] Trial 81 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 83.54677251020853, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,014] Trial 82 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 34.70280602223055, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,062] Trial 83 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 112.9131027271624, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,098] Trial 84 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 58.86719428864114, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,136] Trial 85 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 156.4889631726528, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,183] Trial 86 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 864.5743983065076, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,506] Trial 87 finished with value: 0.909090909090909 and parameters: {'classifier': 'Random Forest', 'n_estimators': 153, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,552] Trial 88 finished with value: 0.9227272727272726 and parameters: {'classifier': 'Logistic Regression', 'C': 23.254924278493142, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,597] Trial 89 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 74.24165236032846, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,632] Trial 90 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 25.18741894661315, 'solver': 'liblinear'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,668] Trial 91 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 2.125339788270935, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,705] Trial 92 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 128.34505612234364, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,753] Trial 93 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 96.53164662639605, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:29,790] Trial 94 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 46.34844997053112, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:30,160] Trial 95 finished with value: 0.9136363636363637 and parameters: {'classifier': 'Random Forest', 'n_estimators': 190, 'max_depth': 25, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:30,209] Trial 96 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 661.428238097195, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:30,256] Trial 97 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 589.235180546586, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:30,363] Trial 98 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 167.59569959615646, 'solver': 'newton-cg'}. Best is trial 10 with value: 0.9227272727272726.\n",
      "[I 2024-07-01 14:54:30,652] Trial 99 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 108, 'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.9227272727272726.\n",
      "\n",
      "\n",
      "Processing files:   1%|          | 2/270 [00:34<1:17:06, 17.26s/it]\u001b[A\u001b[A[I 2024-07-01 14:54:30,721] A new study created in memory with name: no-name-a0e8e0e0-8784-4ce4-be62-95ec397d06c4\n",
      "[I 2024-07-01 14:54:30,751] Trial 0 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 135.9389491092075, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:30,781] Trial 1 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 579.9276622298015, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:30,809] Trial 2 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 403.43772892465654, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  100\n",
      "Best trial:\n",
      "  Value:  0.9227272727272726\n",
      "  Params: \n",
      "    classifier: Logistic Regression\n",
      "    C: 14.864198921428809\n",
      "    solver: lbfgs\n",
      "Skipping model evaluation for TARGET as target column is not in test data.\n",
      "Processing file: /content/exports/manual_check_patch/testing_data/mean/LOF/onehot/application_train.csv\n",
      "Using target column: TARGET\n",
      "Target column TARGET not found in test data. Skipping evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-01 14:54:33,613] Trial 3 finished with value: 0.9 and parameters: {'classifier': 'Random Forest', 'n_estimators': 231, 'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:35,600] Trial 4 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 276, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:36,119] Trial 5 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 254, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:36,159] Trial 6 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 839.0067781350865, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:36,666] Trial 7 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 267, 'max_depth': 27, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,032] Trial 8 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 201, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,061] Trial 9 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 225.9963461266239, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,100] Trial 10 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 0.22543752537322348, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,136] Trial 11 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 692.8106468205265, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,177] Trial 12 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 556.8042839386532, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,216] Trial 13 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 292.52105154120414, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,255] Trial 14 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 52.16119625028125, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,292] Trial 15 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 993.247234720224, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,333] Trial 16 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 508.70900711130844, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,372] Trial 17 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 652.7463863655324, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,416] Trial 18 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 184.58408802027748, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,456] Trial 19 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 381.86564533746485, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,505] Trial 20 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 751.348637715083, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,546] Trial 21 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 346.5247060077366, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,584] Trial 22 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 444.9204361314292, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,631] Trial 23 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 572.0895780024125, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,668] Trial 24 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 131.54346658130208, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,706] Trial 25 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 463.8206131866687, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:37,992] Trial 26 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 139, 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:38,032] Trial 27 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 251.2339025799022, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:38,081] Trial 28 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 116.66998349134066, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:38,309] Trial 29 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 110, 'max_depth': 23, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:38,347] Trial 30 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 341.9515865179469, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:38,800] Trial 31 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 296, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:39,100] Trial 32 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 187, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:39,546] Trial 33 finished with value: 0.9136363636363637 and parameters: {'classifier': 'Random Forest', 'n_estimators': 294, 'max_depth': 21, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:39,868] Trial 34 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 192, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:40,243] Trial 35 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 227, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:40,515] Trial 36 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 160, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:40,949] Trial 37 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 262, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:41,313] Trial 38 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 229, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:41,349] Trial 39 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 600.8092265792903, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:41,384] Trial 40 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 774.6755017903448, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:44,419] Trial 41 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 269, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:45,869] Trial 42 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 245, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:46,302] Trial 43 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 281, 'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:46,675] Trial 44 finished with value: 0.909090909090909 and parameters: {'classifier': 'Random Forest', 'n_estimators': 251, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:46,711] Trial 45 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 415.82084607850265, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:46,748] Trial 46 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 871.4044039055899, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,137] Trial 47 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 209, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,172] Trial 48 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 526.2524163997265, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,206] Trial 49 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 291.9985697217601, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,241] Trial 50 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 632.8345556191648, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,276] Trial 51 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 860.6269056201422, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,313] Trial 52 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 963.9088524938126, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,348] Trial 53 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 730.9369407771104, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,384] Trial 54 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 830.47241097256, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,408] Trial 55 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 1.3292950036575064, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,433] Trial 56 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 925.9021839434217, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,470] Trial 57 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 675.525946529827, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,896] Trial 58 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 282, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,929] Trial 59 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 203.48179685952545, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:47,963] Trial 60 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 108.74754174113566, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:48,226] Trial 61 finished with value: 0.9136363636363637 and parameters: {'classifier': 'Random Forest', 'n_estimators': 163, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:48,550] Trial 62 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 207, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:48,844] Trial 63 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 176, 'max_depth': 17, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:49,178] Trial 64 finished with value: 0.909090909090909 and parameters: {'classifier': 'Random Forest', 'n_estimators': 216, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:49,595] Trial 65 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 246, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:49,632] Trial 66 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 793.0345546008014, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:49,867] Trial 67 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 134, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:49,902] Trial 68 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 709.1422092938319, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:50,329] Trial 69 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 281, 'max_depth': 18, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:50,355] Trial 70 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 487.8021680891169, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:50,389] Trial 71 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 222.73688661084304, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:50,418] Trial 72 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 158.70671645170356, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:50,452] Trial 73 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 63.93843887548647, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:50,477] Trial 74 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 280.88359376614943, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:50,893] Trial 75 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 258, 'max_depth': 22, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:50,927] Trial 76 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 379.6938451676385, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:51,302] Trial 77 finished with value: 0.9 and parameters: {'classifier': 'Random Forest', 'n_estimators': 235, 'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:51,340] Trial 78 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 323.9814156305039, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:51,376] Trial 79 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 49.90903353680102, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:51,845] Trial 80 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 225, 'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:51,888] Trial 81 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 23.44964748023702, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:51,955] Trial 82 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 102.10152180353575, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:52,137] Trial 83 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 236.05470200406103, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:52,313] Trial 84 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 174.44753627873817, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:52,490] Trial 85 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 76.52468119423176, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:52,677] Trial 86 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 556.5966713411902, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:55,890] Trial 87 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 272, 'max_depth': 26, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:56,101] Trial 88 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 147.33426056358496, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:56,154] Trial 89 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 259.9095580569984, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:56,858] Trial 90 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Random Forest', 'n_estimators': 300, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:56,899] Trial 91 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 638.518931026616, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:56,937] Trial 92 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 591.0574474535531, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:56,978] Trial 93 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 531.9403969459935, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:57,033] Trial 94 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 450.60913130880436, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:57,072] Trial 95 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 690.3781563795462, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:57,351] Trial 96 finished with value: 0.909090909090909 and parameters: {'classifier': 'Random Forest', 'n_estimators': 101, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:57,403] Trial 97 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 405.4160728864787, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:57,444] Trial 98 finished with value: 0.9181818181818182 and parameters: {'classifier': 'Logistic Regression', 'C': 800.2621234670772, 'solver': 'liblinear'}. Best is trial 0 with value: 0.9181818181818182.\n",
      "[I 2024-07-01 14:54:57,834] Trial 99 finished with value: 0.9045454545454545 and parameters: {'classifier': 'Random Forest', 'n_estimators': 136, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9181818181818182.\n",
      "\n",
      "\n",
      "Processing files:   1%|          | 3/270 [01:02<1:36:59, 21.80s/it]\u001b[A\u001b[A[I 2024-07-01 14:54:57,943] A new study created in memory with name: no-name-5a4faedf-f988-47cd-9916-4b7e087fc53b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  100\n",
      "Best trial:\n",
      "  Value:  0.9181818181818182\n",
      "  Params: \n",
      "    classifier: Logistic Regression\n",
      "    C: 135.9389491092075\n",
      "    solver: liblinear\n",
      "Skipping model evaluation for TARGET as target column is not in test data.\n",
      "Processing file: /content/exports/manual_check_patch/testing_data/mean/Z-Score/ordinal/application_train.csv\n",
      "Using target column: TARGET\n",
      "Target column TARGET not found in test data. Skipping evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-07-01 14:54:58,137] Trial 0 failed with parameters: {'classifier': 'Logistic Regression', 'C': 287.19449047433335, 'solver': 'newton-cg'} because of the following error: ValueError('This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0').\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 598, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 598, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2807/3316308504.py\", line 39, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100)\n",
      "  File \"/tmp/ipykernel_2807/1747956399.py\", line 26, in objective\n",
      "    score = cross_val_score(pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring='accuracy', error_score='raise')\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 562, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/_param_validation.py\", line 214, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 309, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 1754, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 1789, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 745, in get_result\n",
      "    return self._return_or_raise()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 763, in _return_or_raise\n",
      "    raise self._result\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "[W 2024-07-01 14:54:58,140] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/usr/local/lib/python3.8/dist-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/sklearn/utils/parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\", line 427, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/usr/local/lib/python3.8/dist-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n    raise ValueError(\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_evaluate_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL_EXPORT_MANUAL_CHECK_PATCH_FOLDER_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL_EXPORT_MODELIZATION_FOLDER_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_COLUMNS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTESTING_MODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGENERAL_CHUNK_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtesting_sub_path_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTESTING_MODE_SUB_FOLDER_NAME\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 39\u001b[0m, in \u001b[0;36mtrain_and_evaluate_models\u001b[0;34m(base_path, output_folder, target_columns, max_features, testing, chunk_size, testing_sub_path_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Optimisation des hyperparamètres avec Optuna\u001b[39;00m\n\u001b[1;32m     38\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of finished trials: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[48], line 39\u001b[0m, in \u001b[0;36mtrain_and_evaluate_models.<locals>.<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Optimisation des hyperparamètres avec Optuna\u001b[39;00m\n\u001b[1;32m     38\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of finished trials: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 26\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, X_train, y_train)\u001b[0m\n\u001b[1;32m     20\u001b[0m classifier\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrial_params)\n\u001b[1;32m     22\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     23\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, classifier)\n\u001b[1;32m     24\u001b[0m ])\n\u001b[0;32m---> 26\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraise\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m score\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    560\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 562\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 309\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_models(\n",
    "    base_path=LOCAL_EXPORT_MANUAL_CHECK_PATCH_FOLDER_PATH, \n",
    "    output_folder=LOCAL_EXPORT_MODELIZATION_FOLDER_PATH, \n",
    "    target_columns=TARGET_COLUMNS, \n",
    "    testing=TESTING_MODE, \n",
    "    chunk_size=GENERAL_CHUNK_SIZE, \n",
    "    testing_sub_path_name=TESTING_MODE_SUB_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluation des Modèles\n",
    "\n",
    "After training the models, the evaluate_models function is used to evaluate the performance of each model. This function calculates several performance metrics and also computes the business score based on the given formula.\n",
    "\n",
    "Business Score Calculation\n",
    "The business score is calculated using the following formula:\n",
    "\n",
    "Business Score\n",
    "=\n",
    "(\n",
    "Benefit\n",
    "×\n",
    "PPV\n",
    ")\n",
    "−\n",
    "(\n",
    "Cost\n",
    "×\n",
    "False Positive Rate\n",
    ")\n",
    "Business Score=(Benefit×PPV)−(Cost×False Positive Rate)\n",
    "\n",
    "Where:\n",
    "\n",
    "Benefit: The benefit obtained from a correct prediction.\n",
    "PPV (Positive Predictive Value): \n",
    "TP\n",
    "TP\n",
    "+\n",
    "FP\n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " , where TP is the number of true positives and FP is the number of false positives.\n",
    "Cost: The cost associated with an incorrect prediction.\n",
    "False Positive Rate: \n",
    "FP\n",
    "Total Negatives\n",
    "Total Negatives\n",
    "FP\n",
    "​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(base_path, output_folder, target_columns, chunk_size=1000, benefit=1.0, cost=1.0):\n",
    "    performance_metrics = []\n",
    "    for root, dirs, files in os.walk(output_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.pkl') and any(col in root for col in target_columns):\n",
    "                model_path = os.path.join(root, file)\n",
    "                model_name = file.split('_')[1]\n",
    "                target_column = [col for col in target_columns if col in root][0]\n",
    "\n",
    "                print(f\"Analyzing model: {model_name} for target: {target_column}\")\n",
    "\n",
    "                # Charger le modèle et les données de test\n",
    "                best_pipeline = joblib.load(model_path)\n",
    "                test_file_path = os.path.join(os.path.dirname(model_path).replace(output_folder, base_path), 'application_test.csv')\n",
    "                test_data = pd.read_csv(test_file_path, chunksize=chunk_size)\n",
    "\n",
    "                for chunk in test_data:\n",
    "                    if target_column not in chunk.columns:\n",
    "                        continue\n",
    "\n",
    "                    X_test = chunk.drop(target_column, axis=1)\n",
    "                    y_test = chunk[target_column]\n",
    "\n",
    "                    # Prédictions\n",
    "                    y_pred = best_pipeline.predict(X_test)\n",
    "                    y_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                    # Calcul des métriques de performance\n",
    "                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                    precision = precision_score(y_test, y_pred)\n",
    "                    recall = recall_score(y_test, y_pred)\n",
    "                    f1 = f1_score(y_test, y_pred)\n",
    "                    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "                    # Matrice de confusion\n",
    "                    cm = confusion_matrix(y_test, y_pred)\n",
    "                    TP = cm[1, 1]\n",
    "                    FP = cm[0, 1]\n",
    "                    TN = cm[0, 0]\n",
    "                    FN = cm[1, 0]\n",
    "\n",
    "                    # Calcul du VPP et Taux de faux positifs\n",
    "                    VPP = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "                    taux_fp = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "\n",
    "                    # Calcul du Score Métier\n",
    "                    score_metier = (benefit * VPP) - (cost * taux_fp)\n",
    "\n",
    "                    performance_metrics.append({\n",
    "                        'model': model_name,\n",
    "                        'target': target_column,\n",
    "                        'accuracy': accuracy,\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'f1_score': f1,\n",
    "                        'roc_auc': roc_auc,\n",
    "                        'score_metier': score_metier\n",
    "                    })\n",
    "\n",
    "                    print(f\"Accuracy: {accuracy}\")\n",
    "                    print(f\"Precision: {precision}\")\n",
    "                    print(f\"Recall: {recall}\")\n",
    "                    print(f\"F1 Score: {f1}\")\n",
    "                    print(f\"ROC AUC: {roc_auc}\")\n",
    "                    print(f\"Score Métier: {score_metier}\")\n",
    "\n",
    "                    # Visualisation des résultats\n",
    "                    plot_roc_curve(y_test, y_pred_proba, model_name, root)\n",
    "                    plot_confusion_matrix(y_test, y_pred, model_name, root)\n",
    "\n",
    "    # Convertir les métriques de performance en DataFrame\n",
    "    performance_df = pd.DataFrame(performance_metrics)\n",
    "    performance_df.to_csv(os.path.join(output_folder, 'model_performance_metrics.csv'), index=False)\n",
    "\n",
    "    return performance_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sélection du Modèle Final et Hyperparameter Tuning\n",
    "\n",
    "After evaluating the models, the best performing model can be selected and further tuned using hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def select_and_tune_best_model(performance_df, base_path, output_folder, target_column):\n",
    "    best_model_row = performance_df.loc[performance_df['roc_auc'].idxmax()]\n",
    "    best_model_name = best_model_row['model']\n",
    "    print(f\"Best model: {best_model_name} with ROC AUC: {best_model_row['roc_auc']}\")\n",
    "\n",
    "    model_path = os.path.join(output_folder, best_model_name, target_column, f'best_{best_model_name}_model.pkl')\n",
    "    best_pipeline = joblib.load(model_path)\n",
    "\n",
    "    # Tuning des hyperparamètres du meilleur modèle avec Optuna\n",
    "    X_train = pd.read_csv(os.path.join(base_path, 'application_train.csv')).drop(target_column, axis=1)\n",
    "    y_train = pd.read_csv(os.path.join(base_path, 'application_train.csv'))[target_column]\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100)\n",
    "\n",
    "    print('Number of finished trials: ', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('  Value: ', trial.value)\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))\n",
    "\n",
    "    best_classifier = models[best_model_name]['model']\n",
    "    best_params = {k: v for k, v in trial.params.items() if k != 'classifier'}\n",
    "    best_classifier.set_params(**best_params)\n",
    "\n",
    "    best_pipeline = Pipeline(steps=[\n",
    "        ('classifier', best_classifier)\n",
    "    ])\n",
    "\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "    joblib.dump(best_pipeline, model_path)\n",
    "    print(f\"Model saved at {model_path}\")\n",
    "\n",
    "    return best_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interprétabilité et Explicabilité\n",
    "Using SHAP to explain the predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_model(best_pipeline, X_train, output_folder):\n",
    "    explainer = shap.TreeExplainer(best_pipeline.named_steps['classifier'])\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "    shap.summary_plot(shap_values, X_train)\n",
    "    plt.savefig(os.path.join(output_folder, 'shap_summary_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "    shap.initjs()\n",
    "    shap.force_plot(explainer.expected_value, shap_values[0,:], X_train.iloc[0,:])\n",
    "    plt.savefig(os.path.join(output_folder, 'shap_force_plot.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(performance_df):\n",
    "    best_model_row = performance_df.loc[performance_df['roc_auc'].idxmax()]\n",
    "    best_model_name = best_model_row['model']\n",
    "    summary = f\"Best model: {best_model_name}\\n\"\n",
    "    summary += f\"Accuracy: {best_model_row['accuracy']}\\n\"\n",
    "    summary += f\"Precision: {best_model_row['precision']}\\n\"\n",
    "    summary += f\"Recall: {best_model_row['recall']}\\n\"\n",
    "    summary += f\"F1 Score: {best_model_row['f1_score']}\\n\"\n",
    "    summary += f\"ROC AUC: {best_model_row['roc_auc']}\\n\"\n",
    "    summary += f\"Business Score: {best_model_row['business_score']}\\n\"\n",
    "    \n",
    "    print(summary)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'model_summary.txt'), 'w') as f:\n",
    "        f.write(summary)\n",
    "\n",
    "    print(\"Summary saved.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
